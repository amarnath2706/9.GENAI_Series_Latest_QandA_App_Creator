{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'****************your key************************'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the openai api key\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the openai api key as the environment variable, so that openai package will be load\n",
    "os.environ['OPENAI_API_KEY']= OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Amarnath\\\\9.GENAI_Series_Latest_QandA_App_Creator\\\\research'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Amarnath\\9.GENAI_Series_Latest_QandA_App_Creator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qacreator\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "#go to previous dir\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Amarnath\\\\9.GENAI_Series_Latest_QandA_App_Creator'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the file path\n",
    "file_path = \"data/ULM.pdf\"\n",
    "loader=PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/ULM.pdf', 'page': 0}, page_content='Uniﬁed Language Model Pre-training for\\nNatural Language Understanding and Generation\\nLi Dong∗ Nan Yang∗ Wenhui Wang∗ Furu Wei∗† Xiaodong Liu Yu Wang\\nJianfeng Gao Ming Zhou Hsiao-Wuen Hon\\nMicrosoft Research\\n{lidong1,nanya,wenwan,fuwei}@microsoft.com\\n{xiaodl,yuwan,jfgao,mingzhou,hon}@microsoft.com\\nAbstract\\nThis paper presents a new UNIﬁed pre-trained Language Model (UNILM) that\\ncan be ﬁne-tuned for both natural language understanding and generation tasks.\\nThe model is pre-trained using three types of language modeling tasks: unidirec-\\ntional, bidirectional, and sequence-to-sequence prediction. The uniﬁed modeling\\nis achieved by employing a shared Transformer network and utilizing speciﬁc\\nself-attention masks to control what context the prediction conditions on. UNILM\\ncompares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UNILM achieves new state-of-\\nthe-art results on ﬁve natural language generation datasets, including improving\\nthe CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute\\nimprovement), the Gigaword abstractive summarization ROUGE-L to35.75 (0.86\\nabsolute improvement), the CoQA generative question answering F1 score to 82.5\\n(37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12\\n(3.75 absolute improvement), and the DSTC7 document-grounded dialog response\\ngeneration NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained\\nmodels are available at https://github.com/microsoft/unilm.\\n1 Introduction\\nLanguage model (LM) pre-training has substantially advanced the state of the art across a variety\\nof natural language processing tasks [8, 29, 19, 31, 9, 1]. Pre-trained LMs learn contextualized text\\nrepresentations by predicting words based on their context using large amounts of text data, and can\\nbe ﬁne-tuned to adapt to downstream tasks.\\nDifferent prediction tasks and training objectives have been used for pre-training LMs of different\\ntypes, as shown in Table 1. ELMo [ 29] learns two unidirectional LMs: a forward LM reads the\\ntext from left to right, and a backward LM encodes the text from right to left. GPT [ 31] uses\\na left-to-right Transformer [ 43] to predict a text sequence word-by-word. In contrast, BERT [ 9]\\nemploys a bidirectional Transformer encoder to fuse both the left and right context to predict the\\nmasked words. Although BERT signiﬁcantly improves the performance of a wide range of natural\\nlanguage understanding tasks [9], its bidirectionality nature makes it difﬁcult to be applied to natural\\nlanguage generation tasks [44].\\nIn this work we propose a new UNIﬁed pre-trained Language Model (UNILM) that can be applied to\\nboth natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is\\na multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three\\ntypes of unsupervised language modeling objectives as shown in Table 2. In particular, we design a\\n∗ Equal contribution. † Contact person.\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1905.03197v3  [cs.CL]  15 Oct 2019'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 1}, page_content='ELMo GPT BERT U NILM\\nLeft-to-Right LM ✓ ✓ ✓\\nRight-to-Left LM ✓ ✓\\nBidirectional LM ✓ ✓\\nSequence-to-Sequence LM ✓\\nTable 1: Comparison between language model (LM) pre-training objectives.\\nBackbone\\nNetwork\\nLM Objectives of\\nUniﬁed Pre-training What Uniﬁed LM Learns Example Downstream Tasks\\nTransformer\\nwith shared\\nparameters\\nfor all LM\\nobjectives\\nBidirectional LM Bidirectional encoding GLUE benchmark\\nExtractive question answering\\nUnidirectional LM Unidirectional decoding Long text generation\\nSequence-to-Sequence LM\\nUnidirectional decoding\\nconditioned on\\nbidirectional encoding\\nAbstractive summarization\\nQuestion generation\\nGenerative question answering\\nTable 2: The uniﬁed LM is jointly pre-trained by multiple language modeling objectives, sharing the\\nsame parameters. We ﬁne-tune and evaluate the pre-trained uniﬁed LM on various datasets, including\\nboth language understanding and generation tasks.\\nset of cloze tasks [42] where a masked word is predicted based on its context. These cloze tasks differ\\nin how the context is deﬁned. For a left-to-right unidirectional LM, the context of the masked word\\nto be predicted consists of all the words on its left. For a right-to-left unidirectional LM, the context\\nconsists of all the words on the right. For a bidirectional LM, the context consists of the words on\\nboth the right and the left [9]. For a sequence-to-sequence LM, the context of the to-be-predicted\\nword in the second (target) sequence consists of all the words in the ﬁrst (source) sequence and the\\nwords on the its left in the target sequence.\\nSimilar to BERT, the pre-trained UNILM can be ﬁne-tuned (with additional task-speciﬁc layers\\nif necessary) to adapt to various downstream tasks. But unlike BERT which is used mainly for\\nNLU tasks, UNILM can be conﬁgured, using different self-attention masks (Section 2), to aggregate\\ncontext for different types of language models, and thus can be used for both NLU and NLG tasks.\\nThe proposed UNILM has three main advantages. First, the uniﬁed pre-training procedure leads to a\\nsingle Transformer LM that uses the shared parameters and architecture for different types of LMs,\\nalleviating the need of separately training and hosting multiple LMs. Second, the parameter sharing\\nmakes the learned text representations more general because they are jointly optimized for different\\nlanguage modeling objectives where context is utilized in different ways, mitigating overﬁtting to\\nany single LM task. Third, in addition to its application to NLU tasks, the use of UNILM as a\\nsequence-to-sequence LM (Section 2.3), makes it a natural choice for NLG, such as abstractive\\nsummarization and question generation.\\nExperimental results show that our model, used as a bidirectional encoder, compares favorably with\\nBERT on the GLUE benchmark and two extractive question answering tasks (i.e., SQuAD 2.0 and\\nCoQA). In addition, we demonstrate the effectiveness of UNILM on ﬁve NLG datasets, where it\\nis used as a sequence-to-sequence model, creating new state-of-the-art results on CNN/DailyMail\\nand Gigaword abstractive summarization, SQuAD question generation, CoQA generative question\\nanswering, and DSTC7 dialog response generation.\\n2 Uniﬁed Language Model Pre-training\\nGiven an input sequence x= x1 ···x|x|, UNILM obtains a contextualized vector representation for\\neach token. As shown in Figure 1, the pre-training optimizes the shared Transformer [43] network\\nwith respect to several unsupervised language modeling objectives, namely, unidirectional LM,\\nbidirectional LM, and sequence-to-sequence LM. In order to control the access to the context of the\\nword token to be predicted, we employ different masks for self-attention. In other words, we use\\nmasking to control how much context the token should attend to when computing its contextualized\\n2'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 2}, page_content='x1 x2 x3 x4 x5\\nTransformer\\nTransformer\\nTransformer\\nPosition Embedding\\nToken Embedding\\nSegment Embedding\\nTransformer Block 1\\nTransformer Block 2\\nTransformer Block L\\n...\\nh1 h2 h3 h4 h5\\nTransformer\\nTransformer\\nTransformer\\nSOS S1 EOS S2 EOS\\nSOS S1 S1 S1 EOS\\nSOS S1 EOS S2 EOS\\nUnified LM with \\nShared Parameters\\nSelf-attention Masks\\nSegment 1 Segment 2\\nSegment 1 Segment 2\\nSegment 1\\nLeft-to-Right LM\\nS1&S2: attend to all tokens\\nS1: attend to left context\\nS1: attend to S1 tokens\\nS2: attend to left context\\nS1\\nS2\\nS1 S2\\nS1\\nS2\\nS1 S2\\nAllow to attend\\nPrevent from attending\\nFigure 1: Overview of uniﬁed LM pre-training. The model parameters are shared across the LM\\nobjectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM). We use different\\nself-attention masks to control the access to context for each word token. The right-to-left LM is\\nsimilar to the left-to-right one, which is omitted in the ﬁgure for brevity.\\nrepresentation. Once UNILM is pretrained, we can ﬁne-tune it using task-speciﬁc data for downstream\\ntasks.\\n2.1 Input Representation\\nThe input xis a word sequence, which is either a text segment for unidirectional LMs or a pair of\\nsegments packed together for bidirectional LM and sequence-to-sequence LM. We always add a\\nspecial start-of-sequence ([SOS]) token at the beginning of input, and a special end-of-sequence\\n([EOS]) token at the end of each segment.[EOS] not only marks the sentence boundary in NLU tasks,\\nbut also is used for the model to learn when to terminate the decoding process in NLG tasks. The input\\nrepresentation follows that of BERT [9]. Texts are tokenized to subword units by WordPiece [48].\\nFor each input token, its vector representation is computed by summing the corresponding token\\nembedding, position embedding, and segment embedding. Since UNILM is trained using multiple\\nLM tasks, segment embeddings also play a role of LM identiﬁer in that we use different segment\\nembeddings for different LM objectives.\\n2.2 Backbone Network: Multi-Layer Transformer\\nThe input vectors {xi}|x|\\ni=1 is ﬁrst packed into H0 = [x1,··· ,x|x|], and then encoded into contextual\\nrepresentations at different levels of abstract Hl = [hl\\n1,··· ,hl\\n|x|] using an L-layer Transformer\\nHl = Transformerl(Hl−1),l ∈[1,L]. In each Transformer block, multiple self-attention heads are\\nused to aggregate the output vectors of the previous layer. For the l-th Transformer layer, the output\\n3'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 3}, page_content='of a self-attention head Al is computed via:\\nQ = Hl−1WQ\\nl , K = Hl−1WK\\nl , V = Hl−1WV\\nl (1)\\nMij =\\n{0, allow to attend\\n−∞, prevent from attending (2)\\nAl = softmax(QK⊺\\n√dk\\n+ M)Vl (3)\\nwhere the previous layer’s outputHl−1 ∈R|x|×dh is linearly projected to a triple of queries, keys\\nand values using parameter matrices WQ\\nl ,WK\\nl ,WV\\nl ∈Rdh×dk , respectively, and the mask matrix\\nM ∈R|x|×|x|determines whether a pair of tokens can be attended to each other.\\nWe use different mask matrices M to control what context a token can attend to when computing its\\ncontextualized representation, as illustrated in Figure 1. Take bidirectional LM as an example. The\\nelements of the mask matrix are all 0s, indicating that all the tokens have access to each other.\\n2.3 Pre-training Objectives\\nWe pretrain UNILM using four cloze tasks designed for different language modeling objectives.\\nIn a cloze task, we randomly choose some WordPiece tokens in the input, and replace them with\\nspecial token [MASK]. Then, we feed their corresponding output vectors computed by the Transformer\\nnetwork into a softmax classiﬁer to predict the masked token. The parameters of UNILM are learned\\nto minimize the cross-entropy loss computed using the predicted tokens and the original tokens. It is\\nworth noting that the use of cloze tasks makes it possible to use the same training procedure for all\\nLMs, unidirectional and bidirectional alike.\\nUnidirectional LM We use both left-to-right and right-to-left LM objectives. Take the left-to-right\\nLM as an example. The representation of each token encodes only the leftward context tokens\\nand itself. For instance, to predict the masked token of “x1x2 [MASK] x4”, only tokens x1,x2 and\\nitself can be used. This is done by using a triangular matrix for the self-attention mask M (as in\\nEquation (2)), where the upper triangular part of the self-attention mask is set to −∞, and the other\\nelements to 0, as shown in Figure 1. Similarly, a right-to-left LM predicts a token conditioned on its\\nfuture (right) context.\\nBidirectional LM Following [9], a bidirectional LM allows all tokens to attend to each other in\\nprediction. It encodes contextual information from both directions, and can generate better contextual\\nrepresentations of text than its unidirectional counterpart. As indicated in Equation (2), the self-\\nattention mask M is a zero matrix, so that every token is allowed to attend across all positions in the\\ninput sequence.\\nSequence-to-Sequence LM As shown in Figure 1, for prediction, the tokens in the ﬁrst (source)\\nsegment can attend to each other from both directions within the segment, while the tokens of the\\nsecond (target) segment can only attend to the leftward context in the target segment and itself, as\\nwell as all the tokens in the source segment. For example, given source segment t1t2 and its target\\nsegment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1\\nand t2 have access to the ﬁrst four tokens, including [SOS] and [EOS], t4 can only attend to the ﬁrst\\nsix tokens.\\nFigure 1 shows the self-attention mask M used for the sequence-to-sequence LM objective. The left\\npart of M is set to 0 so that all tokens can attend to the ﬁrst segment. The upper right part is set to\\n−∞to block attentions from the source segment to the target segment. Moreover, for the lower right\\npart, we set its upper triangular part to −∞, and the other elements to 0, which prevents tokens in the\\ntarget segment from attending their future (right) positions.\\nDuring training, we randomly choose tokens in both segments, and replace them with the special\\ntoken [MASK]. The model is learned to recover the masked tokens. Since the pair of source and target\\ntexts are packed as a contiguous input text sequence in training, we implicitly encourage the model\\nto learn the relationship between the two segments. In order to better predict tokens in the target\\nsegment, UNILM learns to effectively encode the source segment. Thus, the cloze task designed for\\n4'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 4}, page_content='the sequence-to-sequence LM, also known as the encoder-decoder model, simultaneously pre-trains\\na bidirectional encoder and an unidirectional decoder. The pre-trained model, used as an encoder-\\ndecoder model, can be easily adapted to a wide range of conditional text generation tasks, such as\\nabstractive summarization.\\nNext Sentence Prediction For the bidirectional LM, we also include the next sentence prediction\\ntask for pre-training, as in [9].\\n2.4 Pre-training Setup\\nThe overall training objective the sum of different types of LM objectives described above. Specif-\\nically, within one training batch, 1/3 of the time we use the bidirectional LM objective, 1/3 of\\nthe time we employ the sequence-to-sequence LM objective, and both left-to-right and right-to-left\\nLM objectives are sampled with rate of 1/6. The model architecture of UNILM follows that of\\nBERTLARGE [9] for a fair comparison. The gelu activation [18] is used as GPT [31]. Speciﬁcally, we\\nuse a 24-layer Transformer with 1,024 hidden size, and 16 attention heads, which contains about\\n340M parameters. The weight matrix of the softmax classiﬁer is tied with token embeddings.UNILM\\nis initialized by BERTLARGE, and then pre-trained using English Wikipedia2 and BookCorpus [53],\\nwhich have been processed in the same way as [9]. The vocabulary size is 28,996. The maximum\\nlength of input sequence is 512. The token masking probability is 15%. Among masked positions,\\n80% of the time we replace the token with [MASK], 10% of the time with a random token, and\\nkeeping the original token for the rest. In addition, 80% of the time we randomly mask one token\\neach time, and 20% of the time we mask a bigram or a trigram.\\nAdam [22] with β1 = 0.9, β2 = 0.999 is used for optimization. The learning rate is 3e-5, with linear\\nwarmup over the ﬁrst 40,000 steps and linear decay. The dropout rate is 0.1. The weight decay is\\n0.01. The batch size is 330. The pre-training procedure runs for about 770,000 steps. It takes about\\n7 hours for 10,000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.\\n2.5 Fine-tuning on Downstream NLU and NLG Tasks\\nFor NLU tasks, we ﬁne-tune UNILM as a bidirectional Transformer encoder, like BERT. Take text\\nclassiﬁcation as an example. We use the encoding vector of [SOS] as the representation of input,\\ndenoted as hL\\n1 , and feed it to a randomly initialized softmax classiﬁer (i.e., the task-speciﬁc output\\nlayer), where the class probabilities are computed as softmax(hL\\n1 WC), where WC ∈Rdh×C is\\na parameter matrix, and C the number of categories. We maximize the likelihood of the labeled\\ntraining data by updating the parameters of the pre-trained LM and the added softmax classiﬁer.\\nFor NLG tasks, we take the sequence-to-sequence task as an example. The ﬁne-tuning procedure is\\nsimilar to pre-training using the self-attention masks as in Section 2.3. Let S1 and S2 denote source\\nand target sequences, respectively. We pack them together with special tokens, to form the input\\n“[SOS] S1 [EOS] S2 [EOS]”. The model is ﬁne-tuned by masking some percentage of tokens in the\\ntarget sequence at random, and learning to recover the masked words. The training objective is to\\nmaximize the likelihood of masked tokens given context. It is worth noting that [EOS], which marks\\nthe end of the target sequence, can also be masked during ﬁne-tuning, thus when this happens, the\\nmodel learns when to emit [EOS] to terminate the generation process of the target sequence.\\n3 Experiments\\nWe have conducted experiments on both NLU (i.e., the GLUE benchmark, and extractive question\\nanswering) and NLG tasks (i.e., abstractive summarization, question generation, generative question\\nanswering, and dialog response generation).\\n3.1 Abstractive Summarization\\nAutomatic text summarization produces a concise and ﬂuent summary conveying the key information\\nin the input (e.g., a news article). We focus on abstractive summarization, a generation task where\\n2Wikipedia version: enwiki-20181101.\\n5'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 5}, page_content='RG-1 RG-2 RG-L\\nExtractive Summarization\\nLEAD-3 40.42 17.62 36.67\\nBest Extractive [27] 43.25 20.24 39.63\\nAbstractive Summarization\\nPGNet [37] 39.53 17.28 37.98\\nBottom-Up [16] 41.22 18.68 38.34\\nS2S-ELMo [13] 41.56 18.94 38.47\\nUNILM 43.33 20.21 40.51\\nTable 3: Evaluation results on CNN/DailyMail\\nsummarization. Models in the ﬁrst block are ex-\\ntractive systems listed here for reference, while\\nthe others are abstractive models. The results\\nof the best reported extractive model are taken\\nfrom [27]. RG is short for ROUGE.\\nRG-1 RG-2 RG-L\\n10K Training Examples\\nTransformer [43] 10.97 2.23 10.42\\nMASS [39] 25.03 9.48 23.48\\nUNILM 32.96 14.68 30.56\\nFull Training Set\\nOpenNMT [23] 36.73 17.86 33.68\\nRe3Sum [4] 37.04 19.03 34.46\\nMASS [39] 37.66 18.53 34.89\\nUNILM 38.45 19.45 35.75\\nTable 4: Results on Gigaword abstractive summa-\\nrization. Models in the ﬁrst block only use 10K\\nexamples for training, while the others use 3.8M\\nexamples. Results of OpenNMT and Transformer\\nare taken from [4, 39]. RG is short for ROUGE.\\nthe summary is not constrained to reusing the phrases or sentences in the input text. We use the\\nnon-anonymized version of the CNN/DailyMail dataset [37] and Gigaword [36] for model ﬁne-tuning\\nand evaluation. We ﬁne-tune UNILM as a sequence-to-sequence model following the procedure\\ndescribed in Section 2.5 by concatenating document (the ﬁrst segment) and summary (the second\\nsegment) as input which is truncated according to a pre-deﬁned maximum length.\\nWe ﬁne-tune our model on the training set for 30 epochs. We reuse most hyper-parameters from\\npre-training. The masking probability is 0.7. We also use label smoothing [40] with rate of 0.1. For\\nCNN/DailyMail, we set batch size to 32, and maximum length to 768. For Gigaword, we set batch\\nsize to 64, and maximum length to 256. During decoding, we use beam search with beam size of 5.\\nThe input document is truncated to the ﬁrst 640 and 192 tokens for CNN/DailyMail and Gigaword,\\nrespectively. We remove duplicated trigrams in beam search, and tweak the maximum summary\\nlength on the development set [28, 13].\\nWe use the F1 version of ROUGE [25] as the evaluation metric for both datasets. In Table 3, we\\ncompare UNILM against the baseline and several state-of-the-art models on CNN/DailyMail. LEAD-\\n3 is a baseline model that extracts the ﬁrst three sentences in a document as its summary. PGNet [37]\\nis a sequence-to-sequence model based on the pointer-generator network. S2S-ELMo [ 13] uses a\\nsequence-to-sequence model augmented with pre-trained ELMo representations, which is termed as\\nSRC-ELMO+SHDEMB in [13]. Bottom-Up [16] is a sequence-to-sequence model augmented with a\\nbottom-up content selector for selecting salient phrases. We also include in Table 3 the best reported\\nextractive summarization result [27] on the dataset. As shown in Table 3, our model outperforms all\\nprevious abstractive systems, creating a new state-of-the-art abstractive summarization result on the\\ndataset. Our model also outperforms the best extractive model [27] by 0.88 point in ROUGE-L.\\nIn Table 4, we evaluate the models on Gigaword with different scales (10K and 3.8M). Both\\nTransformer [43] and OpenNMT [23] implement standard attentional sequence-to-sequence models.\\nRe3Sum [4] retrieves summaries as candidate templates, and then use an extended sequence-to-\\nsequence model to generate summaries. MASS [ 39] is a pre-trained sequence-to-sequence model\\nbased on Transformer networks. Experimental results show that UNILM achieves better performance\\nthan previous work. Besides, in the low-resource setting (i.e., only 10,000 examples are used as\\ntraining data), our model outperforms MASS by 7.08 point in ROUGE-L.\\n3.2 Question Answering (QA)\\nThe task is to answer a question given a passage [ 33, 34, 15]. There are two settings. The ﬁrst is\\ncalled extractive QA, where the answer is assumed to be a text span in the passage. The other is\\ncalled generative QA, where the answer needs to be generated on the ﬂy.\\nExtractive QA This task can be formulated as a NLU task where we need to predict the start and\\nend positions of the answer spans within the passage. We ﬁne-tune the pre-trained UNILM as a\\n6'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 6}, page_content='EM F1\\nRMR+ELMo [20] 71.4 73.7\\nBERTLARGE 78.9 81.8\\nUNILM 80.5 83.4\\nTable 5: Extractive QA results on\\nthe SQuAD development set.\\nF1\\nDrQA+ELMo [35] 67.2\\nBERTLARGE 82.7\\nUNILM 84.9\\nTable 6: Extractive QA results\\non the CoQA development set.\\nF1\\nSeq2Seq [35] 27.5\\nPGNet [35] 45.4\\nUNILM 82.5\\nTable 7: Generative QA results\\non the CoQA development set.\\nbidirectional encoder for the task. We conduct experiments on the Stanford Question Answering\\nDataset (SQuAD) 2.0 [34], and Conversational Question Answering (CoQA) [35] datasets.\\nThe results on SQuAD 2.0 are reported in Table 5, where we compare two models in Exact Match\\n(EM) and F1 score. RMR+ELMo [20] is an LSTM-based question answering model augmented with\\npre-trained language representation. BERTLARGE is a cased model, ﬁne-tuned on the SQuAD training\\ndata for 3 epochs, with batch size 24, and maximum length 384. UNILM is ﬁne-tuned in the same\\nway as BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nCoQA is a conversational question answering dataset. Compared with SQuAD, CoQA has several\\nunique characteristics. First, the examples in CoQA are conversational, so we need to answer the\\ninput question based on conversation histories. Second, the answers in CoQA can be free-form texts,\\nincluding a large portion is of yes/no answers.\\nWe modify the model used for SQuAD as follows. Firstly, in addition to the asked question,\\nwe concatenate the question-answer histories to the ﬁrst segment, so that the model can capture\\nconversational information. Secondly, for yes/no questions, we use the ﬁnal hidden vector of the\\n[SOS] token to predict whether the input is a yes/no question, and whether the answer is yes or no.\\nFor other examples, we select a passage subspan with the highest F1 score for training.\\nThe results on CoQA are reported in Table 6, where we compare two models in F1 scores.\\nDrQA+ELMo [35] is an LSTM-based question answering model augmented with pre-trained ELMo\\nrepresentation. BERTLARGE is a cased model, ﬁne-tuned on the CoQA training data for 2 epochs,\\nwith batch size 16, and maximum length 512. UNILM is ﬁne-tuned with the same hyper-parameters\\nas BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nGenerative QA Generative question answering generates free-form answers for the input question\\nand passage, which is a NLG task. In contrast, extractive methods can only predict subspans of the\\ninput passage as answers. On the CoQA dataset (as described above), Reddy et al. [2019] show that\\nvanilla sequence-to-sequence models still underperforms extractive methods by a wide margin.\\nWe adapt UNILM to generative question answering as a sequence-to-sequence model. The ﬁrst\\nsegment (i.e., the input sequence) is the concatenation of conversational histories, the input question\\nand the passage. The second segment (i.e., the output sequence) is the answer. We ﬁne-tune the\\npre-trained UNILM on the CoQA training set for 10 epochs. We set the batch size to 32, the mask\\nprobability to 0.5, and the maximum length to 512. We also use label smoothing with rate of 0.1.\\nThe other hyper-parameters are kept the same as pre-training. During decoding, we use beam search\\nwith beam size of 3. The maximum length of input question and passage is 470. For passages that\\nare longer than the maximum length, we split the passage into several chunks with a sliding window\\napproach, and select a chunk with the highest word overlap over the question.\\nWe compare our method with the generative question answering models Seq2Seq and PGNet as\\ndescribed in [35]. The Seq2Seq baseline is a sequence-to-sequence model with an attention mech-\\nanism. The PGNet model augments Seq2Seq with a copy mechanism. As shown in Table 7, our\\ngenerative question answering model outperforms previous generative methods by a wide margin,\\nwhich signiﬁcantly closes the gap between generative method and extractive method.\\n3.3 Question Generation\\nWe conduct experiments for the answer-aware question generation task [52]. Given an input passage\\nand an answer span, our goal is to generate a question that asks for the answer. The SQuAD 1.1\\ndataset [33] is used for evaluation. Following [12], we split the original training set into training and\\n7'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 7}, page_content='BLEU-4 MTR RG-L\\nCorefNQG [11] 15.16 19.12 -\\nSemQG [50] 18.37 22.65 46.68\\nUNILM 22.12 25.06 51.07\\nMP-GSN [51] 16.38 20.25 44.48\\nSemQG [50] 20.76 24.20 48.91\\nUNILM 23.75 25.61 52.04\\nTable 8: Question generation results on SQuAD.\\nMTR is short for METEOR, and RG for ROUGE.\\nResults in the groups use different data splits.\\nEM F1\\nUNILM QA Model (Section 3.2) 80.5 83.4\\n+ UNILM Generated Questions 84.7 87.6\\nTable 9: Question generation based on UNILM\\nimproves question answering results on the\\nSQuAD development set.\\nNIST-4 BLEU-4 METEOR Entropy-4 Div-1 Div-2 Avg len\\nBest System in DSTC7 Shared Task 2.523 1.83 8.07 9.030 0.109 0.325 15.133\\nUNILM 2.669 4.39 8.27 9.195 0.120 0.391 14.807\\nHuman Performance 2.650 3.13 8.31 10.445 0.167 0.670 18.76\\nTable 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams,\\nrespectively.\\ntest sets, and keep the original development set. We also conduct experiments following the data split\\nas in [51], which uses the reversed dev-test split.\\nThe question generation task is formulated as a sequence-to-sequence problem. The ﬁrst segment is\\nthe concatenation of input passage and answer, while the second segment is the generated question.\\nWe ﬁne-tune UNILM on the training set for 10 epochs. We set batch size to 32, masking probability\\nto 0.7, and learning rate to 2e-5. The rate of label smoothing is 0.1. The other hyper-parameters are\\nthe same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage\\nchunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are\\ncomputed by the same scripts as in [12].\\nThe results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with\\nattention and a feature-rich encoder. MP-GSN [51] uses an attention-based sequence-to-sequence\\nmodel with a gated self-attention encoder. SemQG [ 50] uses two semantics-enhanced rewards to\\nregularize the generation. UNILM outperforms previous models and achieves a new state-of-the-art\\nfor question generation.\\nGenerated Questions Improve QA The question generation model can automatically harvest a\\nlarge number of question-passage-answer examples from a text corpus. We show that the augmented\\ndata generated by question generation improves the question answering model.\\nWe generate ﬁve million answerable examples, and four million unanswerable examples by modifying\\nthe answerable ones. We ﬁne-tune our question answering model on the generated data for one epoch.\\nThen the model is ﬁne-tuned on the SQuAD 2.0 data for two more epochs.\\nAs shown in Table 9, the augmented data generated by UNILM improves question answering model\\nintroduced in Section 3.2. Note that we use bidirectional masked language modeling as an auxiliary\\ntask for both the generated and SQuAD 2.0 datasets during ﬁne-tuning, which brings 2.3 absolute\\nimprovement compared to directly using automatically generated examples. A possible reason is that\\nthe auxiliary task alleviates catastrophic forgetting [49] when ﬁne-tuning on augmented data.\\n3.4 Response Generation\\nWe evaluate UNILM on the document-grounded dialog response generation task [30, 15]. Given a\\nmulti-turn conversation history and a web document as the knowledge source, the system needs to\\n3Notice that if we directly use the tokenized references provided by Du et al. [2017], the results are (21.63\\nBLEU-4 / 25.04 METEOR / 51.09 ROUGE-L) on the original data split [ 12], and (23.08 BLEU-4 / 25.57\\nMETEOR / 52.03 ROUGE-L) in the reversed dev-test setup [51].\\n8'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 8}, page_content='Model CoLA SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE WNLI AX Score\\nMCC Acc F1 S Corr F1 Acc Acc Acc Acc Acc\\nGPT 45.4 91.3 82.3 80.0 70.3 82.1/81.4 87.4 56.0 53.4 29.8 72.8\\nBERTLARGE 60.5 94.9 89.3 86.5 72.1 86.7/85.9 92.7 70.1 65.1 39.6 80.5\\nUNILM 61.1 94.5 90.0 87.7 71.7 87.0/85.9 92.7 70.9 65.1 38.4 80.8\\nTable 11: GLUE test set results scored using the GLUE evaluation server.\\ngenerate a natural language response that is both conversationally appropriate and reﬂective of the\\ncontents of the web document. We ﬁne-tune UNILM to the task as a sequence-to-sequence model.\\nThe ﬁrst segment (input sequence) is the concatenation of the web document and the conversation\\nhistory. The second segment (output sequence) is the response. We ﬁne-tune UNILM on the DSTC7\\ntraining data for 20 epochs, with batch size 64. The masking probability is set to 0.5. The maximum\\nlength is 512. During decoding, we use beam search with size of 10. The maximum length of\\ngenerated response is set to 40. As shown in Table 10, UNILM outperforms the best system [41] in\\nthe DSTC7 shared task [14] across all evaluation metrics.\\n3.5 GLUE Benchmark\\nWe evaluate UNILM on the General Language Understanding Evaluation (GLUE) benchmark [45].\\nGLUE is a collection of nine language understanding tasks, including question answering [ 33],\\nlinguistic acceptability [46], sentiment analysis [38], text similarity [5], paraphrase detection [10],\\nand natural language inference (NLI) [7, 2, 17, 3, 24, 47].\\nOur model is ﬁne-tuned as a bidirectional LM. We use Adamax [21] as our optimizer with a learning\\nrate of 5e-5 and a batch size of 32. The maximum number of epochs is set to 5. A linear learning rate\\ndecay schedule with warmup of 0.1 is used. The dropout rate of the last linear projection for each\\ntask is set to 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. To avoid the gradient explosion\\nissue, the gradient norm was clipped within 1. We truncated the tokens no longer than 512.\\nTable 11 presents the GLUE test results obtained from the benchmark evaluation server. The\\nresults show that UNILM obtains comparable performance on the GLUE tasks in comparison with\\nBERTLARGE.\\n4 Conclusion and Future Work\\nWe propose a uniﬁed pre-training model, UNILM, which is jointly optimized for several LM\\nobjectives with shared parameters. The uniﬁcation of bidirectional, unidirectional, and sequence-\\nto-sequence LMs enables us to straightforwardly ﬁne-tune the pre-trained UNILM for both NLU\\nand NLG tasks. Experimental results demonstrate that our model compares favorably with BERT\\non the GLUE benchmark and two question answering datasets. In addition, UNILM outperforms\\nprevious state-of-the-art models on ﬁve NLG datasets: CNN/DailyMail and Gigaword abstractive\\nsummarization, SQuAD question generation, CoQA generative question answering, and DSTC7\\ndialog response generation.\\nThe work can be advanced from the following perspectives:\\n• We will push the limit of the current method by training more epochs and larger models on web-\\nscale text corpora. At the same time, we will also conduct more experiments on end applications\\nas well as ablation experiments to investigate the model capability and the beneﬁts of pre-training\\nmultiple language modeling tasks with the same network.\\n• We are focusing on monolingual NLP tasks in our current experiments. We are also interested in\\nextending UNILM to support cross-lingual tasks [6].\\n• We will conduct multi-task ﬁne-tuning on both NLU and NLG tasks, which is a natural extension\\nof Multi-Task Deep Neural Network (MT-DNN) [26].\\nAcknowledgement We would like to acknowledge Shiyue Zhang for the helpful discussions about\\nthe question generation experiments.\\n9'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 9}, page_content='References\\n[1] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven\\npretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019.\\n[2] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second\\nPASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL\\nChallenges Workshop on Recognising Textual Entailment, 01 2006.\\n[3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini.\\nThe ﬁfth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference\\n(TAC-09), 2009.\\n[4] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, pages 152–161, Melbourne, Australia, July 2018. Association for\\nComputational Linguistics.\\n[5] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055, 2017.\\n[6] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. Cross-\\nlingual natural language generation via pre-training. ArXiv, abs/1909.10481, 2019.\\n[7] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entail-\\nment challenge. In Proceedings of the First International Conference on Machine Learning\\nChallenges: Evaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing\\nTextual Entailment, MLCW’05, pages 177–190, Berlin, Heidelberg, 2006. Springer-Verlag.\\n[8] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems 28, pages 3079–3087. Curran Associates, Inc., 2015.\\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.\\n[10] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005) ,\\n2005.\\n[11] Xinya Du and Claire Cardie. Harvesting paragraph-level question-answer pairs from Wikipedia.\\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,\\npages 1907–1917, Melbourne, Australia, July 2018. Association for Computational Linguistics.\\n[12] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for\\nreading comprehension. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long\\nPapers, pages 1342–1352, 2017.\\n[13] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations\\nfor language generation. CoRR, abs/1903.09722, 2019.\\n[14] Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. Grounded response\\ngeneration task at dstc7. In AAAI Dialog System Technology Challenges Workshop, 2019.\\n[15] Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Founda-\\ntions and Trends in Information Retrieval, 13(2-3):127–298, 2019.\\n[16] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\\npages 4098–4109, Brussels, Belgium, October-November 2018. Association for Computational\\nLinguistics.\\n10'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 10}, page_content='[17] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL\\nrecognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop\\non Textual Entailment and Paraphrasing , pages 1–9, Prague, June 2007. Association for\\nComputational Linguistics.\\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\\narXiv:1606.08415, 2016.\\n[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics.\\n[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify:\\nMachine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018.\\n[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd\\nInternational Conference on Learning Representations, San Diego, CA, 2015.\\n[23] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT:\\nOpen-source toolkit for neural machine translation. In Proceedings of ACL 2017, System\\nDemonstrations, pages 67–72, Vancouver, Canada, July 2017. Association for Computational\\nLinguistics.\\n[24] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\\nIn Thirteenth International Conference on the Principles of Knowledge Representation and\\nReasoning, 2012.\\n[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\\ntion Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July\\n2004. Association for Computational Linguistics.\\n[26] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\\nfor natural language understanding. CoRR, abs/1901.11504, 2019.\\n[27] Yang Liu. Fine-tune BERT for extractive summarization. CoRR, abs/1903.10318, 2019.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. CoRR, abs/1705.04304, 2018.\\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of\\nthe 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, pages 2227–2237, New Orleans, Louisiana, June\\n2018. Association for Computational Linguistics.\\n[30] Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin\\nChoi, and Jianfeng Gao. Conversing by reading: Contentful neural conversation with on-\\ndemand machine reading. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 5427–5436, Florence, Italy, July 2019. Association for\\nComputational Linguistics.\\n[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. 2018.\\n[32] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. 2019.\\n[33] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ ques-\\ntions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016.\\nAssociation for Computational Linguistics.\\n11'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 11}, page_content='[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\\nquestions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short\\nPapers, pages 784–789, 2018.\\n[35] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question\\nanswering challenge. Transactions of the Association for Computational Linguistics, 7:249–266,\\nMarch 2019.\\n[36] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\\nsentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in\\nNatural Language Processing, pages 379–389, Lisbon, Portugal, September 2015. Association\\nfor Computational Linguistics.\\n[37] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, pages 1073–1083, Vancouver, Canada, July 2017. Association for\\nComputational Linguistics.\\n[38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language\\nprocessing, pages 1631–1642, 2013.\\n[39] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to\\nsequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\\n[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\\nthinking the inception architecture for computer vision. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 2818–2826, 2016.\\n[41] Y Tam, Jiachen Ding, Cheng Niu, and Jie Zhou. Cluster-based beam search for pointer-generator\\nchatbot grounded by knowledge. In AAAI Dialog System Technology Challenges Workshop,\\n2019.\\n[42] Wilson L Taylor. Cloze procedure: A new tool for measuring readability. Journalism Bulletin,\\n30(4):415–433, 1953.\\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\\ntion Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017.\\n[44] Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov\\nrandom ﬁeld language model. CoRR, abs/1902.04094, 2019.\\n[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\\nInternational Conference on Learning Representations, 2019.\\n[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability\\njudgments. arXiv preprint arXiv:1805.12471, 2018.\\n[47] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational\\nLinguistics.\\n[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah,\\nMelvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,\\nHideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason\\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey\\nDean. Google’s neural machine translation system: Bridging the gap between human and\\nmachine translation. CoRR, abs/1609.08144, 2016.\\n12'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 12}, page_content='[49] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike\\nChrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and\\nPhil Blunsom. Learning and evaluating general linguistic intelligence. arXiv preprint\\narXiv:1901.11373, 2019.\\n[50] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. CoRR, abs/1909.06356, 2019.\\n[51] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question\\ngeneration with maxout pointer and gated self-attention networks. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 3901–3910, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics.\\n[52] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question\\ngeneration from text: A preliminary study. In Xuanjing Huang, Jing Jiang, Dongyan Zhao,\\nYansong Feng, and Yu Hong, editors,Natural Language Processing and Chinese Computing,\\npages 662–671. Springer International Publishing, 2018.\\n[53] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. In Proceedings of the IEEE International Conference on\\nComputer Vision, pages 19–27, 2015.\\nAppendix A Long Text Generation: A Case Study\\nOur model can generate text samples using the left-to-right setting. We picked three text samples\\nsampled from left to right using our model, as shown in Table 12. We use the top- 40 truncating\\nsampling strategy [ 32], and forbid duplicate 4-grams during generation. For each example, we\\nsampled 10 times from the same input and we hand-picked the best one; as such, these samples\\nshould be considered to be better than the average model output. From the examples, we ﬁnd that the\\nmodel can produce ﬂuent output with somewhat consistent contents which ﬁts the inputs’ genres and\\ntopics. In the ﬁrst example, given a modiﬁed excerpt from the novel “ 1984” as input, the model’s\\noutput is like a short paragraph in an fantasy novel; for the other two with input from Wikipedia, the\\nmodel makes up “facts” with plausible names, time stamps and events written in the Wikipedia style.\\nAppendix B GLUE Benchmark\\nAs shown in Table 13, we summarize the data size and the evaluation metrics used for the General\\nLanguage Understanding Evaluation (GLUE) benchmark.\\n13'),\n",
       " Document(metadata={'source': 'data/ULM.pdf', 'page': 13}, page_content='Input Output\\nWinston sat back. A\\nsense of complete\\nhelplessness had\\ndescended upon him.\\nWinston sat back. A sense of complete helplessness had descended\\nupon him. He was now helpless and unmoving. What would that\\nmean? To become a monk under her mother’s rule. What would it\\nmean for him to be put at the center of the most dangerous of all of\\ndemons, the demon known as Azrael? He would never know exactly\\nwhat that was, or even how it would end, unless he could sense the\\nstrength of Azrael’s mind and soul. It wasn’t only the existence of\\nhis mother, it was all he knew, and every moment. He was trapped\\nwithin her embrace, a trapped, torn, battered, crippling presence he\\nonly knew it would mean was eternity. And in the instant that he\\nwas lost his life changed forever, when Azraelis’ spirit touched his\\nmind, he was engulfed in this dark and mystical, so enchanting and\\nbeautiful force. (...)\\nBert is a yellow\\nMuppet character\\non the long running\\nchildren’s television\\nshow, Sesame Street.\\nBert is a yellow Muppet character on the long running children’s\\ntelevision show, Sesame Street. He would typically appear as a\\nsidekick or as a villain in the show; he is usually portrayed as\\na cowardly and untrustworthy ﬁgure that tends to intrude on the\\nnormal life of the street, such as “The Story of the Mole” , “The\\nStory from the Side”, and “Bump the Bush”. His voice was provided\\nby Michael Combs. Bert was a regular cast member of Sesame\\nStreet, and his appearance was always played throughout the series\\nby two of his regular cast members, Frank Laughner and Michael\\nCombs. (...)\\nMaurice Ravel (7\\nMarch 1875 - 28\\nDecember 1937) was\\na French composer,\\npianist and conductor.\\nMaurice Ravel (7 March 1875 - 28 December 1937) was a French\\ncomposer, pianist and conductor.He was born in Paris, and studied\\npiano with Eugene Massenet. In the early 1900s Ravel became\\ninterested in the composition of music, and he composed much of\\nhis early music for the piano. Ravel’s early work is marked by a\\nmarked sense of lyricism, but in the later years of his career he\\ndeveloped a strong preference for orchestral forms. His ﬁrst work,\\n“le Grand Cours d’ Auvergne”, was composed in 1909 by Maurice\\nRavel, a student of Jules Massenet, and was published by A.F.A.\\nin 1912. It was re-published in 1912 by the publisher J.S.D.M. de\\nl’Etablissement Musicale de la Musique Francaise. Ravel wrote\\nthe piano concerto “la Tragedie et la Chanson Dans le Theatre des\\nChamps Elysees” in 1916. (...)\\nTable 12: Text samples generated by our model using left-to-right generation.\\nCorpus #Train/#Dev/#Test Metrics\\nSingle-Sentence Classiﬁcation\\nCoLA (Acceptability) 8.5k/1k/1k Matthews corr\\nSST-2 (Sentiment) 67k/872/1.8k Accuracy\\nPairwise Text Classiﬁcation\\nMNLI (NLI) 393k/20k/20k Accuracy\\nRTE (NLI) 2.5k/276/3k Accuracy\\nQNLI (NLI) 108k/5.7k/5.7k Accuracy\\nWNLI (NLI) 634/71/146 Accuracy\\nQQP (Paraphrase) 364k/40k/391k F1 score\\nMRPC (Paraphrase) 3.7k/408/1.7k F1 score\\nText Similarity\\nSTS-B (Similarity) 7k/1.5k/1.4k Spearman corr\\nTable 13: Summary of the GLUE benchmark.\\n14')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "for page in data:\n",
    "    question_gen += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uniﬁed Language Model Pre-training for\\nNatural Language Understanding and Generation\\nLi Dong∗ Nan Yang∗ Wenhui Wang∗ Furu Wei∗† Xiaodong Liu Yu Wang\\nJianfeng Gao Ming Zhou Hsiao-Wuen Hon\\nMicrosoft Research\\n{lidong1,nanya,wenwan,fuwei}@microsoft.com\\n{xiaodl,yuwan,jfgao,mingzhou,hon}@microsoft.com\\nAbstract\\nThis paper presents a new UNIﬁed pre-trained Language Model (UNILM) that\\ncan be ﬁne-tuned for both natural language understanding and generation tasks.\\nThe model is pre-trained using three types of language modeling tasks: unidirec-\\ntional, bidirectional, and sequence-to-sequence prediction. The uniﬁed modeling\\nis achieved by employing a shared Transformer network and utilizing speciﬁc\\nself-attention masks to control what context the prediction conditions on. UNILM\\ncompares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UNILM achieves new state-of-\\nthe-art results on ﬁve natural language generation datasets, including improving\\nthe CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute\\nimprovement), the Gigaword abstractive summarization ROUGE-L to35.75 (0.86\\nabsolute improvement), the CoQA generative question answering F1 score to 82.5\\n(37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12\\n(3.75 absolute improvement), and the DSTC7 document-grounded dialog response\\ngeneration NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained\\nmodels are available at https://github.com/microsoft/unilm.\\n1 Introduction\\nLanguage model (LM) pre-training has substantially advanced the state of the art across a variety\\nof natural language processing tasks [8, 29, 19, 31, 9, 1]. Pre-trained LMs learn contextualized text\\nrepresentations by predicting words based on their context using large amounts of text data, and can\\nbe ﬁne-tuned to adapt to downstream tasks.\\nDifferent prediction tasks and training objectives have been used for pre-training LMs of different\\ntypes, as shown in Table 1. ELMo [ 29] learns two unidirectional LMs: a forward LM reads the\\ntext from left to right, and a backward LM encodes the text from right to left. GPT [ 31] uses\\na left-to-right Transformer [ 43] to predict a text sequence word-by-word. In contrast, BERT [ 9]\\nemploys a bidirectional Transformer encoder to fuse both the left and right context to predict the\\nmasked words. Although BERT signiﬁcantly improves the performance of a wide range of natural\\nlanguage understanding tasks [9], its bidirectionality nature makes it difﬁcult to be applied to natural\\nlanguage generation tasks [44].\\nIn this work we propose a new UNIﬁed pre-trained Language Model (UNILM) that can be applied to\\nboth natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is\\na multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three\\ntypes of unsupervised language modeling objectives as shown in Table 2. In particular, we design a\\n∗ Equal contribution. † Contact person.\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1905.03197v3  [cs.CL]  15 Oct 2019ELMo GPT BERT U NILM\\nLeft-to-Right LM ✓ ✓ ✓\\nRight-to-Left LM ✓ ✓\\nBidirectional LM ✓ ✓\\nSequence-to-Sequence LM ✓\\nTable 1: Comparison between language model (LM) pre-training objectives.\\nBackbone\\nNetwork\\nLM Objectives of\\nUniﬁed Pre-training What Uniﬁed LM Learns Example Downstream Tasks\\nTransformer\\nwith shared\\nparameters\\nfor all LM\\nobjectives\\nBidirectional LM Bidirectional encoding GLUE benchmark\\nExtractive question answering\\nUnidirectional LM Unidirectional decoding Long text generation\\nSequence-to-Sequence LM\\nUnidirectional decoding\\nconditioned on\\nbidirectional encoding\\nAbstractive summarization\\nQuestion generation\\nGenerative question answering\\nTable 2: The uniﬁed LM is jointly pre-trained by multiple language modeling objectives, sharing the\\nsame parameters. We ﬁne-tune and evaluate the pre-trained uniﬁed LM on various datasets, including\\nboth language understanding and generation tasks.\\nset of cloze tasks [42] where a masked word is predicted based on its context. These cloze tasks differ\\nin how the context is deﬁned. For a left-to-right unidirectional LM, the context of the masked word\\nto be predicted consists of all the words on its left. For a right-to-left unidirectional LM, the context\\nconsists of all the words on the right. For a bidirectional LM, the context consists of the words on\\nboth the right and the left [9]. For a sequence-to-sequence LM, the context of the to-be-predicted\\nword in the second (target) sequence consists of all the words in the ﬁrst (source) sequence and the\\nwords on the its left in the target sequence.\\nSimilar to BERT, the pre-trained UNILM can be ﬁne-tuned (with additional task-speciﬁc layers\\nif necessary) to adapt to various downstream tasks. But unlike BERT which is used mainly for\\nNLU tasks, UNILM can be conﬁgured, using different self-attention masks (Section 2), to aggregate\\ncontext for different types of language models, and thus can be used for both NLU and NLG tasks.\\nThe proposed UNILM has three main advantages. First, the uniﬁed pre-training procedure leads to a\\nsingle Transformer LM that uses the shared parameters and architecture for different types of LMs,\\nalleviating the need of separately training and hosting multiple LMs. Second, the parameter sharing\\nmakes the learned text representations more general because they are jointly optimized for different\\nlanguage modeling objectives where context is utilized in different ways, mitigating overﬁtting to\\nany single LM task. Third, in addition to its application to NLU tasks, the use of UNILM as a\\nsequence-to-sequence LM (Section 2.3), makes it a natural choice for NLG, such as abstractive\\nsummarization and question generation.\\nExperimental results show that our model, used as a bidirectional encoder, compares favorably with\\nBERT on the GLUE benchmark and two extractive question answering tasks (i.e., SQuAD 2.0 and\\nCoQA). In addition, we demonstrate the effectiveness of UNILM on ﬁve NLG datasets, where it\\nis used as a sequence-to-sequence model, creating new state-of-the-art results on CNN/DailyMail\\nand Gigaword abstractive summarization, SQuAD question generation, CoQA generative question\\nanswering, and DSTC7 dialog response generation.\\n2 Uniﬁed Language Model Pre-training\\nGiven an input sequence x= x1 ···x|x|, UNILM obtains a contextualized vector representation for\\neach token. As shown in Figure 1, the pre-training optimizes the shared Transformer [43] network\\nwith respect to several unsupervised language modeling objectives, namely, unidirectional LM,\\nbidirectional LM, and sequence-to-sequence LM. In order to control the access to the context of the\\nword token to be predicted, we employ different masks for self-attention. In other words, we use\\nmasking to control how much context the token should attend to when computing its contextualized\\n2x1 x2 x3 x4 x5\\nTransformer\\nTransformer\\nTransformer\\nPosition Embedding\\nToken Embedding\\nSegment Embedding\\nTransformer Block 1\\nTransformer Block 2\\nTransformer Block L\\n...\\nh1 h2 h3 h4 h5\\nTransformer\\nTransformer\\nTransformer\\nSOS S1 EOS S2 EOS\\nSOS S1 S1 S1 EOS\\nSOS S1 EOS S2 EOS\\nUnified LM with \\nShared Parameters\\nSelf-attention Masks\\nSegment 1 Segment 2\\nSegment 1 Segment 2\\nSegment 1\\nLeft-to-Right LM\\nS1&S2: attend to all tokens\\nS1: attend to left context\\nS1: attend to S1 tokens\\nS2: attend to left context\\nS1\\nS2\\nS1 S2\\nS1\\nS2\\nS1 S2\\nAllow to attend\\nPrevent from attending\\nFigure 1: Overview of uniﬁed LM pre-training. The model parameters are shared across the LM\\nobjectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM). We use different\\nself-attention masks to control the access to context for each word token. The right-to-left LM is\\nsimilar to the left-to-right one, which is omitted in the ﬁgure for brevity.\\nrepresentation. Once UNILM is pretrained, we can ﬁne-tune it using task-speciﬁc data for downstream\\ntasks.\\n2.1 Input Representation\\nThe input xis a word sequence, which is either a text segment for unidirectional LMs or a pair of\\nsegments packed together for bidirectional LM and sequence-to-sequence LM. We always add a\\nspecial start-of-sequence ([SOS]) token at the beginning of input, and a special end-of-sequence\\n([EOS]) token at the end of each segment.[EOS] not only marks the sentence boundary in NLU tasks,\\nbut also is used for the model to learn when to terminate the decoding process in NLG tasks. The input\\nrepresentation follows that of BERT [9]. Texts are tokenized to subword units by WordPiece [48].\\nFor each input token, its vector representation is computed by summing the corresponding token\\nembedding, position embedding, and segment embedding. Since UNILM is trained using multiple\\nLM tasks, segment embeddings also play a role of LM identiﬁer in that we use different segment\\nembeddings for different LM objectives.\\n2.2 Backbone Network: Multi-Layer Transformer\\nThe input vectors {xi}|x|\\ni=1 is ﬁrst packed into H0 = [x1,··· ,x|x|], and then encoded into contextual\\nrepresentations at different levels of abstract Hl = [hl\\n1,··· ,hl\\n|x|] using an L-layer Transformer\\nHl = Transformerl(Hl−1),l ∈[1,L]. In each Transformer block, multiple self-attention heads are\\nused to aggregate the output vectors of the previous layer. For the l-th Transformer layer, the output\\n3of a self-attention head Al is computed via:\\nQ = Hl−1WQ\\nl , K = Hl−1WK\\nl , V = Hl−1WV\\nl (1)\\nMij =\\n{0, allow to attend\\n−∞, prevent from attending (2)\\nAl = softmax(QK⊺\\n√dk\\n+ M)Vl (3)\\nwhere the previous layer’s outputHl−1 ∈R|x|×dh is linearly projected to a triple of queries, keys\\nand values using parameter matrices WQ\\nl ,WK\\nl ,WV\\nl ∈Rdh×dk , respectively, and the mask matrix\\nM ∈R|x|×|x|determines whether a pair of tokens can be attended to each other.\\nWe use different mask matrices M to control what context a token can attend to when computing its\\ncontextualized representation, as illustrated in Figure 1. Take bidirectional LM as an example. The\\nelements of the mask matrix are all 0s, indicating that all the tokens have access to each other.\\n2.3 Pre-training Objectives\\nWe pretrain UNILM using four cloze tasks designed for different language modeling objectives.\\nIn a cloze task, we randomly choose some WordPiece tokens in the input, and replace them with\\nspecial token [MASK]. Then, we feed their corresponding output vectors computed by the Transformer\\nnetwork into a softmax classiﬁer to predict the masked token. The parameters of UNILM are learned\\nto minimize the cross-entropy loss computed using the predicted tokens and the original tokens. It is\\nworth noting that the use of cloze tasks makes it possible to use the same training procedure for all\\nLMs, unidirectional and bidirectional alike.\\nUnidirectional LM We use both left-to-right and right-to-left LM objectives. Take the left-to-right\\nLM as an example. The representation of each token encodes only the leftward context tokens\\nand itself. For instance, to predict the masked token of “x1x2 [MASK] x4”, only tokens x1,x2 and\\nitself can be used. This is done by using a triangular matrix for the self-attention mask M (as in\\nEquation (2)), where the upper triangular part of the self-attention mask is set to −∞, and the other\\nelements to 0, as shown in Figure 1. Similarly, a right-to-left LM predicts a token conditioned on its\\nfuture (right) context.\\nBidirectional LM Following [9], a bidirectional LM allows all tokens to attend to each other in\\nprediction. It encodes contextual information from both directions, and can generate better contextual\\nrepresentations of text than its unidirectional counterpart. As indicated in Equation (2), the self-\\nattention mask M is a zero matrix, so that every token is allowed to attend across all positions in the\\ninput sequence.\\nSequence-to-Sequence LM As shown in Figure 1, for prediction, the tokens in the ﬁrst (source)\\nsegment can attend to each other from both directions within the segment, while the tokens of the\\nsecond (target) segment can only attend to the leftward context in the target segment and itself, as\\nwell as all the tokens in the source segment. For example, given source segment t1t2 and its target\\nsegment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1\\nand t2 have access to the ﬁrst four tokens, including [SOS] and [EOS], t4 can only attend to the ﬁrst\\nsix tokens.\\nFigure 1 shows the self-attention mask M used for the sequence-to-sequence LM objective. The left\\npart of M is set to 0 so that all tokens can attend to the ﬁrst segment. The upper right part is set to\\n−∞to block attentions from the source segment to the target segment. Moreover, for the lower right\\npart, we set its upper triangular part to −∞, and the other elements to 0, which prevents tokens in the\\ntarget segment from attending their future (right) positions.\\nDuring training, we randomly choose tokens in both segments, and replace them with the special\\ntoken [MASK]. The model is learned to recover the masked tokens. Since the pair of source and target\\ntexts are packed as a contiguous input text sequence in training, we implicitly encourage the model\\nto learn the relationship between the two segments. In order to better predict tokens in the target\\nsegment, UNILM learns to effectively encode the source segment. Thus, the cloze task designed for\\n4the sequence-to-sequence LM, also known as the encoder-decoder model, simultaneously pre-trains\\na bidirectional encoder and an unidirectional decoder. The pre-trained model, used as an encoder-\\ndecoder model, can be easily adapted to a wide range of conditional text generation tasks, such as\\nabstractive summarization.\\nNext Sentence Prediction For the bidirectional LM, we also include the next sentence prediction\\ntask for pre-training, as in [9].\\n2.4 Pre-training Setup\\nThe overall training objective the sum of different types of LM objectives described above. Specif-\\nically, within one training batch, 1/3 of the time we use the bidirectional LM objective, 1/3 of\\nthe time we employ the sequence-to-sequence LM objective, and both left-to-right and right-to-left\\nLM objectives are sampled with rate of 1/6. The model architecture of UNILM follows that of\\nBERTLARGE [9] for a fair comparison. The gelu activation [18] is used as GPT [31]. Speciﬁcally, we\\nuse a 24-layer Transformer with 1,024 hidden size, and 16 attention heads, which contains about\\n340M parameters. The weight matrix of the softmax classiﬁer is tied with token embeddings.UNILM\\nis initialized by BERTLARGE, and then pre-trained using English Wikipedia2 and BookCorpus [53],\\nwhich have been processed in the same way as [9]. The vocabulary size is 28,996. The maximum\\nlength of input sequence is 512. The token masking probability is 15%. Among masked positions,\\n80% of the time we replace the token with [MASK], 10% of the time with a random token, and\\nkeeping the original token for the rest. In addition, 80% of the time we randomly mask one token\\neach time, and 20% of the time we mask a bigram or a trigram.\\nAdam [22] with β1 = 0.9, β2 = 0.999 is used for optimization. The learning rate is 3e-5, with linear\\nwarmup over the ﬁrst 40,000 steps and linear decay. The dropout rate is 0.1. The weight decay is\\n0.01. The batch size is 330. The pre-training procedure runs for about 770,000 steps. It takes about\\n7 hours for 10,000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.\\n2.5 Fine-tuning on Downstream NLU and NLG Tasks\\nFor NLU tasks, we ﬁne-tune UNILM as a bidirectional Transformer encoder, like BERT. Take text\\nclassiﬁcation as an example. We use the encoding vector of [SOS] as the representation of input,\\ndenoted as hL\\n1 , and feed it to a randomly initialized softmax classiﬁer (i.e., the task-speciﬁc output\\nlayer), where the class probabilities are computed as softmax(hL\\n1 WC), where WC ∈Rdh×C is\\na parameter matrix, and C the number of categories. We maximize the likelihood of the labeled\\ntraining data by updating the parameters of the pre-trained LM and the added softmax classiﬁer.\\nFor NLG tasks, we take the sequence-to-sequence task as an example. The ﬁne-tuning procedure is\\nsimilar to pre-training using the self-attention masks as in Section 2.3. Let S1 and S2 denote source\\nand target sequences, respectively. We pack them together with special tokens, to form the input\\n“[SOS] S1 [EOS] S2 [EOS]”. The model is ﬁne-tuned by masking some percentage of tokens in the\\ntarget sequence at random, and learning to recover the masked words. The training objective is to\\nmaximize the likelihood of masked tokens given context. It is worth noting that [EOS], which marks\\nthe end of the target sequence, can also be masked during ﬁne-tuning, thus when this happens, the\\nmodel learns when to emit [EOS] to terminate the generation process of the target sequence.\\n3 Experiments\\nWe have conducted experiments on both NLU (i.e., the GLUE benchmark, and extractive question\\nanswering) and NLG tasks (i.e., abstractive summarization, question generation, generative question\\nanswering, and dialog response generation).\\n3.1 Abstractive Summarization\\nAutomatic text summarization produces a concise and ﬂuent summary conveying the key information\\nin the input (e.g., a news article). We focus on abstractive summarization, a generation task where\\n2Wikipedia version: enwiki-20181101.\\n5RG-1 RG-2 RG-L\\nExtractive Summarization\\nLEAD-3 40.42 17.62 36.67\\nBest Extractive [27] 43.25 20.24 39.63\\nAbstractive Summarization\\nPGNet [37] 39.53 17.28 37.98\\nBottom-Up [16] 41.22 18.68 38.34\\nS2S-ELMo [13] 41.56 18.94 38.47\\nUNILM 43.33 20.21 40.51\\nTable 3: Evaluation results on CNN/DailyMail\\nsummarization. Models in the ﬁrst block are ex-\\ntractive systems listed here for reference, while\\nthe others are abstractive models. The results\\nof the best reported extractive model are taken\\nfrom [27]. RG is short for ROUGE.\\nRG-1 RG-2 RG-L\\n10K Training Examples\\nTransformer [43] 10.97 2.23 10.42\\nMASS [39] 25.03 9.48 23.48\\nUNILM 32.96 14.68 30.56\\nFull Training Set\\nOpenNMT [23] 36.73 17.86 33.68\\nRe3Sum [4] 37.04 19.03 34.46\\nMASS [39] 37.66 18.53 34.89\\nUNILM 38.45 19.45 35.75\\nTable 4: Results on Gigaword abstractive summa-\\nrization. Models in the ﬁrst block only use 10K\\nexamples for training, while the others use 3.8M\\nexamples. Results of OpenNMT and Transformer\\nare taken from [4, 39]. RG is short for ROUGE.\\nthe summary is not constrained to reusing the phrases or sentences in the input text. We use the\\nnon-anonymized version of the CNN/DailyMail dataset [37] and Gigaword [36] for model ﬁne-tuning\\nand evaluation. We ﬁne-tune UNILM as a sequence-to-sequence model following the procedure\\ndescribed in Section 2.5 by concatenating document (the ﬁrst segment) and summary (the second\\nsegment) as input which is truncated according to a pre-deﬁned maximum length.\\nWe ﬁne-tune our model on the training set for 30 epochs. We reuse most hyper-parameters from\\npre-training. The masking probability is 0.7. We also use label smoothing [40] with rate of 0.1. For\\nCNN/DailyMail, we set batch size to 32, and maximum length to 768. For Gigaword, we set batch\\nsize to 64, and maximum length to 256. During decoding, we use beam search with beam size of 5.\\nThe input document is truncated to the ﬁrst 640 and 192 tokens for CNN/DailyMail and Gigaword,\\nrespectively. We remove duplicated trigrams in beam search, and tweak the maximum summary\\nlength on the development set [28, 13].\\nWe use the F1 version of ROUGE [25] as the evaluation metric for both datasets. In Table 3, we\\ncompare UNILM against the baseline and several state-of-the-art models on CNN/DailyMail. LEAD-\\n3 is a baseline model that extracts the ﬁrst three sentences in a document as its summary. PGNet [37]\\nis a sequence-to-sequence model based on the pointer-generator network. S2S-ELMo [ 13] uses a\\nsequence-to-sequence model augmented with pre-trained ELMo representations, which is termed as\\nSRC-ELMO+SHDEMB in [13]. Bottom-Up [16] is a sequence-to-sequence model augmented with a\\nbottom-up content selector for selecting salient phrases. We also include in Table 3 the best reported\\nextractive summarization result [27] on the dataset. As shown in Table 3, our model outperforms all\\nprevious abstractive systems, creating a new state-of-the-art abstractive summarization result on the\\ndataset. Our model also outperforms the best extractive model [27] by 0.88 point in ROUGE-L.\\nIn Table 4, we evaluate the models on Gigaword with different scales (10K and 3.8M). Both\\nTransformer [43] and OpenNMT [23] implement standard attentional sequence-to-sequence models.\\nRe3Sum [4] retrieves summaries as candidate templates, and then use an extended sequence-to-\\nsequence model to generate summaries. MASS [ 39] is a pre-trained sequence-to-sequence model\\nbased on Transformer networks. Experimental results show that UNILM achieves better performance\\nthan previous work. Besides, in the low-resource setting (i.e., only 10,000 examples are used as\\ntraining data), our model outperforms MASS by 7.08 point in ROUGE-L.\\n3.2 Question Answering (QA)\\nThe task is to answer a question given a passage [ 33, 34, 15]. There are two settings. The ﬁrst is\\ncalled extractive QA, where the answer is assumed to be a text span in the passage. The other is\\ncalled generative QA, where the answer needs to be generated on the ﬂy.\\nExtractive QA This task can be formulated as a NLU task where we need to predict the start and\\nend positions of the answer spans within the passage. We ﬁne-tune the pre-trained UNILM as a\\n6EM F1\\nRMR+ELMo [20] 71.4 73.7\\nBERTLARGE 78.9 81.8\\nUNILM 80.5 83.4\\nTable 5: Extractive QA results on\\nthe SQuAD development set.\\nF1\\nDrQA+ELMo [35] 67.2\\nBERTLARGE 82.7\\nUNILM 84.9\\nTable 6: Extractive QA results\\non the CoQA development set.\\nF1\\nSeq2Seq [35] 27.5\\nPGNet [35] 45.4\\nUNILM 82.5\\nTable 7: Generative QA results\\non the CoQA development set.\\nbidirectional encoder for the task. We conduct experiments on the Stanford Question Answering\\nDataset (SQuAD) 2.0 [34], and Conversational Question Answering (CoQA) [35] datasets.\\nThe results on SQuAD 2.0 are reported in Table 5, where we compare two models in Exact Match\\n(EM) and F1 score. RMR+ELMo [20] is an LSTM-based question answering model augmented with\\npre-trained language representation. BERTLARGE is a cased model, ﬁne-tuned on the SQuAD training\\ndata for 3 epochs, with batch size 24, and maximum length 384. UNILM is ﬁne-tuned in the same\\nway as BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nCoQA is a conversational question answering dataset. Compared with SQuAD, CoQA has several\\nunique characteristics. First, the examples in CoQA are conversational, so we need to answer the\\ninput question based on conversation histories. Second, the answers in CoQA can be free-form texts,\\nincluding a large portion is of yes/no answers.\\nWe modify the model used for SQuAD as follows. Firstly, in addition to the asked question,\\nwe concatenate the question-answer histories to the ﬁrst segment, so that the model can capture\\nconversational information. Secondly, for yes/no questions, we use the ﬁnal hidden vector of the\\n[SOS] token to predict whether the input is a yes/no question, and whether the answer is yes or no.\\nFor other examples, we select a passage subspan with the highest F1 score for training.\\nThe results on CoQA are reported in Table 6, where we compare two models in F1 scores.\\nDrQA+ELMo [35] is an LSTM-based question answering model augmented with pre-trained ELMo\\nrepresentation. BERTLARGE is a cased model, ﬁne-tuned on the CoQA training data for 2 epochs,\\nwith batch size 16, and maximum length 512. UNILM is ﬁne-tuned with the same hyper-parameters\\nas BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nGenerative QA Generative question answering generates free-form answers for the input question\\nand passage, which is a NLG task. In contrast, extractive methods can only predict subspans of the\\ninput passage as answers. On the CoQA dataset (as described above), Reddy et al. [2019] show that\\nvanilla sequence-to-sequence models still underperforms extractive methods by a wide margin.\\nWe adapt UNILM to generative question answering as a sequence-to-sequence model. The ﬁrst\\nsegment (i.e., the input sequence) is the concatenation of conversational histories, the input question\\nand the passage. The second segment (i.e., the output sequence) is the answer. We ﬁne-tune the\\npre-trained UNILM on the CoQA training set for 10 epochs. We set the batch size to 32, the mask\\nprobability to 0.5, and the maximum length to 512. We also use label smoothing with rate of 0.1.\\nThe other hyper-parameters are kept the same as pre-training. During decoding, we use beam search\\nwith beam size of 3. The maximum length of input question and passage is 470. For passages that\\nare longer than the maximum length, we split the passage into several chunks with a sliding window\\napproach, and select a chunk with the highest word overlap over the question.\\nWe compare our method with the generative question answering models Seq2Seq and PGNet as\\ndescribed in [35]. The Seq2Seq baseline is a sequence-to-sequence model with an attention mech-\\nanism. The PGNet model augments Seq2Seq with a copy mechanism. As shown in Table 7, our\\ngenerative question answering model outperforms previous generative methods by a wide margin,\\nwhich signiﬁcantly closes the gap between generative method and extractive method.\\n3.3 Question Generation\\nWe conduct experiments for the answer-aware question generation task [52]. Given an input passage\\nand an answer span, our goal is to generate a question that asks for the answer. The SQuAD 1.1\\ndataset [33] is used for evaluation. Following [12], we split the original training set into training and\\n7BLEU-4 MTR RG-L\\nCorefNQG [11] 15.16 19.12 -\\nSemQG [50] 18.37 22.65 46.68\\nUNILM 22.12 25.06 51.07\\nMP-GSN [51] 16.38 20.25 44.48\\nSemQG [50] 20.76 24.20 48.91\\nUNILM 23.75 25.61 52.04\\nTable 8: Question generation results on SQuAD.\\nMTR is short for METEOR, and RG for ROUGE.\\nResults in the groups use different data splits.\\nEM F1\\nUNILM QA Model (Section 3.2) 80.5 83.4\\n+ UNILM Generated Questions 84.7 87.6\\nTable 9: Question generation based on UNILM\\nimproves question answering results on the\\nSQuAD development set.\\nNIST-4 BLEU-4 METEOR Entropy-4 Div-1 Div-2 Avg len\\nBest System in DSTC7 Shared Task 2.523 1.83 8.07 9.030 0.109 0.325 15.133\\nUNILM 2.669 4.39 8.27 9.195 0.120 0.391 14.807\\nHuman Performance 2.650 3.13 8.31 10.445 0.167 0.670 18.76\\nTable 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams,\\nrespectively.\\ntest sets, and keep the original development set. We also conduct experiments following the data split\\nas in [51], which uses the reversed dev-test split.\\nThe question generation task is formulated as a sequence-to-sequence problem. The ﬁrst segment is\\nthe concatenation of input passage and answer, while the second segment is the generated question.\\nWe ﬁne-tune UNILM on the training set for 10 epochs. We set batch size to 32, masking probability\\nto 0.7, and learning rate to 2e-5. The rate of label smoothing is 0.1. The other hyper-parameters are\\nthe same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage\\nchunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are\\ncomputed by the same scripts as in [12].\\nThe results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with\\nattention and a feature-rich encoder. MP-GSN [51] uses an attention-based sequence-to-sequence\\nmodel with a gated self-attention encoder. SemQG [ 50] uses two semantics-enhanced rewards to\\nregularize the generation. UNILM outperforms previous models and achieves a new state-of-the-art\\nfor question generation.\\nGenerated Questions Improve QA The question generation model can automatically harvest a\\nlarge number of question-passage-answer examples from a text corpus. We show that the augmented\\ndata generated by question generation improves the question answering model.\\nWe generate ﬁve million answerable examples, and four million unanswerable examples by modifying\\nthe answerable ones. We ﬁne-tune our question answering model on the generated data for one epoch.\\nThen the model is ﬁne-tuned on the SQuAD 2.0 data for two more epochs.\\nAs shown in Table 9, the augmented data generated by UNILM improves question answering model\\nintroduced in Section 3.2. Note that we use bidirectional masked language modeling as an auxiliary\\ntask for both the generated and SQuAD 2.0 datasets during ﬁne-tuning, which brings 2.3 absolute\\nimprovement compared to directly using automatically generated examples. A possible reason is that\\nthe auxiliary task alleviates catastrophic forgetting [49] when ﬁne-tuning on augmented data.\\n3.4 Response Generation\\nWe evaluate UNILM on the document-grounded dialog response generation task [30, 15]. Given a\\nmulti-turn conversation history and a web document as the knowledge source, the system needs to\\n3Notice that if we directly use the tokenized references provided by Du et al. [2017], the results are (21.63\\nBLEU-4 / 25.04 METEOR / 51.09 ROUGE-L) on the original data split [ 12], and (23.08 BLEU-4 / 25.57\\nMETEOR / 52.03 ROUGE-L) in the reversed dev-test setup [51].\\n8Model CoLA SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE WNLI AX Score\\nMCC Acc F1 S Corr F1 Acc Acc Acc Acc Acc\\nGPT 45.4 91.3 82.3 80.0 70.3 82.1/81.4 87.4 56.0 53.4 29.8 72.8\\nBERTLARGE 60.5 94.9 89.3 86.5 72.1 86.7/85.9 92.7 70.1 65.1 39.6 80.5\\nUNILM 61.1 94.5 90.0 87.7 71.7 87.0/85.9 92.7 70.9 65.1 38.4 80.8\\nTable 11: GLUE test set results scored using the GLUE evaluation server.\\ngenerate a natural language response that is both conversationally appropriate and reﬂective of the\\ncontents of the web document. We ﬁne-tune UNILM to the task as a sequence-to-sequence model.\\nThe ﬁrst segment (input sequence) is the concatenation of the web document and the conversation\\nhistory. The second segment (output sequence) is the response. We ﬁne-tune UNILM on the DSTC7\\ntraining data for 20 epochs, with batch size 64. The masking probability is set to 0.5. The maximum\\nlength is 512. During decoding, we use beam search with size of 10. The maximum length of\\ngenerated response is set to 40. As shown in Table 10, UNILM outperforms the best system [41] in\\nthe DSTC7 shared task [14] across all evaluation metrics.\\n3.5 GLUE Benchmark\\nWe evaluate UNILM on the General Language Understanding Evaluation (GLUE) benchmark [45].\\nGLUE is a collection of nine language understanding tasks, including question answering [ 33],\\nlinguistic acceptability [46], sentiment analysis [38], text similarity [5], paraphrase detection [10],\\nand natural language inference (NLI) [7, 2, 17, 3, 24, 47].\\nOur model is ﬁne-tuned as a bidirectional LM. We use Adamax [21] as our optimizer with a learning\\nrate of 5e-5 and a batch size of 32. The maximum number of epochs is set to 5. A linear learning rate\\ndecay schedule with warmup of 0.1 is used. The dropout rate of the last linear projection for each\\ntask is set to 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. To avoid the gradient explosion\\nissue, the gradient norm was clipped within 1. We truncated the tokens no longer than 512.\\nTable 11 presents the GLUE test results obtained from the benchmark evaluation server. The\\nresults show that UNILM obtains comparable performance on the GLUE tasks in comparison with\\nBERTLARGE.\\n4 Conclusion and Future Work\\nWe propose a uniﬁed pre-training model, UNILM, which is jointly optimized for several LM\\nobjectives with shared parameters. The uniﬁcation of bidirectional, unidirectional, and sequence-\\nto-sequence LMs enables us to straightforwardly ﬁne-tune the pre-trained UNILM for both NLU\\nand NLG tasks. Experimental results demonstrate that our model compares favorably with BERT\\non the GLUE benchmark and two question answering datasets. In addition, UNILM outperforms\\nprevious state-of-the-art models on ﬁve NLG datasets: CNN/DailyMail and Gigaword abstractive\\nsummarization, SQuAD question generation, CoQA generative question answering, and DSTC7\\ndialog response generation.\\nThe work can be advanced from the following perspectives:\\n• We will push the limit of the current method by training more epochs and larger models on web-\\nscale text corpora. At the same time, we will also conduct more experiments on end applications\\nas well as ablation experiments to investigate the model capability and the beneﬁts of pre-training\\nmultiple language modeling tasks with the same network.\\n• We are focusing on monolingual NLP tasks in our current experiments. We are also interested in\\nextending UNILM to support cross-lingual tasks [6].\\n• We will conduct multi-task ﬁne-tuning on both NLU and NLG tasks, which is a natural extension\\nof Multi-Task Deep Neural Network (MT-DNN) [26].\\nAcknowledgement We would like to acknowledge Shiyue Zhang for the helpful discussions about\\nthe question generation experiments.\\n9References\\n[1] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven\\npretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019.\\n[2] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second\\nPASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL\\nChallenges Workshop on Recognising Textual Entailment, 01 2006.\\n[3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini.\\nThe ﬁfth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference\\n(TAC-09), 2009.\\n[4] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, pages 152–161, Melbourne, Australia, July 2018. Association for\\nComputational Linguistics.\\n[5] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055, 2017.\\n[6] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. Cross-\\nlingual natural language generation via pre-training. ArXiv, abs/1909.10481, 2019.\\n[7] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entail-\\nment challenge. In Proceedings of the First International Conference on Machine Learning\\nChallenges: Evaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing\\nTextual Entailment, MLCW’05, pages 177–190, Berlin, Heidelberg, 2006. Springer-Verlag.\\n[8] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems 28, pages 3079–3087. Curran Associates, Inc., 2015.\\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.\\n[10] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005) ,\\n2005.\\n[11] Xinya Du and Claire Cardie. Harvesting paragraph-level question-answer pairs from Wikipedia.\\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,\\npages 1907–1917, Melbourne, Australia, July 2018. Association for Computational Linguistics.\\n[12] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for\\nreading comprehension. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long\\nPapers, pages 1342–1352, 2017.\\n[13] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations\\nfor language generation. CoRR, abs/1903.09722, 2019.\\n[14] Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. Grounded response\\ngeneration task at dstc7. In AAAI Dialog System Technology Challenges Workshop, 2019.\\n[15] Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Founda-\\ntions and Trends in Information Retrieval, 13(2-3):127–298, 2019.\\n[16] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\\npages 4098–4109, Brussels, Belgium, October-November 2018. Association for Computational\\nLinguistics.\\n10[17] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL\\nrecognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop\\non Textual Entailment and Paraphrasing , pages 1–9, Prague, June 2007. Association for\\nComputational Linguistics.\\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\\narXiv:1606.08415, 2016.\\n[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics.\\n[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify:\\nMachine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018.\\n[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd\\nInternational Conference on Learning Representations, San Diego, CA, 2015.\\n[23] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT:\\nOpen-source toolkit for neural machine translation. In Proceedings of ACL 2017, System\\nDemonstrations, pages 67–72, Vancouver, Canada, July 2017. Association for Computational\\nLinguistics.\\n[24] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\\nIn Thirteenth International Conference on the Principles of Knowledge Representation and\\nReasoning, 2012.\\n[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\\ntion Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July\\n2004. Association for Computational Linguistics.\\n[26] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\\nfor natural language understanding. CoRR, abs/1901.11504, 2019.\\n[27] Yang Liu. Fine-tune BERT for extractive summarization. CoRR, abs/1903.10318, 2019.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. CoRR, abs/1705.04304, 2018.\\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of\\nthe 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, pages 2227–2237, New Orleans, Louisiana, June\\n2018. Association for Computational Linguistics.\\n[30] Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin\\nChoi, and Jianfeng Gao. Conversing by reading: Contentful neural conversation with on-\\ndemand machine reading. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 5427–5436, Florence, Italy, July 2019. Association for\\nComputational Linguistics.\\n[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. 2018.\\n[32] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. 2019.\\n[33] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ ques-\\ntions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016.\\nAssociation for Computational Linguistics.\\n11[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\\nquestions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short\\nPapers, pages 784–789, 2018.\\n[35] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question\\nanswering challenge. Transactions of the Association for Computational Linguistics, 7:249–266,\\nMarch 2019.\\n[36] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\\nsentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in\\nNatural Language Processing, pages 379–389, Lisbon, Portugal, September 2015. Association\\nfor Computational Linguistics.\\n[37] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, pages 1073–1083, Vancouver, Canada, July 2017. Association for\\nComputational Linguistics.\\n[38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language\\nprocessing, pages 1631–1642, 2013.\\n[39] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to\\nsequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\\n[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\\nthinking the inception architecture for computer vision. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 2818–2826, 2016.\\n[41] Y Tam, Jiachen Ding, Cheng Niu, and Jie Zhou. Cluster-based beam search for pointer-generator\\nchatbot grounded by knowledge. In AAAI Dialog System Technology Challenges Workshop,\\n2019.\\n[42] Wilson L Taylor. Cloze procedure: A new tool for measuring readability. Journalism Bulletin,\\n30(4):415–433, 1953.\\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\\ntion Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017.\\n[44] Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov\\nrandom ﬁeld language model. CoRR, abs/1902.04094, 2019.\\n[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\\nInternational Conference on Learning Representations, 2019.\\n[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability\\njudgments. arXiv preprint arXiv:1805.12471, 2018.\\n[47] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational\\nLinguistics.\\n[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah,\\nMelvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,\\nHideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason\\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey\\nDean. Google’s neural machine translation system: Bridging the gap between human and\\nmachine translation. CoRR, abs/1609.08144, 2016.\\n12[49] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike\\nChrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and\\nPhil Blunsom. Learning and evaluating general linguistic intelligence. arXiv preprint\\narXiv:1901.11373, 2019.\\n[50] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. CoRR, abs/1909.06356, 2019.\\n[51] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question\\ngeneration with maxout pointer and gated self-attention networks. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 3901–3910, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics.\\n[52] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question\\ngeneration from text: A preliminary study. In Xuanjing Huang, Jing Jiang, Dongyan Zhao,\\nYansong Feng, and Yu Hong, editors,Natural Language Processing and Chinese Computing,\\npages 662–671. Springer International Publishing, 2018.\\n[53] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. In Proceedings of the IEEE International Conference on\\nComputer Vision, pages 19–27, 2015.\\nAppendix A Long Text Generation: A Case Study\\nOur model can generate text samples using the left-to-right setting. We picked three text samples\\nsampled from left to right using our model, as shown in Table 12. We use the top- 40 truncating\\nsampling strategy [ 32], and forbid duplicate 4-grams during generation. For each example, we\\nsampled 10 times from the same input and we hand-picked the best one; as such, these samples\\nshould be considered to be better than the average model output. From the examples, we ﬁnd that the\\nmodel can produce ﬂuent output with somewhat consistent contents which ﬁts the inputs’ genres and\\ntopics. In the ﬁrst example, given a modiﬁed excerpt from the novel “ 1984” as input, the model’s\\noutput is like a short paragraph in an fantasy novel; for the other two with input from Wikipedia, the\\nmodel makes up “facts” with plausible names, time stamps and events written in the Wikipedia style.\\nAppendix B GLUE Benchmark\\nAs shown in Table 13, we summarize the data size and the evaluation metrics used for the General\\nLanguage Understanding Evaluation (GLUE) benchmark.\\n13Input Output\\nWinston sat back. A\\nsense of complete\\nhelplessness had\\ndescended upon him.\\nWinston sat back. A sense of complete helplessness had descended\\nupon him. He was now helpless and unmoving. What would that\\nmean? To become a monk under her mother’s rule. What would it\\nmean for him to be put at the center of the most dangerous of all of\\ndemons, the demon known as Azrael? He would never know exactly\\nwhat that was, or even how it would end, unless he could sense the\\nstrength of Azrael’s mind and soul. It wasn’t only the existence of\\nhis mother, it was all he knew, and every moment. He was trapped\\nwithin her embrace, a trapped, torn, battered, crippling presence he\\nonly knew it would mean was eternity. And in the instant that he\\nwas lost his life changed forever, when Azraelis’ spirit touched his\\nmind, he was engulfed in this dark and mystical, so enchanting and\\nbeautiful force. (...)\\nBert is a yellow\\nMuppet character\\non the long running\\nchildren’s television\\nshow, Sesame Street.\\nBert is a yellow Muppet character on the long running children’s\\ntelevision show, Sesame Street. He would typically appear as a\\nsidekick or as a villain in the show; he is usually portrayed as\\na cowardly and untrustworthy ﬁgure that tends to intrude on the\\nnormal life of the street, such as “The Story of the Mole” , “The\\nStory from the Side”, and “Bump the Bush”. His voice was provided\\nby Michael Combs. Bert was a regular cast member of Sesame\\nStreet, and his appearance was always played throughout the series\\nby two of his regular cast members, Frank Laughner and Michael\\nCombs. (...)\\nMaurice Ravel (7\\nMarch 1875 - 28\\nDecember 1937) was\\na French composer,\\npianist and conductor.\\nMaurice Ravel (7 March 1875 - 28 December 1937) was a French\\ncomposer, pianist and conductor.He was born in Paris, and studied\\npiano with Eugene Massenet. In the early 1900s Ravel became\\ninterested in the composition of music, and he composed much of\\nhis early music for the piano. Ravel’s early work is marked by a\\nmarked sense of lyricism, but in the later years of his career he\\ndeveloped a strong preference for orchestral forms. His ﬁrst work,\\n“le Grand Cours d’ Auvergne”, was composed in 1909 by Maurice\\nRavel, a student of Jules Massenet, and was published by A.F.A.\\nin 1912. It was re-published in 1912 by the publisher J.S.D.M. de\\nl’Etablissement Musicale de la Musique Francaise. Ravel wrote\\nthe piano concerto “la Tragedie et la Chanson Dans le Theatre des\\nChamps Elysees” in 1916. (...)\\nTable 12: Text samples generated by our model using left-to-right generation.\\nCorpus #Train/#Dev/#Test Metrics\\nSingle-Sentence Classiﬁcation\\nCoLA (Acceptability) 8.5k/1k/1k Matthews corr\\nSST-2 (Sentiment) 67k/872/1.8k Accuracy\\nPairwise Text Classiﬁcation\\nMNLI (NLI) 393k/20k/20k Accuracy\\nRTE (NLI) 2.5k/276/3k Accuracy\\nQNLI (NLI) 108k/5.7k/5.7k Accuracy\\nWNLI (NLI) 634/71/146 Accuracy\\nQQP (Paraphrase) 364k/40k/391k F1 score\\nMRPC (Paraphrase) 3.7k/408/1.7k F1 score\\nText Similarity\\nSTS-B (Similarity) 7k/1.5k/1.4k Spearman corr\\nTable 13: Summary of the GLUE benchmark.\\n14'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_gen\n",
    "#All the contents print in the one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We completed the extracted process, next we perform the chunking operation\n",
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Syntax to use tokentextsplitter\n",
    "splitter_ques_gen = TokenTextSplitter(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    chunk_size = 10000, #100000 - 1 chunk\n",
    "    chunk_overlap = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the splitter - inside that iam going to pass the entire documents\n",
    "chunk_ques_gen = splitter_ques_gen.split_text(question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uniﬁed Language Model Pre-training for\\nNatural Language Understanding and Generation\\nLi Dong∗ Nan Yang∗ Wenhui Wang∗ Furu Wei∗† Xiaodong Liu Yu Wang\\nJianfeng Gao Ming Zhou Hsiao-Wuen Hon\\nMicrosoft Research\\n{lidong1,nanya,wenwan,fuwei}@microsoft.com\\n{xiaodl,yuwan,jfgao,mingzhou,hon}@microsoft.com\\nAbstract\\nThis paper presents a new UNIﬁed pre-trained Language Model (UNILM) that\\ncan be ﬁne-tuned for both natural language understanding and generation tasks.\\nThe model is pre-trained using three types of language modeling tasks: unidirec-\\ntional, bidirectional, and sequence-to-sequence prediction. The uniﬁed modeling\\nis achieved by employing a shared Transformer network and utilizing speciﬁc\\nself-attention masks to control what context the prediction conditions on. UNILM\\ncompares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UNILM achieves new state-of-\\nthe-art results on ﬁve natural language generation datasets, including improving\\nthe CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute\\nimprovement), the Gigaword abstractive summarization ROUGE-L to35.75 (0.86\\nabsolute improvement), the CoQA generative question answering F1 score to 82.5\\n(37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12\\n(3.75 absolute improvement), and the DSTC7 document-grounded dialog response\\ngeneration NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained\\nmodels are available at https://github.com/microsoft/unilm.\\n1 Introduction\\nLanguage model (LM) pre-training has substantially advanced the state of the art across a variety\\nof natural language processing tasks [8, 29, 19, 31, 9, 1]. Pre-trained LMs learn contextualized text\\nrepresentations by predicting words based on their context using large amounts of text data, and can\\nbe ﬁne-tuned to adapt to downstream tasks.\\nDifferent prediction tasks and training objectives have been used for pre-training LMs of different\\ntypes, as shown in Table 1. ELMo [ 29] learns two unidirectional LMs: a forward LM reads the\\ntext from left to right, and a backward LM encodes the text from right to left. GPT [ 31] uses\\na left-to-right Transformer [ 43] to predict a text sequence word-by-word. In contrast, BERT [ 9]\\nemploys a bidirectional Transformer encoder to fuse both the left and right context to predict the\\nmasked words. Although BERT signiﬁcantly improves the performance of a wide range of natural\\nlanguage understanding tasks [9], its bidirectionality nature makes it difﬁcult to be applied to natural\\nlanguage generation tasks [44].\\nIn this work we propose a new UNIﬁed pre-trained Language Model (UNILM) that can be applied to\\nboth natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is\\na multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three\\ntypes of unsupervised language modeling objectives as shown in Table 2. In particular, we design a\\n∗ Equal contribution. † Contact person.\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1905.03197v3  [cs.CL]  15 Oct 2019ELMo GPT BERT U NILM\\nLeft-to-Right LM ✓ ✓ ✓\\nRight-to-Left LM ✓ ✓\\nBidirectional LM ✓ ✓\\nSequence-to-Sequence LM ✓\\nTable 1: Comparison between language model (LM) pre-training objectives.\\nBackbone\\nNetwork\\nLM Objectives of\\nUniﬁed Pre-training What Uniﬁed LM Learns Example Downstream Tasks\\nTransformer\\nwith shared\\nparameters\\nfor all LM\\nobjectives\\nBidirectional LM Bidirectional encoding GLUE benchmark\\nExtractive question answering\\nUnidirectional LM Unidirectional decoding Long text generation\\nSequence-to-Sequence LM\\nUnidirectional decoding\\nconditioned on\\nbidirectional encoding\\nAbstractive summarization\\nQuestion generation\\nGenerative question answering\\nTable 2: The uniﬁed LM is jointly pre-trained by multiple language modeling objectives, sharing the\\nsame parameters. We ﬁne-tune and evaluate the pre-trained uniﬁed LM on various datasets, including\\nboth language understanding and generation tasks.\\nset of cloze tasks [42] where a masked word is predicted based on its context. These cloze tasks differ\\nin how the context is deﬁned. For a left-to-right unidirectional LM, the context of the masked word\\nto be predicted consists of all the words on its left. For a right-to-left unidirectional LM, the context\\nconsists of all the words on the right. For a bidirectional LM, the context consists of the words on\\nboth the right and the left [9]. For a sequence-to-sequence LM, the context of the to-be-predicted\\nword in the second (target) sequence consists of all the words in the ﬁrst (source) sequence and the\\nwords on the its left in the target sequence.\\nSimilar to BERT, the pre-trained UNILM can be ﬁne-tuned (with additional task-speciﬁc layers\\nif necessary) to adapt to various downstream tasks. But unlike BERT which is used mainly for\\nNLU tasks, UNILM can be conﬁgured, using different self-attention masks (Section 2), to aggregate\\ncontext for different types of language models, and thus can be used for both NLU and NLG tasks.\\nThe proposed UNILM has three main advantages. First, the uniﬁed pre-training procedure leads to a\\nsingle Transformer LM that uses the shared parameters and architecture for different types of LMs,\\nalleviating the need of separately training and hosting multiple LMs. Second, the parameter sharing\\nmakes the learned text representations more general because they are jointly optimized for different\\nlanguage modeling objectives where context is utilized in different ways, mitigating overﬁtting to\\nany single LM task. Third, in addition to its application to NLU tasks, the use of UNILM as a\\nsequence-to-sequence LM (Section 2.3), makes it a natural choice for NLG, such as abstractive\\nsummarization and question generation.\\nExperimental results show that our model, used as a bidirectional encoder, compares favorably with\\nBERT on the GLUE benchmark and two extractive question answering tasks (i.e., SQuAD 2.0 and\\nCoQA). In addition, we demonstrate the effectiveness of UNILM on ﬁve NLG datasets, where it\\nis used as a sequence-to-sequence model, creating new state-of-the-art results on CNN/DailyMail\\nand Gigaword abstractive summarization, SQuAD question generation, CoQA generative question\\nanswering, and DSTC7 dialog response generation.\\n2 Uniﬁed Language Model Pre-training\\nGiven an input sequence x= x1 ···x|x|, UNILM obtains a contextualized vector representation for\\neach token. As shown in Figure 1, the pre-training optimizes the shared Transformer [43] network\\nwith respect to several unsupervised language modeling objectives, namely, unidirectional LM,\\nbidirectional LM, and sequence-to-sequence LM. In order to control the access to the context of the\\nword token to be predicted, we employ different masks for self-attention. In other words, we use\\nmasking to control how much context the token should attend to when computing its contextualized\\n2x1 x2 x3 x4 x5\\nTransformer\\nTransformer\\nTransformer\\nPosition Embedding\\nToken Embedding\\nSegment Embedding\\nTransformer Block 1\\nTransformer Block 2\\nTransformer Block L\\n...\\nh1 h2 h3 h4 h5\\nTransformer\\nTransformer\\nTransformer\\nSOS S1 EOS S2 EOS\\nSOS S1 S1 S1 EOS\\nSOS S1 EOS S2 EOS\\nUnified LM with \\nShared Parameters\\nSelf-attention Masks\\nSegment 1 Segment 2\\nSegment 1 Segment 2\\nSegment 1\\nLeft-to-Right LM\\nS1&S2: attend to all tokens\\nS1: attend to left context\\nS1: attend to S1 tokens\\nS2: attend to left context\\nS1\\nS2\\nS1 S2\\nS1\\nS2\\nS1 S2\\nAllow to attend\\nPrevent from attending\\nFigure 1: Overview of uniﬁed LM pre-training. The model parameters are shared across the LM\\nobjectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM). We use different\\nself-attention masks to control the access to context for each word token. The right-to-left LM is\\nsimilar to the left-to-right one, which is omitted in the ﬁgure for brevity.\\nrepresentation. Once UNILM is pretrained, we can ﬁne-tune it using task-speciﬁc data for downstream\\ntasks.\\n2.1 Input Representation\\nThe input xis a word sequence, which is either a text segment for unidirectional LMs or a pair of\\nsegments packed together for bidirectional LM and sequence-to-sequence LM. We always add a\\nspecial start-of-sequence ([SOS]) token at the beginning of input, and a special end-of-sequence\\n([EOS]) token at the end of each segment.[EOS] not only marks the sentence boundary in NLU tasks,\\nbut also is used for the model to learn when to terminate the decoding process in NLG tasks. The input\\nrepresentation follows that of BERT [9]. Texts are tokenized to subword units by WordPiece [48].\\nFor each input token, its vector representation is computed by summing the corresponding token\\nembedding, position embedding, and segment embedding. Since UNILM is trained using multiple\\nLM tasks, segment embeddings also play a role of LM identiﬁer in that we use different segment\\nembeddings for different LM objectives.\\n2.2 Backbone Network: Multi-Layer Transformer\\nThe input vectors {xi}|x|\\ni=1 is ﬁrst packed into H0 = [x1,··· ,x|x|], and then encoded into contextual\\nrepresentations at different levels of abstract Hl = [hl\\n1,··· ,hl\\n|x|] using an L-layer Transformer\\nHl = Transformerl(Hl−1),l ∈[1,L]. In each Transformer block, multiple self-attention heads are\\nused to aggregate the output vectors of the previous layer. For the l-th Transformer layer, the output\\n3of a self-attention head Al is computed via:\\nQ = Hl−1WQ\\nl , K = Hl−1WK\\nl , V = Hl−1WV\\nl (1)\\nMij =\\n{0, allow to attend\\n−∞, prevent from attending (2)\\nAl = softmax(QK⊺\\n√dk\\n+ M)Vl (3)\\nwhere the previous layer’s outputHl−1 ∈R|x|×dh is linearly projected to a triple of queries, keys\\nand values using parameter matrices WQ\\nl ,WK\\nl ,WV\\nl ∈Rdh×dk , respectively, and the mask matrix\\nM ∈R|x|×|x|determines whether a pair of tokens can be attended to each other.\\nWe use different mask matrices M to control what context a token can attend to when computing its\\ncontextualized representation, as illustrated in Figure 1. Take bidirectional LM as an example. The\\nelements of the mask matrix are all 0s, indicating that all the tokens have access to each other.\\n2.3 Pre-training Objectives\\nWe pretrain UNILM using four cloze tasks designed for different language modeling objectives.\\nIn a cloze task, we randomly choose some WordPiece tokens in the input, and replace them with\\nspecial token [MASK]. Then, we feed their corresponding output vectors computed by the Transformer\\nnetwork into a softmax classiﬁer to predict the masked token. The parameters of UNILM are learned\\nto minimize the cross-entropy loss computed using the predicted tokens and the original tokens. It is\\nworth noting that the use of cloze tasks makes it possible to use the same training procedure for all\\nLMs, unidirectional and bidirectional alike.\\nUnidirectional LM We use both left-to-right and right-to-left LM objectives. Take the left-to-right\\nLM as an example. The representation of each token encodes only the leftward context tokens\\nand itself. For instance, to predict the masked token of “x1x2 [MASK] x4”, only tokens x1,x2 and\\nitself can be used. This is done by using a triangular matrix for the self-attention mask M (as in\\nEquation (2)), where the upper triangular part of the self-attention mask is set to −∞, and the other\\nelements to 0, as shown in Figure 1. Similarly, a right-to-left LM predicts a token conditioned on its\\nfuture (right) context.\\nBidirectional LM Following [9], a bidirectional LM allows all tokens to attend to each other in\\nprediction. It encodes contextual information from both directions, and can generate better contextual\\nrepresentations of text than its unidirectional counterpart. As indicated in Equation (2), the self-\\nattention mask M is a zero matrix, so that every token is allowed to attend across all positions in the\\ninput sequence.\\nSequence-to-Sequence LM As shown in Figure 1, for prediction, the tokens in the ﬁrst (source)\\nsegment can attend to each other from both directions within the segment, while the tokens of the\\nsecond (target) segment can only attend to the leftward context in the target segment and itself, as\\nwell as all the tokens in the source segment. For example, given source segment t1t2 and its target\\nsegment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1\\nand t2 have access to the ﬁrst four tokens, including [SOS] and [EOS], t4 can only attend to the ﬁrst\\nsix tokens.\\nFigure 1 shows the self-attention mask M used for the sequence-to-sequence LM objective. The left\\npart of M is set to 0 so that all tokens can attend to the ﬁrst segment. The upper right part is set to\\n−∞to block attentions from the source segment to the target segment. Moreover, for the lower right\\npart, we set its upper triangular part to −∞, and the other elements to 0, which prevents tokens in the\\ntarget segment from attending their future (right) positions.\\nDuring training, we randomly choose tokens in both segments, and replace them with the special\\ntoken [MASK]. The model is learned to recover the masked tokens. Since the pair of source and target\\ntexts are packed as a contiguous input text sequence in training, we implicitly encourage the model\\nto learn the relationship between the two segments. In order to better predict tokens in the target\\nsegment, UNILM learns to effectively encode the source segment. Thus, the cloze task designed for\\n4the sequence-to-sequence LM, also known as the encoder-decoder model, simultaneously pre-trains\\na bidirectional encoder and an unidirectional decoder. The pre-trained model, used as an encoder-\\ndecoder model, can be easily adapted to a wide range of conditional text generation tasks, such as\\nabstractive summarization.\\nNext Sentence Prediction For the bidirectional LM, we also include the next sentence prediction\\ntask for pre-training, as in [9].\\n2.4 Pre-training Setup\\nThe overall training objective the sum of different types of LM objectives described above. Specif-\\nically, within one training batch, 1/3 of the time we use the bidirectional LM objective, 1/3 of\\nthe time we employ the sequence-to-sequence LM objective, and both left-to-right and right-to-left\\nLM objectives are sampled with rate of 1/6. The model architecture of UNILM follows that of\\nBERTLARGE [9] for a fair comparison. The gelu activation [18] is used as GPT [31]. Speciﬁcally, we\\nuse a 24-layer Transformer with 1,024 hidden size, and 16 attention heads, which contains about\\n340M parameters. The weight matrix of the softmax classiﬁer is tied with token embeddings.UNILM\\nis initialized by BERTLARGE, and then pre-trained using English Wikipedia2 and BookCorpus [53],\\nwhich have been processed in the same way as [9]. The vocabulary size is 28,996. The maximum\\nlength of input sequence is 512. The token masking probability is 15%. Among masked positions,\\n80% of the time we replace the token with [MASK], 10% of the time with a random token, and\\nkeeping the original token for the rest. In addition, 80% of the time we randomly mask one token\\neach time, and 20% of the time we mask a bigram or a trigram.\\nAdam [22] with β1 = 0.9, β2 = 0.999 is used for optimization. The learning rate is 3e-5, with linear\\nwarmup over the ﬁrst 40,000 steps and linear decay. The dropout rate is 0.1. The weight decay is\\n0.01. The batch size is 330. The pre-training procedure runs for about 770,000 steps. It takes about\\n7 hours for 10,000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.\\n2.5 Fine-tuning on Downstream NLU and NLG Tasks\\nFor NLU tasks, we ﬁne-tune UNILM as a bidirectional Transformer encoder, like BERT. Take text\\nclassiﬁcation as an example. We use the encoding vector of [SOS] as the representation of input,\\ndenoted as hL\\n1 , and feed it to a randomly initialized softmax classiﬁer (i.e., the task-speciﬁc output\\nlayer), where the class probabilities are computed as softmax(hL\\n1 WC), where WC ∈Rdh×C is\\na parameter matrix, and C the number of categories. We maximize the likelihood of the labeled\\ntraining data by updating the parameters of the pre-trained LM and the added softmax classiﬁer.\\nFor NLG tasks, we take the sequence-to-sequence task as an example. The ﬁne-tuning procedure is\\nsimilar to pre-training using the self-attention masks as in Section 2.3. Let S1 and S2 denote source\\nand target sequences, respectively. We pack them together with special tokens, to form the input\\n“[SOS] S1 [EOS] S2 [EOS]”. The model is ﬁne-tuned by masking some percentage of tokens in the\\ntarget sequence at random, and learning to recover the masked words. The training objective is to\\nmaximize the likelihood of masked tokens given context. It is worth noting that [EOS], which marks\\nthe end of the target sequence, can also be masked during ﬁne-tuning, thus when this happens, the\\nmodel learns when to emit [EOS] to terminate the generation process of the target sequence.\\n3 Experiments\\nWe have conducted experiments on both NLU (i.e., the GLUE benchmark, and extractive question\\nanswering) and NLG tasks (i.e., abstractive summarization, question generation, generative question\\nanswering, and dialog response generation).\\n3.1 Abstractive Summarization\\nAutomatic text summarization produces a concise and ﬂuent summary conveying the key information\\nin the input (e.g., a news article). We focus on abstractive summarization, a generation task where\\n2Wikipedia version: enwiki-20181101.\\n5RG-1 RG-2 RG-L\\nExtractive Summarization\\nLEAD-3 40.42 17.62 36.67\\nBest Extractive [27] 43.25 20.24 39.63\\nAbstractive Summarization\\nPGNet [37] 39.53 17.28 37.98\\nBottom-Up [16] 41.22 18.68 38.34\\nS2S-ELMo [13] 41.56 18.94 38.47\\nUNILM 43.33 20.21 40.51\\nTable 3: Evaluation results on CNN/DailyMail\\nsummarization. Models in the ﬁrst block are ex-\\ntractive systems listed here for reference, while\\nthe others are abstractive models. The results\\nof the best reported extractive model are taken\\nfrom [27]. RG is short for ROUGE.\\nRG-1 RG-2 RG-L\\n10K Training Examples\\nTransformer [43] 10.97 2.23 10.42\\nMASS [39] 25.03 9.48 23.48\\nUNILM 32.96 14.68 30.56\\nFull Training Set\\nOpenNMT [23] 36.73 17.86 33.68\\nRe3Sum [4] 37.04 19.03 34.46\\nMASS [39] 37.66 18.53 34.89\\nUNILM 38.45 19.45 35.75\\nTable 4: Results on Gigaword abstractive summa-\\nrization. Models in the ﬁrst block only use 10K\\nexamples for training, while the others use 3.8M\\nexamples. Results of OpenNMT and Transformer\\nare taken from [4, 39]. RG is short for ROUGE.\\nthe summary is not constrained to reusing the phrases or sentences in the input text. We use the\\nnon-anonymized version of the CNN/DailyMail dataset [37] and Gigaword [36] for model ﬁne-tuning\\nand evaluation. We ﬁne-tune UNILM as a sequence-to-sequence model following the procedure\\ndescribed in Section 2.5 by concatenating document (the ﬁrst segment) and summary (the second\\nsegment) as input which is truncated according to a pre-deﬁned maximum length.\\nWe ﬁne-tune our model on the training set for 30 epochs. We reuse most hyper-parameters from\\npre-training. The masking probability is 0.7. We also use label smoothing [40] with rate of 0.1. For\\nCNN/DailyMail, we set batch size to 32, and maximum length to 768. For Gigaword, we set batch\\nsize to 64, and maximum length to 256. During decoding, we use beam search with beam size of 5.\\nThe input document is truncated to the ﬁrst 640 and 192 tokens for CNN/DailyMail and Gigaword,\\nrespectively. We remove duplicated trigrams in beam search, and tweak the maximum summary\\nlength on the development set [28, 13].\\nWe use the F1 version of ROUGE [25] as the evaluation metric for both datasets. In Table 3, we\\ncompare UNILM against the baseline and several state-of-the-art models on CNN/DailyMail. LEAD-\\n3 is a baseline model that extracts the ﬁrst three sentences in a document as its summary. PGNet [37]\\nis a sequence-to-sequence model based on the pointer-generator network. S2S-ELMo [ 13] uses a\\nsequence-to-sequence model augmented with pre-trained ELMo representations, which is termed as\\nSRC-ELMO+SHDEMB in [13]. Bottom-Up [16] is a sequence-to-sequence model augmented with a\\nbottom-up content selector for selecting salient phrases. We also include in Table 3 the best reported\\nextractive summarization result [27] on the dataset. As shown in Table 3, our model outperforms all\\nprevious abstractive systems, creating a new state-of-the-art abstractive summarization result on the\\ndataset. Our model also outperforms the best extractive model [27] by 0.88 point in ROUGE-L.\\nIn Table 4, we evaluate the models on Gigaword with different scales (10K and 3.8M). Both\\nTransformer [43] and OpenNMT [23] implement standard attentional sequence-to-sequence models.\\nRe3Sum [4] retrieves summaries as candidate templates, and then use an extended sequence-to-\\nsequence model to generate summaries. MASS [ 39] is a pre-trained sequence-to-sequence model\\nbased on Transformer networks. Experimental results show that UNILM achieves better performance\\nthan previous work. Besides, in the low-resource setting (i.e., only 10,000 examples are used as\\ntraining data), our model outperforms MASS by 7.08 point in ROUGE-L.\\n3.2 Question Answering (QA)\\nThe task is to answer a question given a passage [ 33, 34, 15]. There are two settings. The ﬁrst is\\ncalled extractive QA, where the answer is assumed to be a text span in the passage. The other is\\ncalled generative QA, where the answer needs to be generated on the ﬂy.\\nExtractive QA This task can be formulated as a NLU task where we need to predict the start and\\nend positions of the answer spans within the passage. We ﬁne-tune the pre-trained UNILM as a\\n6EM F1\\nRMR+ELMo [20] 71.4 73.7\\nBERTLARGE 78.9 81.8\\nUNILM 80.5 83.4\\nTable 5: Extractive QA results on\\nthe SQuAD development set.\\nF1\\nDrQA+ELMo [35] 67.2\\nBERTLARGE 82.7\\nUNILM 84.9\\nTable 6: Extractive QA results\\non the CoQA development set.\\nF1\\nSeq2Seq [35] 27.5\\nPGNet [35] 45.4\\nUNILM 82.5\\nTable 7: Generative QA results\\non the CoQA development set.\\nbidirectional encoder for the task. We conduct experiments on the Stanford Question Answering\\nDataset (SQuAD) 2.0 [34], and Conversational Question Answering (CoQA) [35] datasets.\\nThe results on SQuAD 2.0 are reported in Table 5, where we compare two models in Exact Match\\n(EM) and F1 score. RMR+ELMo [20] is an LSTM-based question answering model augmented with\\npre-trained language representation. BERTLARGE is a cased model, ﬁne-tuned on the SQuAD training\\ndata for 3 epochs, with batch size 24, and maximum length 384. UNILM is ﬁne-tuned in the same\\nway as BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nCoQA is a conversational question answering dataset. Compared with SQuAD, CoQA has several\\nunique characteristics. First, the examples in CoQA are conversational, so we need to answer the\\ninput question based on conversation histories. Second, the answers in CoQA can be free-form texts,\\nincluding a large portion is of yes/no answers.\\nWe modify the model used for SQuAD as follows. Firstly, in addition to the asked question,\\nwe concatenate the question-answer histories to the ﬁrst segment, so that the model can capture\\nconversational information. Secondly, for yes/no questions, we use the ﬁnal hidden vector of the\\n[SOS] token to predict whether the input is a yes/no question, and whether the answer is yes or no.\\nFor other examples, we select a passage subspan with the highest F1 score for training.\\nThe results on CoQA are reported in Table 6, where we compare two models in F1 scores.\\nDrQA+ELMo [35] is an LSTM-based question answering model augmented with pre-trained ELMo\\nrepresentation. BERTLARGE is a cased model, ﬁne-tuned on the CoQA training data for 2 epochs,\\nwith batch size 16, and maximum length 512. UNILM is ﬁne-tuned with the same hyper-parameters\\nas BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nGenerative QA Generative question answering generates free-form answers for the input question\\nand passage, which is a NLG task. In contrast, extractive methods can only predict subspans of the\\ninput passage as answers. On the CoQA dataset (as described above), Reddy et al. [2019] show that\\nvanilla sequence-to-sequence models still underperforms extractive methods by a wide margin.\\nWe adapt UNILM to generative question answering as a sequence-to-sequence model. The ﬁrst\\nsegment (i.e., the input sequence) is the concatenation of conversational histories, the input question\\nand the passage. The second segment (i.e., the output sequence) is the answer. We ﬁne-tune the\\npre-trained UNILM on the CoQA training set for 10 epochs. We set the batch size to 32, the mask\\nprobability to 0.5, and the maximum length to 512. We also use label smoothing with rate of 0.1.\\nThe other hyper-parameters are kept the same as pre-training. During decoding, we use beam search\\nwith beam size of 3. The maximum length of input question and passage is 470. For passages that\\nare longer than the maximum length, we split the passage into several chunks with a sliding window\\napproach, and select a chunk with the highest word overlap over the question.\\nWe compare our method with the generative question answering models Seq2Seq and PGNet as\\ndescribed in [35]. The Seq2Seq baseline is a sequence-to-sequence model with an attention mech-\\nanism. The PGNet model augments Seq2Seq with a copy mechanism. As shown in Table 7, our\\ngenerative question answering model outperforms previous generative methods by a wide margin,\\nwhich signiﬁcantly closes the gap between generative method and extractive method.\\n3.3 Question Generation\\nWe conduct experiments for the answer-aware question generation task [52]. Given an input passage\\nand an answer span, our goal is to generate a question that asks for the answer. The SQuAD 1.1\\ndataset [33] is used for evaluation. Following [12], we split the original training set into training and\\n7BLEU-4 MTR RG-L\\nCorefNQG [11] 15.16 19.12 -\\nSemQG [50] 18.37 22.65 46.68\\nUNILM 22.12 25.06 51.07\\nMP-GSN [51] 16.38 20.25 44.48\\nSemQG [50] 20.76 24.20 48.91\\nUNILM 23.75 25.61 52.04\\nTable 8: Question generation results on SQuAD.\\nMTR is short for METEOR, and RG for ROUGE.\\nResults in the groups use different data splits.\\nEM F1\\nUNILM QA Model (Section 3.2) 80.5 83.4\\n+ UNILM Generated Questions 84.7 87.6\\nTable 9: Question generation based on UNILM\\nimproves question answering results on the\\nSQuAD development set.\\nNIST-4 BLEU-4 METEOR Entropy-4 Div-1 Div-2 Avg len\\nBest System in DSTC7 Shared Task 2.523 1.83 8.07 9.030 0.109 0.325 15.133\\nUNILM 2.669 4.39 8.27 9.195 0.120 0.391 14.807\\nHuman Performance 2.650 3.13 8.31 10.445 0.167 0.670 18.76\\nTable 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams,\\nrespectively.\\ntest sets, and keep the original development set. We also conduct experiments following the data split\\nas in [51], which uses the reversed dev-test split.\\nThe question generation task is formulated as a sequence-to-sequence problem. The ﬁrst segment is\\nthe concatenation of input passage and answer, while the second segment is the generated question.\\nWe ﬁne-tune UNILM on the training set for 10 epochs. We set batch size to 32, masking probability\\nto 0.7, and learning rate to 2e-5. The rate of label smoothing is 0.1. The other hyper-parameters are\\nthe same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage\\nchunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are\\ncomputed by the same scripts as in [12].\\nThe results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with\\nattention and a feature-rich encoder. MP-GSN [51] uses an attention-based sequence-to-sequence\\nmodel with a gated self-attention encoder. SemQG [ 50] uses two semantics-enhanced rewards to\\nregularize the generation. UNILM outperforms previous models and achieves a new state-of-the-art\\nfor question generation.\\nGenerated Questions Improve QA The question generation model can automatically harvest a\\nlarge number of question-passage-answer examples from a text corpus. We show that the augmented\\ndata generated by question generation improves the question answering model.\\nWe generate ﬁve million answerable examples, and four million unanswerable examples by modifying\\nthe answerable ones. We ﬁne-tune our question answering model on the generated data for one epoch.\\nThen the model is ﬁne-tuned on the SQuAD 2.0 data for two more epochs.\\nAs shown in Table 9, the augmented data generated by UNILM improves question answering model\\nintroduced in Section 3.2. Note that we use bidirectional masked language modeling as an auxiliary\\ntask for both the generated and SQuAD 2.0 datasets during ﬁne-tuning, which brings 2.3 absolute\\nimprovement compared to directly using automatically generated examples. A possible reason is that\\nthe auxiliary task alleviates catastrophic forgetting [49] when ﬁne-tuning on augmented data.\\n3.4 Response Generation\\nWe evaluate UNILM on the document-grounded dialog response generation task [30, 15]. Given a\\nmulti-turn conversation history and a web document as the knowledge source, the system needs to\\n3Notice that if we directly use the tokenized references provided by Du et al. [2017], the results are (21.63\\nBLEU-4 / 25.04 METEOR / 51.09 ROUGE-L) on the original data split [ 12], and (23.08 BLEU-4 / 25.57\\nMETEOR / 52.03 ROUGE-L) in the reversed dev-test setup [51].\\n8Model CoLA SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE WNLI AX Score\\nMCC Acc F1 S Corr F1 Acc Acc Acc Acc Acc\\nGPT 45.4 91.3 82.3 80.0 70.3 82.1/81.4 87.4 56.0 53.4 29.8 72.8\\nBERTLARGE 60.5 94.9 89.3 86.5 72.1 86.7/85.9 92.7 70.1 65.1 39.6 80.5\\nUNILM 61.1 94.5 90.0 87.7 71.7 87.0/85.9 92.7 70.9 65.1 38.4 80.8\\nTable 11: GLUE test set results scored using the GLUE evaluation server.\\ngenerate a natural language response that is both conversationally appropriate and reﬂective of the\\ncontents of the web document. We ﬁne-tune UNILM to the task as a sequence-to-sequence model.\\nThe ﬁrst segment (input sequence) is the concatenation of the web document and the conversation\\nhistory. The second segment (output sequence) is the response. We ﬁne-tune UNILM on the DSTC7\\ntraining data for 20 epochs, with batch size 64. The masking probability is set to 0.5. The maximum\\nlength is 512. During decoding, we use beam search with size of 10. The maximum length of\\ngenerated response is set to 40. As shown in Table 10, UNILM outperforms the best system [41] in\\nthe DSTC7 shared task [14] across all evaluation metrics.\\n3.5 GLUE Benchmark\\nWe evaluate UNILM on the General Language Understanding Evaluation (GLUE) benchmark [45].\\nGLUE is a collection of nine language understanding tasks, including question answering [ 33],\\nlinguistic acceptability [46], sentiment analysis [38], text similarity [5], paraphrase detection [10],\\nand natural language inference (NLI) [7, 2, 17, 3, 24, 47].\\nOur model is ﬁne-tuned as a bidirectional LM. We use Adamax [21] as our optimizer with a learning\\nrate of 5e-5 and a batch size of 32. The maximum number of epochs is set to 5. A linear learning rate\\ndecay schedule with warmup of 0.1 is used. The dropout rate of the last linear projection for each\\ntask is set to 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. To avoid the gradient explosion\\nissue, the gradient norm was clipped within 1. We truncated the tokens no longer than 512.\\nTable 11 presents the GLUE test results obtained from the benchmark evaluation server. The\\nresults show that UNILM obtains comparable performance on the GLUE tasks in comparison with\\nBERTLARGE.\\n4 Conclusion and Future Work\\nWe propose a uniﬁed pre-training model, UNILM, which is jointly optimized for several LM\\nobjectives with shared parameters. The uniﬁcation of bidirectional, unidirectional, and sequence-\\nto-sequence LMs enables us to straightforwardly ﬁne-tune the pre-trained UNILM for both NLU\\nand NLG tasks. Experimental results demonstrate that our model compares favorably with BERT\\non the GLUE benchmark and two question answering datasets. In addition, UNILM outperforms\\nprevious state-of-the-art models on ﬁve NLG datasets: CNN/DailyMail and Gigaword abstractive\\nsummarization, SQuAD question generation, CoQA generative question answering, and DSTC7\\ndialog response generation.\\nThe work can be advanced from the following perspectives:\\n• We will push the limit of the current method by training more epochs and larger models on web-\\nscale text corpora. At the same time, we will also conduct more experiments on end applications\\nas well as ablation experiments to investigate the model capability and the beneﬁts of pre-training\\nmultiple language modeling tasks with the same network.\\n• We are focusing on monolingual NLP tasks in our current experiments. We are also interested in\\nextending UNILM to support cross-lingual tasks [6].\\n• We will conduct multi-task ﬁne-tuning on both NLU and NLG tasks, which is a natural extension\\nof Multi-Task Deep Neural Network (MT-DNN) [26].\\nAcknowledgement We would like to acknowledge Shiyue Zhang for the helpful discussions about\\nthe question generation experiments.\\n9References\\n[1] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven\\npretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019.\\n[2] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second\\nPASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL\\nChallenges Workshop on Recognising Textual Entailment, 01 2006.\\n[3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini.\\nThe ﬁfth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference\\n(TAC-09), 2009.\\n[4] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, pages 152–161, Melbourne, Australia, July 2018. Association for\\nComputational Linguistics.\\n[5] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055, 2017.\\n[6] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. Cross-\\nlingual natural language generation via pre-training. ArXiv, abs/1909.10481, 2019.\\n[7] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entail-\\nment challenge. In Proceedings of the First International Conference on Machine Learning\\nChallenges: Evaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing\\nTextual Entailment, MLCW’05, pages 177–190, Berlin, Heidelberg, 2006. Springer-Verlag.\\n[8] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems 28, pages 3079–3087. Curran Associates, Inc., 2015.\\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.\\n[10] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005) ,\\n2005.\\n[11] Xinya Du and Claire Cardie. Harvesting paragraph-level question-answer pairs from Wikipedia.\\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,\\npages 1907–1917, Melbourne, Australia, July 2018. Association for Computational Linguistics.\\n[12] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for\\nreading comprehension. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long\\nPapers, pages 1342–1352, 2017.\\n[13] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations\\nfor language generation. CoRR, abs/1903.09722, 2019.\\n[14] Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. Grounded response\\ngeneration task at dstc7. In AAAI Dialog System Technology Challenges Workshop, 2019.\\n[15] Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Founda-\\ntions and Trends in Information Retrieval, 13(2-3):127–298, 2019.\\n[16] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\\npages 4098–4109, Brussels, Belgium, October-November 2018. Association for Computational\\nLinguistics.\\n10[17] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL\\nrecognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop\\non Textual Entailment and Paraphrasing , pages 1–9, Prague, June 2007. Association for\\nComputational Linguistics.\\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\\narXiv:1606.08415, 2016.\\n[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics.\\n[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify:\\nMachine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018',\n",
       " ' Workshop\\non Textual Entailment and Paraphrasing , pages 1–9, Prague, June 2007. Association for\\nComputational Linguistics.\\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\\narXiv:1606.08415, 2016.\\n[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics.\\n[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify:\\nMachine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018.\\n[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd\\nInternational Conference on Learning Representations, San Diego, CA, 2015.\\n[23] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT:\\nOpen-source toolkit for neural machine translation. In Proceedings of ACL 2017, System\\nDemonstrations, pages 67–72, Vancouver, Canada, July 2017. Association for Computational\\nLinguistics.\\n[24] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\\nIn Thirteenth International Conference on the Principles of Knowledge Representation and\\nReasoning, 2012.\\n[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\\ntion Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July\\n2004. Association for Computational Linguistics.\\n[26] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\\nfor natural language understanding. CoRR, abs/1901.11504, 2019.\\n[27] Yang Liu. Fine-tune BERT for extractive summarization. CoRR, abs/1903.10318, 2019.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. CoRR, abs/1705.04304, 2018.\\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of\\nthe 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, pages 2227–2237, New Orleans, Louisiana, June\\n2018. Association for Computational Linguistics.\\n[30] Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin\\nChoi, and Jianfeng Gao. Conversing by reading: Contentful neural conversation with on-\\ndemand machine reading. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 5427–5436, Florence, Italy, July 2019. Association for\\nComputational Linguistics.\\n[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. 2018.\\n[32] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. 2019.\\n[33] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ ques-\\ntions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016.\\nAssociation for Computational Linguistics.\\n11[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\\nquestions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short\\nPapers, pages 784–789, 2018.\\n[35] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question\\nanswering challenge. Transactions of the Association for Computational Linguistics, 7:249–266,\\nMarch 2019.\\n[36] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\\nsentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in\\nNatural Language Processing, pages 379–389, Lisbon, Portugal, September 2015. Association\\nfor Computational Linguistics.\\n[37] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, pages 1073–1083, Vancouver, Canada, July 2017. Association for\\nComputational Linguistics.\\n[38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language\\nprocessing, pages 1631–1642, 2013.\\n[39] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to\\nsequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\\n[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\\nthinking the inception architecture for computer vision. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 2818–2826, 2016.\\n[41] Y Tam, Jiachen Ding, Cheng Niu, and Jie Zhou. Cluster-based beam search for pointer-generator\\nchatbot grounded by knowledge. In AAAI Dialog System Technology Challenges Workshop,\\n2019.\\n[42] Wilson L Taylor. Cloze procedure: A new tool for measuring readability. Journalism Bulletin,\\n30(4):415–433, 1953.\\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\\ntion Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017.\\n[44] Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov\\nrandom ﬁeld language model. CoRR, abs/1902.04094, 2019.\\n[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\\nInternational Conference on Learning Representations, 2019.\\n[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability\\njudgments. arXiv preprint arXiv:1805.12471, 2018.\\n[47] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational\\nLinguistics.\\n[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah,\\nMelvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,\\nHideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason\\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey\\nDean. Google’s neural machine translation system: Bridging the gap between human and\\nmachine translation. CoRR, abs/1609.08144, 2016.\\n12[49] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike\\nChrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and\\nPhil Blunsom. Learning and evaluating general linguistic intelligence. arXiv preprint\\narXiv:1901.11373, 2019.\\n[50] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. CoRR, abs/1909.06356, 2019.\\n[51] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question\\ngeneration with maxout pointer and gated self-attention networks. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 3901–3910, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics.\\n[52] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question\\ngeneration from text: A preliminary study. In Xuanjing Huang, Jing Jiang, Dongyan Zhao,\\nYansong Feng, and Yu Hong, editors,Natural Language Processing and Chinese Computing,\\npages 662–671. Springer International Publishing, 2018.\\n[53] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. In Proceedings of the IEEE International Conference on\\nComputer Vision, pages 19–27, 2015.\\nAppendix A Long Text Generation: A Case Study\\nOur model can generate text samples using the left-to-right setting. We picked three text samples\\nsampled from left to right using our model, as shown in Table 12. We use the top- 40 truncating\\nsampling strategy [ 32], and forbid duplicate 4-grams during generation. For each example, we\\nsampled 10 times from the same input and we hand-picked the best one; as such, these samples\\nshould be considered to be better than the average model output. From the examples, we ﬁnd that the\\nmodel can produce ﬂuent output with somewhat consistent contents which ﬁts the inputs’ genres and\\ntopics. In the ﬁrst example, given a modiﬁed excerpt from the novel “ 1984” as input, the model’s\\noutput is like a short paragraph in an fantasy novel; for the other two with input from Wikipedia, the\\nmodel makes up “facts” with plausible names, time stamps and events written in the Wikipedia style.\\nAppendix B GLUE Benchmark\\nAs shown in Table 13, we summarize the data size and the evaluation metrics used for the General\\nLanguage Understanding Evaluation (GLUE) benchmark.\\n13Input Output\\nWinston sat back. A\\nsense of complete\\nhelplessness had\\ndescended upon him.\\nWinston sat back. A sense of complete helplessness had descended\\nupon him. He was now helpless and unmoving. What would that\\nmean? To become a monk under her mother’s rule. What would it\\nmean for him to be put at the center of the most dangerous of all of\\ndemons, the demon known as Azrael? He would never know exactly\\nwhat that was, or even how it would end, unless he could sense the\\nstrength of Azrael’s mind and soul. It wasn’t only the existence of\\nhis mother, it was all he knew, and every moment. He was trapped\\nwithin her embrace, a trapped, torn, battered, crippling presence he\\nonly knew it would mean was eternity. And in the instant that he\\nwas lost his life changed forever, when Azraelis’ spirit touched his\\nmind, he was engulfed in this dark and mystical, so enchanting and\\nbeautiful force. (...)\\nBert is a yellow\\nMuppet character\\non the long running\\nchildren’s television\\nshow, Sesame Street.\\nBert is a yellow Muppet character on the long running children’s\\ntelevision show, Sesame Street. He would typically appear as a\\nsidekick or as a villain in the show; he is usually portrayed as\\na cowardly and untrustworthy ﬁgure that tends to intrude on the\\nnormal life of the street, such as “The Story of the Mole” , “The\\nStory from the Side”, and “Bump the Bush”. His voice was provided\\nby Michael Combs. Bert was a regular cast member of Sesame\\nStreet, and his appearance was always played throughout the series\\nby two of his regular cast members, Frank Laughner and Michael\\nCombs. (...)\\nMaurice Ravel (7\\nMarch 1875 - 28\\nDecember 1937) was\\na French composer,\\npianist and conductor.\\nMaurice Ravel (7 March 1875 - 28 December 1937) was a French\\ncomposer, pianist and conductor.He was born in Paris, and studied\\npiano with Eugene Massenet. In the early 1900s Ravel became\\ninterested in the composition of music, and he composed much of\\nhis early music for the piano. Ravel’s early work is marked by a\\nmarked sense of lyricism, but in the later years of his career he\\ndeveloped a strong preference for orchestral forms. His ﬁrst work,\\n“le Grand Cours d’ Auvergne”, was composed in 1909 by Maurice\\nRavel, a student of Jules Massenet, and was published by A.F.A.\\nin 1912. It was re-published in 1912 by the publisher J.S.D.M. de\\nl’Etablissement Musicale de la Musique Francaise. Ravel wrote\\nthe piano concerto “la Tragedie et la Chanson Dans le Theatre des\\nChamps Elysees” in 1916. (...)\\nTable 12: Text samples generated by our model using left-to-right generation.\\nCorpus #Train/#Dev/#Test Metrics\\nSingle-Sentence Classiﬁcation\\nCoLA (Acceptability) 8.5k/1k/1k Matthews corr\\nSST-2 (Sentiment) 67k/872/1.8k Accuracy\\nPairwise Text Classiﬁcation\\nMNLI (NLI) 393k/20k/20k Accuracy\\nRTE (NLI) 2.5k/276/3k Accuracy\\nQNLI (NLI) 108k/5.7k/5.7k Accuracy\\nWNLI (NLI) 634/71/146 Accuracy\\nQQP (Paraphrase) 364k/40k/391k F1 score\\nMRPC (Paraphrase) 3.7k/408/1.7k F1 score\\nText Similarity\\nSTS-B (Similarity) 7k/1.5k/1.4k Spearman corr\\nTable 13: Summary of the GLUE benchmark.\\n14']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_ques_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunk_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is in the list format and we need to convert into document representation\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ques_gen = [Document(page_content = t)for t in chunk_ques_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Uniﬁed Language Model Pre-training for\\nNatural Language Understanding and Generation\\nLi Dong∗ Nan Yang∗ Wenhui Wang∗ Furu Wei∗† Xiaodong Liu Yu Wang\\nJianfeng Gao Ming Zhou Hsiao-Wuen Hon\\nMicrosoft Research\\n{lidong1,nanya,wenwan,fuwei}@microsoft.com\\n{xiaodl,yuwan,jfgao,mingzhou,hon}@microsoft.com\\nAbstract\\nThis paper presents a new UNIﬁed pre-trained Language Model (UNILM) that\\ncan be ﬁne-tuned for both natural language understanding and generation tasks.\\nThe model is pre-trained using three types of language modeling tasks: unidirec-\\ntional, bidirectional, and sequence-to-sequence prediction. The uniﬁed modeling\\nis achieved by employing a shared Transformer network and utilizing speciﬁc\\nself-attention masks to control what context the prediction conditions on. UNILM\\ncompares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UNILM achieves new state-of-\\nthe-art results on ﬁve natural language generation datasets, including improving\\nthe CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute\\nimprovement), the Gigaword abstractive summarization ROUGE-L to35.75 (0.86\\nabsolute improvement), the CoQA generative question answering F1 score to 82.5\\n(37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12\\n(3.75 absolute improvement), and the DSTC7 document-grounded dialog response\\ngeneration NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained\\nmodels are available at https://github.com/microsoft/unilm.\\n1 Introduction\\nLanguage model (LM) pre-training has substantially advanced the state of the art across a variety\\nof natural language processing tasks [8, 29, 19, 31, 9, 1]. Pre-trained LMs learn contextualized text\\nrepresentations by predicting words based on their context using large amounts of text data, and can\\nbe ﬁne-tuned to adapt to downstream tasks.\\nDifferent prediction tasks and training objectives have been used for pre-training LMs of different\\ntypes, as shown in Table 1. ELMo [ 29] learns two unidirectional LMs: a forward LM reads the\\ntext from left to right, and a backward LM encodes the text from right to left. GPT [ 31] uses\\na left-to-right Transformer [ 43] to predict a text sequence word-by-word. In contrast, BERT [ 9]\\nemploys a bidirectional Transformer encoder to fuse both the left and right context to predict the\\nmasked words. Although BERT signiﬁcantly improves the performance of a wide range of natural\\nlanguage understanding tasks [9], its bidirectionality nature makes it difﬁcult to be applied to natural\\nlanguage generation tasks [44].\\nIn this work we propose a new UNIﬁed pre-trained Language Model (UNILM) that can be applied to\\nboth natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is\\na multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three\\ntypes of unsupervised language modeling objectives as shown in Table 2. In particular, we design a\\n∗ Equal contribution. † Contact person.\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1905.03197v3  [cs.CL]  15 Oct 2019ELMo GPT BERT U NILM\\nLeft-to-Right LM ✓ ✓ ✓\\nRight-to-Left LM ✓ ✓\\nBidirectional LM ✓ ✓\\nSequence-to-Sequence LM ✓\\nTable 1: Comparison between language model (LM) pre-training objectives.\\nBackbone\\nNetwork\\nLM Objectives of\\nUniﬁed Pre-training What Uniﬁed LM Learns Example Downstream Tasks\\nTransformer\\nwith shared\\nparameters\\nfor all LM\\nobjectives\\nBidirectional LM Bidirectional encoding GLUE benchmark\\nExtractive question answering\\nUnidirectional LM Unidirectional decoding Long text generation\\nSequence-to-Sequence LM\\nUnidirectional decoding\\nconditioned on\\nbidirectional encoding\\nAbstractive summarization\\nQuestion generation\\nGenerative question answering\\nTable 2: The uniﬁed LM is jointly pre-trained by multiple language modeling objectives, sharing the\\nsame parameters. We ﬁne-tune and evaluate the pre-trained uniﬁed LM on various datasets, including\\nboth language understanding and generation tasks.\\nset of cloze tasks [42] where a masked word is predicted based on its context. These cloze tasks differ\\nin how the context is deﬁned. For a left-to-right unidirectional LM, the context of the masked word\\nto be predicted consists of all the words on its left. For a right-to-left unidirectional LM, the context\\nconsists of all the words on the right. For a bidirectional LM, the context consists of the words on\\nboth the right and the left [9]. For a sequence-to-sequence LM, the context of the to-be-predicted\\nword in the second (target) sequence consists of all the words in the ﬁrst (source) sequence and the\\nwords on the its left in the target sequence.\\nSimilar to BERT, the pre-trained UNILM can be ﬁne-tuned (with additional task-speciﬁc layers\\nif necessary) to adapt to various downstream tasks. But unlike BERT which is used mainly for\\nNLU tasks, UNILM can be conﬁgured, using different self-attention masks (Section 2), to aggregate\\ncontext for different types of language models, and thus can be used for both NLU and NLG tasks.\\nThe proposed UNILM has three main advantages. First, the uniﬁed pre-training procedure leads to a\\nsingle Transformer LM that uses the shared parameters and architecture for different types of LMs,\\nalleviating the need of separately training and hosting multiple LMs. Second, the parameter sharing\\nmakes the learned text representations more general because they are jointly optimized for different\\nlanguage modeling objectives where context is utilized in different ways, mitigating overﬁtting to\\nany single LM task. Third, in addition to its application to NLU tasks, the use of UNILM as a\\nsequence-to-sequence LM (Section 2.3), makes it a natural choice for NLG, such as abstractive\\nsummarization and question generation.\\nExperimental results show that our model, used as a bidirectional encoder, compares favorably with\\nBERT on the GLUE benchmark and two extractive question answering tasks (i.e., SQuAD 2.0 and\\nCoQA). In addition, we demonstrate the effectiveness of UNILM on ﬁve NLG datasets, where it\\nis used as a sequence-to-sequence model, creating new state-of-the-art results on CNN/DailyMail\\nand Gigaword abstractive summarization, SQuAD question generation, CoQA generative question\\nanswering, and DSTC7 dialog response generation.\\n2 Uniﬁed Language Model Pre-training\\nGiven an input sequence x= x1 ···x|x|, UNILM obtains a contextualized vector representation for\\neach token. As shown in Figure 1, the pre-training optimizes the shared Transformer [43] network\\nwith respect to several unsupervised language modeling objectives, namely, unidirectional LM,\\nbidirectional LM, and sequence-to-sequence LM. In order to control the access to the context of the\\nword token to be predicted, we employ different masks for self-attention. In other words, we use\\nmasking to control how much context the token should attend to when computing its contextualized\\n2x1 x2 x3 x4 x5\\nTransformer\\nTransformer\\nTransformer\\nPosition Embedding\\nToken Embedding\\nSegment Embedding\\nTransformer Block 1\\nTransformer Block 2\\nTransformer Block L\\n...\\nh1 h2 h3 h4 h5\\nTransformer\\nTransformer\\nTransformer\\nSOS S1 EOS S2 EOS\\nSOS S1 S1 S1 EOS\\nSOS S1 EOS S2 EOS\\nUnified LM with \\nShared Parameters\\nSelf-attention Masks\\nSegment 1 Segment 2\\nSegment 1 Segment 2\\nSegment 1\\nLeft-to-Right LM\\nS1&S2: attend to all tokens\\nS1: attend to left context\\nS1: attend to S1 tokens\\nS2: attend to left context\\nS1\\nS2\\nS1 S2\\nS1\\nS2\\nS1 S2\\nAllow to attend\\nPrevent from attending\\nFigure 1: Overview of uniﬁed LM pre-training. The model parameters are shared across the LM\\nobjectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM). We use different\\nself-attention masks to control the access to context for each word token. The right-to-left LM is\\nsimilar to the left-to-right one, which is omitted in the ﬁgure for brevity.\\nrepresentation. Once UNILM is pretrained, we can ﬁne-tune it using task-speciﬁc data for downstream\\ntasks.\\n2.1 Input Representation\\nThe input xis a word sequence, which is either a text segment for unidirectional LMs or a pair of\\nsegments packed together for bidirectional LM and sequence-to-sequence LM. We always add a\\nspecial start-of-sequence ([SOS]) token at the beginning of input, and a special end-of-sequence\\n([EOS]) token at the end of each segment.[EOS] not only marks the sentence boundary in NLU tasks,\\nbut also is used for the model to learn when to terminate the decoding process in NLG tasks. The input\\nrepresentation follows that of BERT [9]. Texts are tokenized to subword units by WordPiece [48].\\nFor each input token, its vector representation is computed by summing the corresponding token\\nembedding, position embedding, and segment embedding. Since UNILM is trained using multiple\\nLM tasks, segment embeddings also play a role of LM identiﬁer in that we use different segment\\nembeddings for different LM objectives.\\n2.2 Backbone Network: Multi-Layer Transformer\\nThe input vectors {xi}|x|\\ni=1 is ﬁrst packed into H0 = [x1,··· ,x|x|], and then encoded into contextual\\nrepresentations at different levels of abstract Hl = [hl\\n1,··· ,hl\\n|x|] using an L-layer Transformer\\nHl = Transformerl(Hl−1),l ∈[1,L]. In each Transformer block, multiple self-attention heads are\\nused to aggregate the output vectors of the previous layer. For the l-th Transformer layer, the output\\n3of a self-attention head Al is computed via:\\nQ = Hl−1WQ\\nl , K = Hl−1WK\\nl , V = Hl−1WV\\nl (1)\\nMij =\\n{0, allow to attend\\n−∞, prevent from attending (2)\\nAl = softmax(QK⊺\\n√dk\\n+ M)Vl (3)\\nwhere the previous layer’s outputHl−1 ∈R|x|×dh is linearly projected to a triple of queries, keys\\nand values using parameter matrices WQ\\nl ,WK\\nl ,WV\\nl ∈Rdh×dk , respectively, and the mask matrix\\nM ∈R|x|×|x|determines whether a pair of tokens can be attended to each other.\\nWe use different mask matrices M to control what context a token can attend to when computing its\\ncontextualized representation, as illustrated in Figure 1. Take bidirectional LM as an example. The\\nelements of the mask matrix are all 0s, indicating that all the tokens have access to each other.\\n2.3 Pre-training Objectives\\nWe pretrain UNILM using four cloze tasks designed for different language modeling objectives.\\nIn a cloze task, we randomly choose some WordPiece tokens in the input, and replace them with\\nspecial token [MASK]. Then, we feed their corresponding output vectors computed by the Transformer\\nnetwork into a softmax classiﬁer to predict the masked token. The parameters of UNILM are learned\\nto minimize the cross-entropy loss computed using the predicted tokens and the original tokens. It is\\nworth noting that the use of cloze tasks makes it possible to use the same training procedure for all\\nLMs, unidirectional and bidirectional alike.\\nUnidirectional LM We use both left-to-right and right-to-left LM objectives. Take the left-to-right\\nLM as an example. The representation of each token encodes only the leftward context tokens\\nand itself. For instance, to predict the masked token of “x1x2 [MASK] x4”, only tokens x1,x2 and\\nitself can be used. This is done by using a triangular matrix for the self-attention mask M (as in\\nEquation (2)), where the upper triangular part of the self-attention mask is set to −∞, and the other\\nelements to 0, as shown in Figure 1. Similarly, a right-to-left LM predicts a token conditioned on its\\nfuture (right) context.\\nBidirectional LM Following [9], a bidirectional LM allows all tokens to attend to each other in\\nprediction. It encodes contextual information from both directions, and can generate better contextual\\nrepresentations of text than its unidirectional counterpart. As indicated in Equation (2), the self-\\nattention mask M is a zero matrix, so that every token is allowed to attend across all positions in the\\ninput sequence.\\nSequence-to-Sequence LM As shown in Figure 1, for prediction, the tokens in the ﬁrst (source)\\nsegment can attend to each other from both directions within the segment, while the tokens of the\\nsecond (target) segment can only attend to the leftward context in the target segment and itself, as\\nwell as all the tokens in the source segment. For example, given source segment t1t2 and its target\\nsegment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1\\nand t2 have access to the ﬁrst four tokens, including [SOS] and [EOS], t4 can only attend to the ﬁrst\\nsix tokens.\\nFigure 1 shows the self-attention mask M used for the sequence-to-sequence LM objective. The left\\npart of M is set to 0 so that all tokens can attend to the ﬁrst segment. The upper right part is set to\\n−∞to block attentions from the source segment to the target segment. Moreover, for the lower right\\npart, we set its upper triangular part to −∞, and the other elements to 0, which prevents tokens in the\\ntarget segment from attending their future (right) positions.\\nDuring training, we randomly choose tokens in both segments, and replace them with the special\\ntoken [MASK]. The model is learned to recover the masked tokens. Since the pair of source and target\\ntexts are packed as a contiguous input text sequence in training, we implicitly encourage the model\\nto learn the relationship between the two segments. In order to better predict tokens in the target\\nsegment, UNILM learns to effectively encode the source segment. Thus, the cloze task designed for\\n4the sequence-to-sequence LM, also known as the encoder-decoder model, simultaneously pre-trains\\na bidirectional encoder and an unidirectional decoder. The pre-trained model, used as an encoder-\\ndecoder model, can be easily adapted to a wide range of conditional text generation tasks, such as\\nabstractive summarization.\\nNext Sentence Prediction For the bidirectional LM, we also include the next sentence prediction\\ntask for pre-training, as in [9].\\n2.4 Pre-training Setup\\nThe overall training objective the sum of different types of LM objectives described above. Specif-\\nically, within one training batch, 1/3 of the time we use the bidirectional LM objective, 1/3 of\\nthe time we employ the sequence-to-sequence LM objective, and both left-to-right and right-to-left\\nLM objectives are sampled with rate of 1/6. The model architecture of UNILM follows that of\\nBERTLARGE [9] for a fair comparison. The gelu activation [18] is used as GPT [31]. Speciﬁcally, we\\nuse a 24-layer Transformer with 1,024 hidden size, and 16 attention heads, which contains about\\n340M parameters. The weight matrix of the softmax classiﬁer is tied with token embeddings.UNILM\\nis initialized by BERTLARGE, and then pre-trained using English Wikipedia2 and BookCorpus [53],\\nwhich have been processed in the same way as [9]. The vocabulary size is 28,996. The maximum\\nlength of input sequence is 512. The token masking probability is 15%. Among masked positions,\\n80% of the time we replace the token with [MASK], 10% of the time with a random token, and\\nkeeping the original token for the rest. In addition, 80% of the time we randomly mask one token\\neach time, and 20% of the time we mask a bigram or a trigram.\\nAdam [22] with β1 = 0.9, β2 = 0.999 is used for optimization. The learning rate is 3e-5, with linear\\nwarmup over the ﬁrst 40,000 steps and linear decay. The dropout rate is 0.1. The weight decay is\\n0.01. The batch size is 330. The pre-training procedure runs for about 770,000 steps. It takes about\\n7 hours for 10,000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.\\n2.5 Fine-tuning on Downstream NLU and NLG Tasks\\nFor NLU tasks, we ﬁne-tune UNILM as a bidirectional Transformer encoder, like BERT. Take text\\nclassiﬁcation as an example. We use the encoding vector of [SOS] as the representation of input,\\ndenoted as hL\\n1 , and feed it to a randomly initialized softmax classiﬁer (i.e., the task-speciﬁc output\\nlayer), where the class probabilities are computed as softmax(hL\\n1 WC), where WC ∈Rdh×C is\\na parameter matrix, and C the number of categories. We maximize the likelihood of the labeled\\ntraining data by updating the parameters of the pre-trained LM and the added softmax classiﬁer.\\nFor NLG tasks, we take the sequence-to-sequence task as an example. The ﬁne-tuning procedure is\\nsimilar to pre-training using the self-attention masks as in Section 2.3. Let S1 and S2 denote source\\nand target sequences, respectively. We pack them together with special tokens, to form the input\\n“[SOS] S1 [EOS] S2 [EOS]”. The model is ﬁne-tuned by masking some percentage of tokens in the\\ntarget sequence at random, and learning to recover the masked words. The training objective is to\\nmaximize the likelihood of masked tokens given context. It is worth noting that [EOS], which marks\\nthe end of the target sequence, can also be masked during ﬁne-tuning, thus when this happens, the\\nmodel learns when to emit [EOS] to terminate the generation process of the target sequence.\\n3 Experiments\\nWe have conducted experiments on both NLU (i.e., the GLUE benchmark, and extractive question\\nanswering) and NLG tasks (i.e., abstractive summarization, question generation, generative question\\nanswering, and dialog response generation).\\n3.1 Abstractive Summarization\\nAutomatic text summarization produces a concise and ﬂuent summary conveying the key information\\nin the input (e.g., a news article). We focus on abstractive summarization, a generation task where\\n2Wikipedia version: enwiki-20181101.\\n5RG-1 RG-2 RG-L\\nExtractive Summarization\\nLEAD-3 40.42 17.62 36.67\\nBest Extractive [27] 43.25 20.24 39.63\\nAbstractive Summarization\\nPGNet [37] 39.53 17.28 37.98\\nBottom-Up [16] 41.22 18.68 38.34\\nS2S-ELMo [13] 41.56 18.94 38.47\\nUNILM 43.33 20.21 40.51\\nTable 3: Evaluation results on CNN/DailyMail\\nsummarization. Models in the ﬁrst block are ex-\\ntractive systems listed here for reference, while\\nthe others are abstractive models. The results\\nof the best reported extractive model are taken\\nfrom [27]. RG is short for ROUGE.\\nRG-1 RG-2 RG-L\\n10K Training Examples\\nTransformer [43] 10.97 2.23 10.42\\nMASS [39] 25.03 9.48 23.48\\nUNILM 32.96 14.68 30.56\\nFull Training Set\\nOpenNMT [23] 36.73 17.86 33.68\\nRe3Sum [4] 37.04 19.03 34.46\\nMASS [39] 37.66 18.53 34.89\\nUNILM 38.45 19.45 35.75\\nTable 4: Results on Gigaword abstractive summa-\\nrization. Models in the ﬁrst block only use 10K\\nexamples for training, while the others use 3.8M\\nexamples. Results of OpenNMT and Transformer\\nare taken from [4, 39]. RG is short for ROUGE.\\nthe summary is not constrained to reusing the phrases or sentences in the input text. We use the\\nnon-anonymized version of the CNN/DailyMail dataset [37] and Gigaword [36] for model ﬁne-tuning\\nand evaluation. We ﬁne-tune UNILM as a sequence-to-sequence model following the procedure\\ndescribed in Section 2.5 by concatenating document (the ﬁrst segment) and summary (the second\\nsegment) as input which is truncated according to a pre-deﬁned maximum length.\\nWe ﬁne-tune our model on the training set for 30 epochs. We reuse most hyper-parameters from\\npre-training. The masking probability is 0.7. We also use label smoothing [40] with rate of 0.1. For\\nCNN/DailyMail, we set batch size to 32, and maximum length to 768. For Gigaword, we set batch\\nsize to 64, and maximum length to 256. During decoding, we use beam search with beam size of 5.\\nThe input document is truncated to the ﬁrst 640 and 192 tokens for CNN/DailyMail and Gigaword,\\nrespectively. We remove duplicated trigrams in beam search, and tweak the maximum summary\\nlength on the development set [28, 13].\\nWe use the F1 version of ROUGE [25] as the evaluation metric for both datasets. In Table 3, we\\ncompare UNILM against the baseline and several state-of-the-art models on CNN/DailyMail. LEAD-\\n3 is a baseline model that extracts the ﬁrst three sentences in a document as its summary. PGNet [37]\\nis a sequence-to-sequence model based on the pointer-generator network. S2S-ELMo [ 13] uses a\\nsequence-to-sequence model augmented with pre-trained ELMo representations, which is termed as\\nSRC-ELMO+SHDEMB in [13]. Bottom-Up [16] is a sequence-to-sequence model augmented with a\\nbottom-up content selector for selecting salient phrases. We also include in Table 3 the best reported\\nextractive summarization result [27] on the dataset. As shown in Table 3, our model outperforms all\\nprevious abstractive systems, creating a new state-of-the-art abstractive summarization result on the\\ndataset. Our model also outperforms the best extractive model [27] by 0.88 point in ROUGE-L.\\nIn Table 4, we evaluate the models on Gigaword with different scales (10K and 3.8M). Both\\nTransformer [43] and OpenNMT [23] implement standard attentional sequence-to-sequence models.\\nRe3Sum [4] retrieves summaries as candidate templates, and then use an extended sequence-to-\\nsequence model to generate summaries. MASS [ 39] is a pre-trained sequence-to-sequence model\\nbased on Transformer networks. Experimental results show that UNILM achieves better performance\\nthan previous work. Besides, in the low-resource setting (i.e., only 10,000 examples are used as\\ntraining data), our model outperforms MASS by 7.08 point in ROUGE-L.\\n3.2 Question Answering (QA)\\nThe task is to answer a question given a passage [ 33, 34, 15]. There are two settings. The ﬁrst is\\ncalled extractive QA, where the answer is assumed to be a text span in the passage. The other is\\ncalled generative QA, where the answer needs to be generated on the ﬂy.\\nExtractive QA This task can be formulated as a NLU task where we need to predict the start and\\nend positions of the answer spans within the passage. We ﬁne-tune the pre-trained UNILM as a\\n6EM F1\\nRMR+ELMo [20] 71.4 73.7\\nBERTLARGE 78.9 81.8\\nUNILM 80.5 83.4\\nTable 5: Extractive QA results on\\nthe SQuAD development set.\\nF1\\nDrQA+ELMo [35] 67.2\\nBERTLARGE 82.7\\nUNILM 84.9\\nTable 6: Extractive QA results\\non the CoQA development set.\\nF1\\nSeq2Seq [35] 27.5\\nPGNet [35] 45.4\\nUNILM 82.5\\nTable 7: Generative QA results\\non the CoQA development set.\\nbidirectional encoder for the task. We conduct experiments on the Stanford Question Answering\\nDataset (SQuAD) 2.0 [34], and Conversational Question Answering (CoQA) [35] datasets.\\nThe results on SQuAD 2.0 are reported in Table 5, where we compare two models in Exact Match\\n(EM) and F1 score. RMR+ELMo [20] is an LSTM-based question answering model augmented with\\npre-trained language representation. BERTLARGE is a cased model, ﬁne-tuned on the SQuAD training\\ndata for 3 epochs, with batch size 24, and maximum length 384. UNILM is ﬁne-tuned in the same\\nway as BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nCoQA is a conversational question answering dataset. Compared with SQuAD, CoQA has several\\nunique characteristics. First, the examples in CoQA are conversational, so we need to answer the\\ninput question based on conversation histories. Second, the answers in CoQA can be free-form texts,\\nincluding a large portion is of yes/no answers.\\nWe modify the model used for SQuAD as follows. Firstly, in addition to the asked question,\\nwe concatenate the question-answer histories to the ﬁrst segment, so that the model can capture\\nconversational information. Secondly, for yes/no questions, we use the ﬁnal hidden vector of the\\n[SOS] token to predict whether the input is a yes/no question, and whether the answer is yes or no.\\nFor other examples, we select a passage subspan with the highest F1 score for training.\\nThe results on CoQA are reported in Table 6, where we compare two models in F1 scores.\\nDrQA+ELMo [35] is an LSTM-based question answering model augmented with pre-trained ELMo\\nrepresentation. BERTLARGE is a cased model, ﬁne-tuned on the CoQA training data for 2 epochs,\\nwith batch size 16, and maximum length 512. UNILM is ﬁne-tuned with the same hyper-parameters\\nas BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nGenerative QA Generative question answering generates free-form answers for the input question\\nand passage, which is a NLG task. In contrast, extractive methods can only predict subspans of the\\ninput passage as answers. On the CoQA dataset (as described above), Reddy et al. [2019] show that\\nvanilla sequence-to-sequence models still underperforms extractive methods by a wide margin.\\nWe adapt UNILM to generative question answering as a sequence-to-sequence model. The ﬁrst\\nsegment (i.e., the input sequence) is the concatenation of conversational histories, the input question\\nand the passage. The second segment (i.e., the output sequence) is the answer. We ﬁne-tune the\\npre-trained UNILM on the CoQA training set for 10 epochs. We set the batch size to 32, the mask\\nprobability to 0.5, and the maximum length to 512. We also use label smoothing with rate of 0.1.\\nThe other hyper-parameters are kept the same as pre-training. During decoding, we use beam search\\nwith beam size of 3. The maximum length of input question and passage is 470. For passages that\\nare longer than the maximum length, we split the passage into several chunks with a sliding window\\napproach, and select a chunk with the highest word overlap over the question.\\nWe compare our method with the generative question answering models Seq2Seq and PGNet as\\ndescribed in [35]. The Seq2Seq baseline is a sequence-to-sequence model with an attention mech-\\nanism. The PGNet model augments Seq2Seq with a copy mechanism. As shown in Table 7, our\\ngenerative question answering model outperforms previous generative methods by a wide margin,\\nwhich signiﬁcantly closes the gap between generative method and extractive method.\\n3.3 Question Generation\\nWe conduct experiments for the answer-aware question generation task [52]. Given an input passage\\nand an answer span, our goal is to generate a question that asks for the answer. The SQuAD 1.1\\ndataset [33] is used for evaluation. Following [12], we split the original training set into training and\\n7BLEU-4 MTR RG-L\\nCorefNQG [11] 15.16 19.12 -\\nSemQG [50] 18.37 22.65 46.68\\nUNILM 22.12 25.06 51.07\\nMP-GSN [51] 16.38 20.25 44.48\\nSemQG [50] 20.76 24.20 48.91\\nUNILM 23.75 25.61 52.04\\nTable 8: Question generation results on SQuAD.\\nMTR is short for METEOR, and RG for ROUGE.\\nResults in the groups use different data splits.\\nEM F1\\nUNILM QA Model (Section 3.2) 80.5 83.4\\n+ UNILM Generated Questions 84.7 87.6\\nTable 9: Question generation based on UNILM\\nimproves question answering results on the\\nSQuAD development set.\\nNIST-4 BLEU-4 METEOR Entropy-4 Div-1 Div-2 Avg len\\nBest System in DSTC7 Shared Task 2.523 1.83 8.07 9.030 0.109 0.325 15.133\\nUNILM 2.669 4.39 8.27 9.195 0.120 0.391 14.807\\nHuman Performance 2.650 3.13 8.31 10.445 0.167 0.670 18.76\\nTable 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams,\\nrespectively.\\ntest sets, and keep the original development set. We also conduct experiments following the data split\\nas in [51], which uses the reversed dev-test split.\\nThe question generation task is formulated as a sequence-to-sequence problem. The ﬁrst segment is\\nthe concatenation of input passage and answer, while the second segment is the generated question.\\nWe ﬁne-tune UNILM on the training set for 10 epochs. We set batch size to 32, masking probability\\nto 0.7, and learning rate to 2e-5. The rate of label smoothing is 0.1. The other hyper-parameters are\\nthe same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage\\nchunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are\\ncomputed by the same scripts as in [12].\\nThe results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with\\nattention and a feature-rich encoder. MP-GSN [51] uses an attention-based sequence-to-sequence\\nmodel with a gated self-attention encoder. SemQG [ 50] uses two semantics-enhanced rewards to\\nregularize the generation. UNILM outperforms previous models and achieves a new state-of-the-art\\nfor question generation.\\nGenerated Questions Improve QA The question generation model can automatically harvest a\\nlarge number of question-passage-answer examples from a text corpus. We show that the augmented\\ndata generated by question generation improves the question answering model.\\nWe generate ﬁve million answerable examples, and four million unanswerable examples by modifying\\nthe answerable ones. We ﬁne-tune our question answering model on the generated data for one epoch.\\nThen the model is ﬁne-tuned on the SQuAD 2.0 data for two more epochs.\\nAs shown in Table 9, the augmented data generated by UNILM improves question answering model\\nintroduced in Section 3.2. Note that we use bidirectional masked language modeling as an auxiliary\\ntask for both the generated and SQuAD 2.0 datasets during ﬁne-tuning, which brings 2.3 absolute\\nimprovement compared to directly using automatically generated examples. A possible reason is that\\nthe auxiliary task alleviates catastrophic forgetting [49] when ﬁne-tuning on augmented data.\\n3.4 Response Generation\\nWe evaluate UNILM on the document-grounded dialog response generation task [30, 15]. Given a\\nmulti-turn conversation history and a web document as the knowledge source, the system needs to\\n3Notice that if we directly use the tokenized references provided by Du et al. [2017], the results are (21.63\\nBLEU-4 / 25.04 METEOR / 51.09 ROUGE-L) on the original data split [ 12], and (23.08 BLEU-4 / 25.57\\nMETEOR / 52.03 ROUGE-L) in the reversed dev-test setup [51].\\n8Model CoLA SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE WNLI AX Score\\nMCC Acc F1 S Corr F1 Acc Acc Acc Acc Acc\\nGPT 45.4 91.3 82.3 80.0 70.3 82.1/81.4 87.4 56.0 53.4 29.8 72.8\\nBERTLARGE 60.5 94.9 89.3 86.5 72.1 86.7/85.9 92.7 70.1 65.1 39.6 80.5\\nUNILM 61.1 94.5 90.0 87.7 71.7 87.0/85.9 92.7 70.9 65.1 38.4 80.8\\nTable 11: GLUE test set results scored using the GLUE evaluation server.\\ngenerate a natural language response that is both conversationally appropriate and reﬂective of the\\ncontents of the web document. We ﬁne-tune UNILM to the task as a sequence-to-sequence model.\\nThe ﬁrst segment (input sequence) is the concatenation of the web document and the conversation\\nhistory. The second segment (output sequence) is the response. We ﬁne-tune UNILM on the DSTC7\\ntraining data for 20 epochs, with batch size 64. The masking probability is set to 0.5. The maximum\\nlength is 512. During decoding, we use beam search with size of 10. The maximum length of\\ngenerated response is set to 40. As shown in Table 10, UNILM outperforms the best system [41] in\\nthe DSTC7 shared task [14] across all evaluation metrics.\\n3.5 GLUE Benchmark\\nWe evaluate UNILM on the General Language Understanding Evaluation (GLUE) benchmark [45].\\nGLUE is a collection of nine language understanding tasks, including question answering [ 33],\\nlinguistic acceptability [46], sentiment analysis [38], text similarity [5], paraphrase detection [10],\\nand natural language inference (NLI) [7, 2, 17, 3, 24, 47].\\nOur model is ﬁne-tuned as a bidirectional LM. We use Adamax [21] as our optimizer with a learning\\nrate of 5e-5 and a batch size of 32. The maximum number of epochs is set to 5. A linear learning rate\\ndecay schedule with warmup of 0.1 is used. The dropout rate of the last linear projection for each\\ntask is set to 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. To avoid the gradient explosion\\nissue, the gradient norm was clipped within 1. We truncated the tokens no longer than 512.\\nTable 11 presents the GLUE test results obtained from the benchmark evaluation server. The\\nresults show that UNILM obtains comparable performance on the GLUE tasks in comparison with\\nBERTLARGE.\\n4 Conclusion and Future Work\\nWe propose a uniﬁed pre-training model, UNILM, which is jointly optimized for several LM\\nobjectives with shared parameters. The uniﬁcation of bidirectional, unidirectional, and sequence-\\nto-sequence LMs enables us to straightforwardly ﬁne-tune the pre-trained UNILM for both NLU\\nand NLG tasks. Experimental results demonstrate that our model compares favorably with BERT\\non the GLUE benchmark and two question answering datasets. In addition, UNILM outperforms\\nprevious state-of-the-art models on ﬁve NLG datasets: CNN/DailyMail and Gigaword abstractive\\nsummarization, SQuAD question generation, CoQA generative question answering, and DSTC7\\ndialog response generation.\\nThe work can be advanced from the following perspectives:\\n• We will push the limit of the current method by training more epochs and larger models on web-\\nscale text corpora. At the same time, we will also conduct more experiments on end applications\\nas well as ablation experiments to investigate the model capability and the beneﬁts of pre-training\\nmultiple language modeling tasks with the same network.\\n• We are focusing on monolingual NLP tasks in our current experiments. We are also interested in\\nextending UNILM to support cross-lingual tasks [6].\\n• We will conduct multi-task ﬁne-tuning on both NLU and NLG tasks, which is a natural extension\\nof Multi-Task Deep Neural Network (MT-DNN) [26].\\nAcknowledgement We would like to acknowledge Shiyue Zhang for the helpful discussions about\\nthe question generation experiments.\\n9References\\n[1] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven\\npretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019.\\n[2] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second\\nPASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL\\nChallenges Workshop on Recognising Textual Entailment, 01 2006.\\n[3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini.\\nThe ﬁfth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference\\n(TAC-09), 2009.\\n[4] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, pages 152–161, Melbourne, Australia, July 2018. Association for\\nComputational Linguistics.\\n[5] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055, 2017.\\n[6] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. Cross-\\nlingual natural language generation via pre-training. ArXiv, abs/1909.10481, 2019.\\n[7] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entail-\\nment challenge. In Proceedings of the First International Conference on Machine Learning\\nChallenges: Evaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing\\nTextual Entailment, MLCW’05, pages 177–190, Berlin, Heidelberg, 2006. Springer-Verlag.\\n[8] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems 28, pages 3079–3087. Curran Associates, Inc., 2015.\\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.\\n[10] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005) ,\\n2005.\\n[11] Xinya Du and Claire Cardie. Harvesting paragraph-level question-answer pairs from Wikipedia.\\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,\\npages 1907–1917, Melbourne, Australia, July 2018. Association for Computational Linguistics.\\n[12] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for\\nreading comprehension. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long\\nPapers, pages 1342–1352, 2017.\\n[13] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations\\nfor language generation. CoRR, abs/1903.09722, 2019.\\n[14] Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. Grounded response\\ngeneration task at dstc7. In AAAI Dialog System Technology Challenges Workshop, 2019.\\n[15] Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Founda-\\ntions and Trends in Information Retrieval, 13(2-3):127–298, 2019.\\n[16] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\\npages 4098–4109, Brussels, Belgium, October-November 2018. Association for Computational\\nLinguistics.\\n10[17] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL\\nrecognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop\\non Textual Entailment and Paraphrasing , pages 1–9, Prague, June 2007. Association for\\nComputational Linguistics.\\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\\narXiv:1606.08415, 2016.\\n[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics.\\n[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify:\\nMachine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018'),\n",
       " Document(metadata={}, page_content=' Workshop\\non Textual Entailment and Paraphrasing , pages 1–9, Prague, June 2007. Association for\\nComputational Linguistics.\\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\\narXiv:1606.08415, 2016.\\n[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics.\\n[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify:\\nMachine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018.\\n[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd\\nInternational Conference on Learning Representations, San Diego, CA, 2015.\\n[23] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT:\\nOpen-source toolkit for neural machine translation. In Proceedings of ACL 2017, System\\nDemonstrations, pages 67–72, Vancouver, Canada, July 2017. Association for Computational\\nLinguistics.\\n[24] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\\nIn Thirteenth International Conference on the Principles of Knowledge Representation and\\nReasoning, 2012.\\n[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\\ntion Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July\\n2004. Association for Computational Linguistics.\\n[26] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\\nfor natural language understanding. CoRR, abs/1901.11504, 2019.\\n[27] Yang Liu. Fine-tune BERT for extractive summarization. CoRR, abs/1903.10318, 2019.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. CoRR, abs/1705.04304, 2018.\\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of\\nthe 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, pages 2227–2237, New Orleans, Louisiana, June\\n2018. Association for Computational Linguistics.\\n[30] Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin\\nChoi, and Jianfeng Gao. Conversing by reading: Contentful neural conversation with on-\\ndemand machine reading. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 5427–5436, Florence, Italy, July 2019. Association for\\nComputational Linguistics.\\n[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. 2018.\\n[32] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. 2019.\\n[33] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ ques-\\ntions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016.\\nAssociation for Computational Linguistics.\\n11[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\\nquestions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short\\nPapers, pages 784–789, 2018.\\n[35] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question\\nanswering challenge. Transactions of the Association for Computational Linguistics, 7:249–266,\\nMarch 2019.\\n[36] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\\nsentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in\\nNatural Language Processing, pages 379–389, Lisbon, Portugal, September 2015. Association\\nfor Computational Linguistics.\\n[37] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, pages 1073–1083, Vancouver, Canada, July 2017. Association for\\nComputational Linguistics.\\n[38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language\\nprocessing, pages 1631–1642, 2013.\\n[39] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to\\nsequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\\n[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\\nthinking the inception architecture for computer vision. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 2818–2826, 2016.\\n[41] Y Tam, Jiachen Ding, Cheng Niu, and Jie Zhou. Cluster-based beam search for pointer-generator\\nchatbot grounded by knowledge. In AAAI Dialog System Technology Challenges Workshop,\\n2019.\\n[42] Wilson L Taylor. Cloze procedure: A new tool for measuring readability. Journalism Bulletin,\\n30(4):415–433, 1953.\\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\\ntion Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017.\\n[44] Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov\\nrandom ﬁeld language model. CoRR, abs/1902.04094, 2019.\\n[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\\nInternational Conference on Learning Representations, 2019.\\n[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability\\njudgments. arXiv preprint arXiv:1805.12471, 2018.\\n[47] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational\\nLinguistics.\\n[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah,\\nMelvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,\\nHideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason\\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey\\nDean. Google’s neural machine translation system: Bridging the gap between human and\\nmachine translation. CoRR, abs/1609.08144, 2016.\\n12[49] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike\\nChrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and\\nPhil Blunsom. Learning and evaluating general linguistic intelligence. arXiv preprint\\narXiv:1901.11373, 2019.\\n[50] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. CoRR, abs/1909.06356, 2019.\\n[51] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question\\ngeneration with maxout pointer and gated self-attention networks. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 3901–3910, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics.\\n[52] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question\\ngeneration from text: A preliminary study. In Xuanjing Huang, Jing Jiang, Dongyan Zhao,\\nYansong Feng, and Yu Hong, editors,Natural Language Processing and Chinese Computing,\\npages 662–671. Springer International Publishing, 2018.\\n[53] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. In Proceedings of the IEEE International Conference on\\nComputer Vision, pages 19–27, 2015.\\nAppendix A Long Text Generation: A Case Study\\nOur model can generate text samples using the left-to-right setting. We picked three text samples\\nsampled from left to right using our model, as shown in Table 12. We use the top- 40 truncating\\nsampling strategy [ 32], and forbid duplicate 4-grams during generation. For each example, we\\nsampled 10 times from the same input and we hand-picked the best one; as such, these samples\\nshould be considered to be better than the average model output. From the examples, we ﬁnd that the\\nmodel can produce ﬂuent output with somewhat consistent contents which ﬁts the inputs’ genres and\\ntopics. In the ﬁrst example, given a modiﬁed excerpt from the novel “ 1984” as input, the model’s\\noutput is like a short paragraph in an fantasy novel; for the other two with input from Wikipedia, the\\nmodel makes up “facts” with plausible names, time stamps and events written in the Wikipedia style.\\nAppendix B GLUE Benchmark\\nAs shown in Table 13, we summarize the data size and the evaluation metrics used for the General\\nLanguage Understanding Evaluation (GLUE) benchmark.\\n13Input Output\\nWinston sat back. A\\nsense of complete\\nhelplessness had\\ndescended upon him.\\nWinston sat back. A sense of complete helplessness had descended\\nupon him. He was now helpless and unmoving. What would that\\nmean? To become a monk under her mother’s rule. What would it\\nmean for him to be put at the center of the most dangerous of all of\\ndemons, the demon known as Azrael? He would never know exactly\\nwhat that was, or even how it would end, unless he could sense the\\nstrength of Azrael’s mind and soul. It wasn’t only the existence of\\nhis mother, it was all he knew, and every moment. He was trapped\\nwithin her embrace, a trapped, torn, battered, crippling presence he\\nonly knew it would mean was eternity. And in the instant that he\\nwas lost his life changed forever, when Azraelis’ spirit touched his\\nmind, he was engulfed in this dark and mystical, so enchanting and\\nbeautiful force. (...)\\nBert is a yellow\\nMuppet character\\non the long running\\nchildren’s television\\nshow, Sesame Street.\\nBert is a yellow Muppet character on the long running children’s\\ntelevision show, Sesame Street. He would typically appear as a\\nsidekick or as a villain in the show; he is usually portrayed as\\na cowardly and untrustworthy ﬁgure that tends to intrude on the\\nnormal life of the street, such as “The Story of the Mole” , “The\\nStory from the Side”, and “Bump the Bush”. His voice was provided\\nby Michael Combs. Bert was a regular cast member of Sesame\\nStreet, and his appearance was always played throughout the series\\nby two of his regular cast members, Frank Laughner and Michael\\nCombs. (...)\\nMaurice Ravel (7\\nMarch 1875 - 28\\nDecember 1937) was\\na French composer,\\npianist and conductor.\\nMaurice Ravel (7 March 1875 - 28 December 1937) was a French\\ncomposer, pianist and conductor.He was born in Paris, and studied\\npiano with Eugene Massenet. In the early 1900s Ravel became\\ninterested in the composition of music, and he composed much of\\nhis early music for the piano. Ravel’s early work is marked by a\\nmarked sense of lyricism, but in the later years of his career he\\ndeveloped a strong preference for orchestral forms. His ﬁrst work,\\n“le Grand Cours d’ Auvergne”, was composed in 1909 by Maurice\\nRavel, a student of Jules Massenet, and was published by A.F.A.\\nin 1912. It was re-published in 1912 by the publisher J.S.D.M. de\\nl’Etablissement Musicale de la Musique Francaise. Ravel wrote\\nthe piano concerto “la Tragedie et la Chanson Dans le Theatre des\\nChamps Elysees” in 1916. (...)\\nTable 12: Text samples generated by our model using left-to-right generation.\\nCorpus #Train/#Dev/#Test Metrics\\nSingle-Sentence Classiﬁcation\\nCoLA (Acceptability) 8.5k/1k/1k Matthews corr\\nSST-2 (Sentiment) 67k/872/1.8k Accuracy\\nPairwise Text Classiﬁcation\\nMNLI (NLI) 393k/20k/20k Accuracy\\nRTE (NLI) 2.5k/276/3k Accuracy\\nQNLI (NLI) 108k/5.7k/5.7k Accuracy\\nWNLI (NLI) 634/71/146 Accuracy\\nQQP (Paraphrase) 364k/40k/391k F1 score\\nMRPC (Paraphrase) 3.7k/408/1.7k F1 score\\nText Similarity\\nSTS-B (Similarity) 7k/1.5k/1.4k Spearman corr\\nTable 13: Summary of the GLUE benchmark.\\n14')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_ques_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_ques_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Everything we converted into documents and again we do the splitter (chunking)\n",
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name = 'gpt-3.5-turbo',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_answer_gen = splitter_ans_gen.split_documents(\n",
    "    document_ques_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Uniﬁed Language Model Pre-training for\\nNatural Language Understanding and Generation\\nLi Dong∗ Nan Yang∗ Wenhui Wang∗ Furu Wei∗† Xiaodong Liu Yu Wang\\nJianfeng Gao Ming Zhou Hsiao-Wuen Hon\\nMicrosoft Research\\n{lidong1,nanya,wenwan,fuwei}@microsoft.com\\n{xiaodl,yuwan,jfgao,mingzhou,hon}@microsoft.com\\nAbstract\\nThis paper presents a new UNIﬁed pre-trained Language Model (UNILM) that\\ncan be ﬁne-tuned for both natural language understanding and generation tasks.\\nThe model is pre-trained using three types of language modeling tasks: unidirec-\\ntional, bidirectional, and sequence-to-sequence prediction. The uniﬁed modeling\\nis achieved by employing a shared Transformer network and utilizing speciﬁc\\nself-attention masks to control what context the prediction conditions on. UNILM\\ncompares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\\nand CoQA question answering tasks. Moreover, UNILM achieves new state-of-\\nthe-art results on ﬁve natural language generation datasets, including improving\\nthe CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute\\nimprovement), the Gigaword abstractive summarization ROUGE-L to35.75 (0.86\\nabsolute improvement), the CoQA generative question answering F1 score to 82.5\\n(37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12\\n(3.75 absolute improvement), and the DSTC7 document-grounded dialog response\\ngeneration NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained\\nmodels are available at https://github.com/microsoft/unilm.\\n1 Introduction\\nLanguage model (LM) pre-training has substantially advanced the state of the art across a variety\\nof natural language processing tasks [8, 29, 19, 31, 9, 1]. Pre-trained LMs learn contextualized text\\nrepresentations by predicting words based on their context using large amounts of text data, and can\\nbe ﬁne-tuned to adapt to downstream tasks.\\nDifferent prediction tasks and training objectives have been used for pre-training LMs of different\\ntypes, as shown in Table 1. ELMo [ 29] learns two unidirectional LMs: a forward LM reads the\\ntext from left to right, and a backward LM encodes the text from right to left. GPT [ 31] uses\\na left-to-right Transformer [ 43] to predict a text sequence word-by-word. In contrast, BERT [ 9]\\nemploys a bidirectional Transformer encoder to fuse both the left and right context to predict the\\nmasked words. Although BERT signiﬁcantly improves the performance of a wide range of natural\\nlanguage understanding tasks [9], its bidirectionality nature makes it difﬁcult to be applied to natural\\nlanguage generation tasks [44].\\nIn this work we propose a new UNIﬁed pre-trained Language Model (UNILM) that can be applied to\\nboth natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is\\na multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three\\ntypes of unsupervised language modeling objectives as shown in Table 2. In particular, we design a\\n∗ Equal contribution. † Contact person.\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1905.03197v3  [cs.CL]  15 Oct 2019ELMo GPT BERT U NILM\\nLeft-to-Right LM ✓ ✓ ✓\\nRight-to-Left LM ✓ ✓\\nBidirectional LM ✓ ✓\\nSequence-to-Sequence LM ✓\\nTable 1: Comparison between language model (LM) pre-training objectives.\\nBackbone\\nNetwork\\nLM Objectives of\\nUniﬁed Pre-training What Uniﬁed LM Learns Example Downstream Tasks\\nTransformer\\nwith shared\\nparameters\\nfor all LM\\nobjectives\\nBidirectional LM Bidirectional encoding GLUE benchmark\\nExtractive question answering\\nUnidirectional LM Unidirectional decoding Long text generation\\nSequence-to-Sequence LM\\nUnidirectional decoding\\nconditioned on\\nbidirectional encoding\\nAbstractive summarization\\nQuestion generation\\nGenerative question answering\\nTable 2: The uniﬁed LM is jointly pre-trained by multiple language modeling objectives, sharing the\\nsame parameters. We ﬁne-tune'),\n",
       " Document(metadata={}, page_content='\\nfor all LM\\nobjectives\\nBidirectional LM Bidirectional encoding GLUE benchmark\\nExtractive question answering\\nUnidirectional LM Unidirectional decoding Long text generation\\nSequence-to-Sequence LM\\nUnidirectional decoding\\nconditioned on\\nbidirectional encoding\\nAbstractive summarization\\nQuestion generation\\nGenerative question answering\\nTable 2: The uniﬁed LM is jointly pre-trained by multiple language modeling objectives, sharing the\\nsame parameters. We ﬁne-tune and evaluate the pre-trained uniﬁed LM on various datasets, including\\nboth language understanding and generation tasks.\\nset of cloze tasks [42] where a masked word is predicted based on its context. These cloze tasks differ\\nin how the context is deﬁned. For a left-to-right unidirectional LM, the context of the masked word\\nto be predicted consists of all the words on its left. For a right-to-left unidirectional LM, the context\\nconsists of all the words on the right. For a bidirectional LM, the context consists of the words on\\nboth the right and the left [9]. For a sequence-to-sequence LM, the context of the to-be-predicted\\nword in the second (target) sequence consists of all the words in the ﬁrst (source) sequence and the\\nwords on the its left in the target sequence.\\nSimilar to BERT, the pre-trained UNILM can be ﬁne-tuned (with additional task-speciﬁc layers\\nif necessary) to adapt to various downstream tasks. But unlike BERT which is used mainly for\\nNLU tasks, UNILM can be conﬁgured, using different self-attention masks (Section 2), to aggregate\\ncontext for different types of language models, and thus can be used for both NLU and NLG tasks.\\nThe proposed UNILM has three main advantages. First, the uniﬁed pre-training procedure leads to a\\nsingle Transformer LM that uses the shared parameters and architecture for different types of LMs,\\nalleviating the need of separately training and hosting multiple LMs. Second, the parameter sharing\\nmakes the learned text representations more general because they are jointly optimized for different\\nlanguage modeling objectives where context is utilized in different ways, mitigating overﬁtting to\\nany single LM task. Third, in addition to its application to NLU tasks, the use of UNILM as a\\nsequence-to-sequence LM (Section 2.3), makes it a natural choice for NLG, such as abstractive\\nsummarization and question generation.\\nExperimental results show that our model, used as a bidirectional encoder, compares favorably with\\nBERT on the GLUE benchmark and two extractive question answering tasks (i.e., SQuAD 2.0 and\\nCoQA). In addition, we demonstrate the effectiveness of UNILM on ﬁve NLG datasets, where it\\nis used as a sequence-to-sequence model, creating new state-of-the-art results on CNN/DailyMail\\nand Gigaword abstractive summarization, SQuAD question generation, CoQA generative question\\nanswering, and DSTC7 dialog response generation.\\n2 Uniﬁed Language Model Pre-training\\nGiven an input sequence x= x1 ···x|x|, UNILM obtains a contextualized vector representation for\\neach token. As shown in Figure 1, the pre-training optimizes the shared Transformer [43] network\\nwith respect to several unsupervised language modeling objectives, namely, unidirectional LM,\\nbidirectional LM, and sequence-to-sequence LM. In order to control the access to the context of the\\nword token to be predicted, we employ different masks for self-attention. In other words, we use\\nmasking to control how much context the token should attend to when computing its contextualized\\n2x1 x2 x3 x4 x5\\nTransformer\\nTransformer\\nTransformer\\nPosition Embedding\\nToken Embedding\\nSegment Embedding\\nTransformer Block 1\\nTransformer Block 2\\nTransformer Block L\\n...\\nh1 h2 h3 h4 h5\\nTransformer\\nTransformer\\nTransformer\\nSOS S1 EOS S2 EOS\\nSOS S1 S1 S1 EOS\\nSOS S1 EOS S2 EOS\\nUnified LM with \\nShared Parameters\\nSelf-attention Masks\\nSegment 1 Segment 2\\nSegment 1 Segment 2\\nSegment 1\\nLeft-to-Right LM\\nS1&S2: attend to all tokens\\nS1: attend to left context\\nS1: attend to S1 tokens\\nS2: attend to left context\\nS1\\nS2\\nS1 S2\\nS1\\nS2\\nS1 S2\\nAllow to attend\\nPrevent from attending\\n'),\n",
       " Document(metadata={}, page_content=' with \\nShared Parameters\\nSelf-attention Masks\\nSegment 1 Segment 2\\nSegment 1 Segment 2\\nSegment 1\\nLeft-to-Right LM\\nS1&S2: attend to all tokens\\nS1: attend to left context\\nS1: attend to S1 tokens\\nS2: attend to left context\\nS1\\nS2\\nS1 S2\\nS1\\nS2\\nS1 S2\\nAllow to attend\\nPrevent from attending\\nFigure 1: Overview of uniﬁed LM pre-training. The model parameters are shared across the LM\\nobjectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM). We use different\\nself-attention masks to control the access to context for each word token. The right-to-left LM is\\nsimilar to the left-to-right one, which is omitted in the ﬁgure for brevity.\\nrepresentation. Once UNILM is pretrained, we can ﬁne-tune it using task-speciﬁc data for downstream\\ntasks.\\n2.1 Input Representation\\nThe input xis a word sequence, which is either a text segment for unidirectional LMs or a pair of\\nsegments packed together for bidirectional LM and sequence-to-sequence LM. We always add a\\nspecial start-of-sequence ([SOS]) token at the beginning of input, and a special end-of-sequence\\n([EOS]) token at the end of each segment.[EOS] not only marks the sentence boundary in NLU tasks,\\nbut also is used for the model to learn when to terminate the decoding process in NLG tasks. The input\\nrepresentation follows that of BERT [9]. Texts are tokenized to subword units by WordPiece [48].\\nFor each input token, its vector representation is computed by summing the corresponding token\\nembedding, position embedding, and segment embedding. Since UNILM is trained using multiple\\nLM tasks, segment embeddings also play a role of LM identiﬁer in that we use different segment\\nembeddings for different LM objectives.\\n2.2 Backbone Network: Multi-Layer Transformer\\nThe input vectors {xi}|x|\\ni=1 is ﬁrst packed into H0 = [x1,··· ,x|x|], and then encoded into contextual\\nrepresentations at different levels of abstract Hl = [hl\\n1,··· ,hl\\n|x|] using an L-layer Transformer\\nHl = Transformerl(Hl−1),l ∈[1,L]. In each Transformer block, multiple self-attention heads are\\nused to aggregate the output vectors of the previous layer. For the l-th Transformer layer, the output\\n3of a self-attention head Al is computed via:\\nQ = Hl−1WQ\\nl , K = Hl−1WK\\nl , V = Hl−1WV\\nl (1)\\nMij =\\n{0, allow to attend\\n−∞, prevent from attending (2)\\nAl = softmax(QK⊺\\n√dk\\n+ M)Vl (3)\\nwhere the previous layer’s outputHl−1 ∈R|x|×dh is linearly projected to a triple of queries, keys\\nand values using parameter matrices WQ\\nl ,WK\\nl ,WV\\nl ∈Rdh×dk , respectively, and the mask matrix\\nM ∈R|x|×|x|determines whether a pair of tokens can be attended to each other.\\nWe use different mask matrices M to control what context a token can attend to when computing its\\ncontextualized representation, as illustrated in Figure 1. Take bidirectional LM as an example. The\\nelements of the mask matrix are all 0s, indicating that all the tokens have access to each other.\\n2.3 Pre-training Objectives\\nWe pretrain UNILM using four cloze tasks designed for different language modeling objectives.\\nIn a cloze task, we randomly choose some WordPiece tokens in the input, and replace them with\\nspecial token [MASK]. Then, we feed their corresponding output vectors computed by the Transformer\\nnetwork into a softmax classiﬁer to predict the masked token. The parameters of UNILM are learned\\nto minimize the cross-entropy loss computed using the predicted tokens and the original tokens. It is\\nworth noting that the use of cloze tasks makes it possible to use the same training procedure for all\\nLMs, unidirectional and bidirectional alike.\\nUnidirectional LM We use both left-to-right and right-to-left LM objectives. Take the left-to-right\\nLM as an example. The representation of each token encodes only the leftward context tokens\\nand itself. For instance, to predict the masked token of “x1x2 [MASK] x4”, only tokens'),\n",
       " Document(metadata={}, page_content='\\nworth noting that the use of cloze tasks makes it possible to use the same training procedure for all\\nLMs, unidirectional and bidirectional alike.\\nUnidirectional LM We use both left-to-right and right-to-left LM objectives. Take the left-to-right\\nLM as an example. The representation of each token encodes only the leftward context tokens\\nand itself. For instance, to predict the masked token of “x1x2 [MASK] x4”, only tokens x1,x2 and\\nitself can be used. This is done by using a triangular matrix for the self-attention mask M (as in\\nEquation (2)), where the upper triangular part of the self-attention mask is set to −∞, and the other\\nelements to 0, as shown in Figure 1. Similarly, a right-to-left LM predicts a token conditioned on its\\nfuture (right) context.\\nBidirectional LM Following [9], a bidirectional LM allows all tokens to attend to each other in\\nprediction. It encodes contextual information from both directions, and can generate better contextual\\nrepresentations of text than its unidirectional counterpart. As indicated in Equation (2), the self-\\nattention mask M is a zero matrix, so that every token is allowed to attend across all positions in the\\ninput sequence.\\nSequence-to-Sequence LM As shown in Figure 1, for prediction, the tokens in the ﬁrst (source)\\nsegment can attend to each other from both directions within the segment, while the tokens of the\\nsecond (target) segment can only attend to the leftward context in the target segment and itself, as\\nwell as all the tokens in the source segment. For example, given source segment t1t2 and its target\\nsegment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1\\nand t2 have access to the ﬁrst four tokens, including [SOS] and [EOS], t4 can only attend to the ﬁrst\\nsix tokens.\\nFigure 1 shows the self-attention mask M used for the sequence-to-sequence LM objective. The left\\npart of M is set to 0 so that all tokens can attend to the ﬁrst segment. The upper right part is set to\\n−∞to block attentions from the source segment to the target segment. Moreover, for the lower right\\npart, we set its upper triangular part to −∞, and the other elements to 0, which prevents tokens in the\\ntarget segment from attending their future (right) positions.\\nDuring training, we randomly choose tokens in both segments, and replace them with the special\\ntoken [MASK]. The model is learned to recover the masked tokens. Since the pair of source and target\\ntexts are packed as a contiguous input text sequence in training, we implicitly encourage the model\\nto learn the relationship between the two segments. In order to better predict tokens in the target\\nsegment, UNILM learns to effectively encode the source segment. Thus, the cloze task designed for\\n4the sequence-to-sequence LM, also known as the encoder-decoder model, simultaneously pre-trains\\na bidirectional encoder and an unidirectional decoder. The pre-trained model, used as an encoder-\\ndecoder model, can be easily adapted to a wide range of conditional text generation tasks, such as\\nabstractive summarization.\\nNext Sentence Prediction For the bidirectional LM, we also include the next sentence prediction\\ntask for pre-training, as in [9].\\n2.4 Pre-training Setup\\nThe overall training objective the sum of different types of LM objectives described above. Specif-\\nically, within one training batch, 1/3 of the time we use the bidirectional LM objective, 1/3 of\\nthe time we employ the sequence-to-sequence LM objective, and both left-to-right and right-to-left\\nLM objectives are sampled with rate of 1/6. The model architecture of UNILM follows that of\\nBERTLARGE [9] for a fair comparison. The gelu activation [18] is used as GPT [31]. Speciﬁcally, we\\nuse a 24-layer Transformer with 1,024 hidden size, and 16 attention heads, which contains about\\n340M parameters. The weight matrix of the softmax classiﬁer is tied with token embeddings.UNILM\\nis initialized by BERTLARGE, and then pre-trained using English Wikipedia2 and BookCorpus [53],\\nwhich have been processed in the same way as [9]. The vocabulary size is 28,996. The maximum\\nlength of input sequence is 512. The token masking probability is 15%. Among masked positions,\\n80% of'),\n",
       " Document(metadata={}, page_content=' heads, which contains about\\n340M parameters. The weight matrix of the softmax classiﬁer is tied with token embeddings.UNILM\\nis initialized by BERTLARGE, and then pre-trained using English Wikipedia2 and BookCorpus [53],\\nwhich have been processed in the same way as [9]. The vocabulary size is 28,996. The maximum\\nlength of input sequence is 512. The token masking probability is 15%. Among masked positions,\\n80% of the time we replace the token with [MASK], 10% of the time with a random token, and\\nkeeping the original token for the rest. In addition, 80% of the time we randomly mask one token\\neach time, and 20% of the time we mask a bigram or a trigram.\\nAdam [22] with β1 = 0.9, β2 = 0.999 is used for optimization. The learning rate is 3e-5, with linear\\nwarmup over the ﬁrst 40,000 steps and linear decay. The dropout rate is 0.1. The weight decay is\\n0.01. The batch size is 330. The pre-training procedure runs for about 770,000 steps. It takes about\\n7 hours for 10,000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.\\n2.5 Fine-tuning on Downstream NLU and NLG Tasks\\nFor NLU tasks, we ﬁne-tune UNILM as a bidirectional Transformer encoder, like BERT. Take text\\nclassiﬁcation as an example. We use the encoding vector of [SOS] as the representation of input,\\ndenoted as hL\\n1 , and feed it to a randomly initialized softmax classiﬁer (i.e., the task-speciﬁc output\\nlayer), where the class probabilities are computed as softmax(hL\\n1 WC), where WC ∈Rdh×C is\\na parameter matrix, and C the number of categories. We maximize the likelihood of the labeled\\ntraining data by updating the parameters of the pre-trained LM and the added softmax classiﬁer.\\nFor NLG tasks, we take the sequence-to-sequence task as an example. The ﬁne-tuning procedure is\\nsimilar to pre-training using the self-attention masks as in Section 2.3. Let S1 and S2 denote source\\nand target sequences, respectively. We pack them together with special tokens, to form the input\\n“[SOS] S1 [EOS] S2 [EOS]”. The model is ﬁne-tuned by masking some percentage of tokens in the\\ntarget sequence at random, and learning to recover the masked words. The training objective is to\\nmaximize the likelihood of masked tokens given context. It is worth noting that [EOS], which marks\\nthe end of the target sequence, can also be masked during ﬁne-tuning, thus when this happens, the\\nmodel learns when to emit [EOS] to terminate the generation process of the target sequence.\\n3 Experiments\\nWe have conducted experiments on both NLU (i.e., the GLUE benchmark, and extractive question\\nanswering) and NLG tasks (i.e., abstractive summarization, question generation, generative question\\nanswering, and dialog response generation).\\n3.1 Abstractive Summarization\\nAutomatic text summarization produces a concise and ﬂuent summary conveying the key information\\nin the input (e.g., a news article). We focus on abstractive summarization, a generation task where\\n2Wikipedia version: enwiki-20181101.\\n5RG-1 RG-2 RG-L\\nExtractive Summarization\\nLEAD-3 40.42 17.62 36.67\\nBest Extractive [27] 43.25 20.24 39.63\\nAbstractive Summarization\\nPGNet [37] 39.53 17.28 37.98\\nBottom-Up [16] 41.22 18.68 38.34\\nS2S-ELMo [13] 41.56 18.94 38.47\\nUNILM 43.33 20.21 40.51\\nTable 3: Evaluation results on CNN/DailyMail\\nsummarization. Models in the ﬁrst block are ex-\\ntractive systems listed here for reference, while\\nthe others are abstractive models. The results\\nof the best reported extractive model are taken\\nfrom [27]. RG is short for ROUGE.\\nRG-1 RG-2 RG-L\\n10K Training Examples\\nTransformer [43] 10.97 2.23 10.42'),\n",
       " Document(metadata={}, page_content='.51\\nTable 3: Evaluation results on CNN/DailyMail\\nsummarization. Models in the ﬁrst block are ex-\\ntractive systems listed here for reference, while\\nthe others are abstractive models. The results\\nof the best reported extractive model are taken\\nfrom [27]. RG is short for ROUGE.\\nRG-1 RG-2 RG-L\\n10K Training Examples\\nTransformer [43] 10.97 2.23 10.42\\nMASS [39] 25.03 9.48 23.48\\nUNILM 32.96 14.68 30.56\\nFull Training Set\\nOpenNMT [23] 36.73 17.86 33.68\\nRe3Sum [4] 37.04 19.03 34.46\\nMASS [39] 37.66 18.53 34.89\\nUNILM 38.45 19.45 35.75\\nTable 4: Results on Gigaword abstractive summa-\\nrization. Models in the ﬁrst block only use 10K\\nexamples for training, while the others use 3.8M\\nexamples. Results of OpenNMT and Transformer\\nare taken from [4, 39]. RG is short for ROUGE.\\nthe summary is not constrained to reusing the phrases or sentences in the input text. We use the\\nnon-anonymized version of the CNN/DailyMail dataset [37] and Gigaword [36] for model ﬁne-tuning\\nand evaluation. We ﬁne-tune UNILM as a sequence-to-sequence model following the procedure\\ndescribed in Section 2.5 by concatenating document (the ﬁrst segment) and summary (the second\\nsegment) as input which is truncated according to a pre-deﬁned maximum length.\\nWe ﬁne-tune our model on the training set for 30 epochs. We reuse most hyper-parameters from\\npre-training. The masking probability is 0.7. We also use label smoothing [40] with rate of 0.1. For\\nCNN/DailyMail, we set batch size to 32, and maximum length to 768. For Gigaword, we set batch\\nsize to 64, and maximum length to 256. During decoding, we use beam search with beam size of 5.\\nThe input document is truncated to the ﬁrst 640 and 192 tokens for CNN/DailyMail and Gigaword,\\nrespectively. We remove duplicated trigrams in beam search, and tweak the maximum summary\\nlength on the development set [28, 13].\\nWe use the F1 version of ROUGE [25] as the evaluation metric for both datasets. In Table 3, we\\ncompare UNILM against the baseline and several state-of-the-art models on CNN/DailyMail. LEAD-\\n3 is a baseline model that extracts the ﬁrst three sentences in a document as its summary. PGNet [37]\\nis a sequence-to-sequence model based on the pointer-generator network. S2S-ELMo [ 13] uses a\\nsequence-to-sequence model augmented with pre-trained ELMo representations, which is termed as\\nSRC-ELMO+SHDEMB in [13]. Bottom-Up [16] is a sequence-to-sequence model augmented with a\\nbottom-up content selector for selecting salient phrases. We also include in Table 3 the best reported\\nextractive summarization result [27] on the dataset. As shown in Table 3, our model outperforms all\\nprevious abstractive systems, creating a new state-of-the-art abstractive summarization result on the\\ndataset. Our model also outperforms the best extractive model [27] by 0.88 point in ROUGE-L.\\nIn Table 4, we evaluate the models on Gigaword with different scales (10K and 3.8M). Both\\nTransformer [43] and OpenNMT [23] implement standard attentional sequence-to-sequence models.\\nRe3Sum [4] retrieves summaries as candidate templates, and then use an extended sequence-to-\\nsequence model to generate summaries. MASS [ 39] is a pre-trained sequence-to-sequence model\\nbased on Transformer networks. Experimental results show that UNILM achieves better performance\\nthan previous work. Besides, in the low-resource setting (i.e., only 10,000 examples are used as\\ntraining data), our model outperforms MASS by 7.08 point in ROUGE-L.\\n3.2 Question Answering (QA)\\nThe task is to answer a question given a passage [ 33, 34, 15]. There are two settings. The ﬁrst is\\n'),\n",
       " Document(metadata={}, page_content=' on Transformer networks. Experimental results show that UNILM achieves better performance\\nthan previous work. Besides, in the low-resource setting (i.e., only 10,000 examples are used as\\ntraining data), our model outperforms MASS by 7.08 point in ROUGE-L.\\n3.2 Question Answering (QA)\\nThe task is to answer a question given a passage [ 33, 34, 15]. There are two settings. The ﬁrst is\\ncalled extractive QA, where the answer is assumed to be a text span in the passage. The other is\\ncalled generative QA, where the answer needs to be generated on the ﬂy.\\nExtractive QA This task can be formulated as a NLU task where we need to predict the start and\\nend positions of the answer spans within the passage. We ﬁne-tune the pre-trained UNILM as a\\n6EM F1\\nRMR+ELMo [20] 71.4 73.7\\nBERTLARGE 78.9 81.8\\nUNILM 80.5 83.4\\nTable 5: Extractive QA results on\\nthe SQuAD development set.\\nF1\\nDrQA+ELMo [35] 67.2\\nBERTLARGE 82.7\\nUNILM 84.9\\nTable 6: Extractive QA results\\non the CoQA development set.\\nF1\\nSeq2Seq [35] 27.5\\nPGNet [35] 45.4\\nUNILM 82.5\\nTable 7: Generative QA results\\non the CoQA development set.\\nbidirectional encoder for the task. We conduct experiments on the Stanford Question Answering\\nDataset (SQuAD) 2.0 [34], and Conversational Question Answering (CoQA) [35] datasets.\\nThe results on SQuAD 2.0 are reported in Table 5, where we compare two models in Exact Match\\n(EM) and F1 score. RMR+ELMo [20] is an LSTM-based question answering model augmented with\\npre-trained language representation. BERTLARGE is a cased model, ﬁne-tuned on the SQuAD training\\ndata for 3 epochs, with batch size 24, and maximum length 384. UNILM is ﬁne-tuned in the same\\nway as BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nCoQA is a conversational question answering dataset. Compared with SQuAD, CoQA has several\\nunique characteristics. First, the examples in CoQA are conversational, so we need to answer the\\ninput question based on conversation histories. Second, the answers in CoQA can be free-form texts,\\nincluding a large portion is of yes/no answers.\\nWe modify the model used for SQuAD as follows. Firstly, in addition to the asked question,\\nwe concatenate the question-answer histories to the ﬁrst segment, so that the model can capture\\nconversational information. Secondly, for yes/no questions, we use the ﬁnal hidden vector of the\\n[SOS] token to predict whether the input is a yes/no question, and whether the answer is yes or no.\\nFor other examples, we select a passage subspan with the highest F1 score for training.\\nThe results on CoQA are reported in Table 6, where we compare two models in F1 scores.\\nDrQA+ELMo [35] is an LSTM-based question answering model augmented with pre-trained ELMo\\nrepresentation. BERTLARGE is a cased model, ﬁne-tuned on the CoQA training data for 2 epochs,\\nwith batch size 16, and maximum length 512. UNILM is ﬁne-tuned with the same hyper-parameters\\nas BERTLARGE. We see that UNILM outperforms BERTLARGE.\\nGenerative QA Generative question answering generates free-form answers for the input question\\nand passage, which is a NLG task. In contrast, extractive methods can only predict subspans of the\\ninput passage as answers. On the CoQA dataset (as described above), Reddy et al. [2019] show that\\nvanilla sequence-to-sequence models still underperforms extractive methods by a wide margin.\\nWe adapt UNILM to generative question answering as a sequence-to-sequence model. The ﬁrst\\nsegment (i.e., the input sequence) is the concatenation of conversational histories, the input question\\nand the passage. The second segment (i.e., the output sequence) is the answer. We ﬁne-tune the\\npre-trained UNILM on the CoQA training set for 10 epochs. We'),\n",
       " Document(metadata={}, page_content=' underperforms extractive methods by a wide margin.\\nWe adapt UNILM to generative question answering as a sequence-to-sequence model. The ﬁrst\\nsegment (i.e., the input sequence) is the concatenation of conversational histories, the input question\\nand the passage. The second segment (i.e., the output sequence) is the answer. We ﬁne-tune the\\npre-trained UNILM on the CoQA training set for 10 epochs. We set the batch size to 32, the mask\\nprobability to 0.5, and the maximum length to 512. We also use label smoothing with rate of 0.1.\\nThe other hyper-parameters are kept the same as pre-training. During decoding, we use beam search\\nwith beam size of 3. The maximum length of input question and passage is 470. For passages that\\nare longer than the maximum length, we split the passage into several chunks with a sliding window\\napproach, and select a chunk with the highest word overlap over the question.\\nWe compare our method with the generative question answering models Seq2Seq and PGNet as\\ndescribed in [35]. The Seq2Seq baseline is a sequence-to-sequence model with an attention mech-\\nanism. The PGNet model augments Seq2Seq with a copy mechanism. As shown in Table 7, our\\ngenerative question answering model outperforms previous generative methods by a wide margin,\\nwhich signiﬁcantly closes the gap between generative method and extractive method.\\n3.3 Question Generation\\nWe conduct experiments for the answer-aware question generation task [52]. Given an input passage\\nand an answer span, our goal is to generate a question that asks for the answer. The SQuAD 1.1\\ndataset [33] is used for evaluation. Following [12], we split the original training set into training and\\n7BLEU-4 MTR RG-L\\nCorefNQG [11] 15.16 19.12 -\\nSemQG [50] 18.37 22.65 46.68\\nUNILM 22.12 25.06 51.07\\nMP-GSN [51] 16.38 20.25 44.48\\nSemQG [50] 20.76 24.20 48.91\\nUNILM 23.75 25.61 52.04\\nTable 8: Question generation results on SQuAD.\\nMTR is short for METEOR, and RG for ROUGE.\\nResults in the groups use different data splits.\\nEM F1\\nUNILM QA Model (Section 3.2) 80.5 83.4\\n+ UNILM Generated Questions 84.7 87.6\\nTable 9: Question generation based on UNILM\\nimproves question answering results on the\\nSQuAD development set.\\nNIST-4 BLEU-4 METEOR Entropy-4 Div-1 Div-2 Avg len\\nBest System in DSTC7 Shared Task 2.523 1.83 8.07 9.030 0.109 0.325 15.133\\nUNILM 2.669 4.39 8.27 9.195 0.120 0.391 14.807\\nHuman Performance 2.650 3.13 8.31 10.445 0.167 0.670 18.76\\nTable 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams,\\nrespectively.\\ntest sets, and keep the original development set. We also conduct experiments following the data split\\nas in [51], which uses the reversed dev-test split.\\nThe question generation task is formulated as a sequence-to-sequence problem. The ﬁrst segment is\\nthe concatenation of input passage and answer, while the second segment is the generated question.\\nWe ﬁne-tune UNILM on the training set for 10 epochs. We set batch size to 32, masking probability\\nto 0.7, and learning rate to 2e-5. The rate of label smoothing is 0.1. The other hyper-parameters are\\nthe same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage\\nchunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are\\ncomputed by the same scripts as in [12].\\nThe results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with\\nattention and a feature-rich encoder. MP-GSN [51] uses an attention'),\n",
       " Document(metadata={}, page_content=' same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage\\nchunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are\\ncomputed by the same scripts as in [12].\\nThe results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with\\nattention and a feature-rich encoder. MP-GSN [51] uses an attention-based sequence-to-sequence\\nmodel with a gated self-attention encoder. SemQG [ 50] uses two semantics-enhanced rewards to\\nregularize the generation. UNILM outperforms previous models and achieves a new state-of-the-art\\nfor question generation.\\nGenerated Questions Improve QA The question generation model can automatically harvest a\\nlarge number of question-passage-answer examples from a text corpus. We show that the augmented\\ndata generated by question generation improves the question answering model.\\nWe generate ﬁve million answerable examples, and four million unanswerable examples by modifying\\nthe answerable ones. We ﬁne-tune our question answering model on the generated data for one epoch.\\nThen the model is ﬁne-tuned on the SQuAD 2.0 data for two more epochs.\\nAs shown in Table 9, the augmented data generated by UNILM improves question answering model\\nintroduced in Section 3.2. Note that we use bidirectional masked language modeling as an auxiliary\\ntask for both the generated and SQuAD 2.0 datasets during ﬁne-tuning, which brings 2.3 absolute\\nimprovement compared to directly using automatically generated examples. A possible reason is that\\nthe auxiliary task alleviates catastrophic forgetting [49] when ﬁne-tuning on augmented data.\\n3.4 Response Generation\\nWe evaluate UNILM on the document-grounded dialog response generation task [30, 15]. Given a\\nmulti-turn conversation history and a web document as the knowledge source, the system needs to\\n3Notice that if we directly use the tokenized references provided by Du et al. [2017], the results are (21.63\\nBLEU-4 / 25.04 METEOR / 51.09 ROUGE-L) on the original data split [ 12], and (23.08 BLEU-4 / 25.57\\nMETEOR / 52.03 ROUGE-L) in the reversed dev-test setup [51].\\n8Model CoLA SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE WNLI AX Score\\nMCC Acc F1 S Corr F1 Acc Acc Acc Acc Acc\\nGPT 45.4 91.3 82.3 80.0 70.3 82.1/81.4 87.4 56.0 53.4 29.8 72.8\\nBERTLARGE 60.5 94.9 89.3 86.5 72.1 86.7/85.9 92.7 70.1 65.1 39.6 80.5\\nUNILM 61.1 94.5 90.0 87.7 71.7 87.0/85.9 92.7 70.9 65.1 38.4 80.8\\nTable 11: GLUE test set results scored using the GLUE evaluation server.\\ngenerate a natural language response that is both conversationally appropriate and reﬂective of the\\ncontents of the web document. We ﬁne-tune UNILM to the task as a sequence-to-sequence model.\\nThe ﬁrst segment (input sequence) is the concatenation of the web document and the conversation\\nhistory. The second segment (output sequence) is the response. We ﬁne-tune UNILM on the DSTC7\\ntraining data for 20 epochs, with batch size 64. The masking probability is set to 0.5. The maximum\\nlength is 512. During decoding, we use beam search with size of 10. The maximum length of\\ngenerated response is set to 40. As shown in Table 10, UNILM outperforms the best system [41] in\\nthe DSTC7 shared task [14] across all evaluation metrics.\\n3.5 GLUE Benchmark\\nWe evaluate UNILM on the General Language Understanding Evaluation (GLUE) benchmark [45].\\nGLUE is a collection of nine language understanding tasks, including question answering [ 33],\\nlinguistic acceptability [46], sentiment analysis [38], text similarity [5], paraphrase detection [10],\\nand natural language inference (NLI) [7, 2'),\n",
       " Document(metadata={}, page_content=' [41] in\\nthe DSTC7 shared task [14] across all evaluation metrics.\\n3.5 GLUE Benchmark\\nWe evaluate UNILM on the General Language Understanding Evaluation (GLUE) benchmark [45].\\nGLUE is a collection of nine language understanding tasks, including question answering [ 33],\\nlinguistic acceptability [46], sentiment analysis [38], text similarity [5], paraphrase detection [10],\\nand natural language inference (NLI) [7, 2, 17, 3, 24, 47].\\nOur model is ﬁne-tuned as a bidirectional LM. We use Adamax [21] as our optimizer with a learning\\nrate of 5e-5 and a batch size of 32. The maximum number of epochs is set to 5. A linear learning rate\\ndecay schedule with warmup of 0.1 is used. The dropout rate of the last linear projection for each\\ntask is set to 0.1, except 0.3 for MNLI and 0.05 for CoLA/SST-2. To avoid the gradient explosion\\nissue, the gradient norm was clipped within 1. We truncated the tokens no longer than 512.\\nTable 11 presents the GLUE test results obtained from the benchmark evaluation server. The\\nresults show that UNILM obtains comparable performance on the GLUE tasks in comparison with\\nBERTLARGE.\\n4 Conclusion and Future Work\\nWe propose a uniﬁed pre-training model, UNILM, which is jointly optimized for several LM\\nobjectives with shared parameters. The uniﬁcation of bidirectional, unidirectional, and sequence-\\nto-sequence LMs enables us to straightforwardly ﬁne-tune the pre-trained UNILM for both NLU\\nand NLG tasks. Experimental results demonstrate that our model compares favorably with BERT\\non the GLUE benchmark and two question answering datasets. In addition, UNILM outperforms\\nprevious state-of-the-art models on ﬁve NLG datasets: CNN/DailyMail and Gigaword abstractive\\nsummarization, SQuAD question generation, CoQA generative question answering, and DSTC7\\ndialog response generation.\\nThe work can be advanced from the following perspectives:\\n• We will push the limit of the current method by training more epochs and larger models on web-\\nscale text corpora. At the same time, we will also conduct more experiments on end applications\\nas well as ablation experiments to investigate the model capability and the beneﬁts of pre-training\\nmultiple language modeling tasks with the same network.\\n• We are focusing on monolingual NLP tasks in our current experiments. We are also interested in\\nextending UNILM to support cross-lingual tasks [6].\\n• We will conduct multi-task ﬁne-tuning on both NLU and NLG tasks, which is a natural extension\\nof Multi-Task Deep Neural Network (MT-DNN) [26].\\nAcknowledgement We would like to acknowledge Shiyue Zhang for the helpful discussions about\\nthe question generation experiments.\\n9References\\n[1] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven\\npretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019.\\n[2] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second\\nPASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL\\nChallenges Workshop on Recognising Textual Entailment, 01 2006.\\n[3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini.\\nThe ﬁfth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference\\n(TAC-09), 2009.\\n[4] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, pages 152–161, Melbourne, Australia, July 2018. Association for\\nComputational Linguistics.\\n[5] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055, 2017.\\n[6] Zewen Chi, Li Dong'),\n",
       " Document(metadata={}, page_content=' Melbourne, Australia, July 2018. Association for\\nComputational Linguistics.\\n[5] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv\\npreprint arXiv:1708.00055, 2017.\\n[6] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. Cross-\\nlingual natural language generation via pre-training. ArXiv, abs/1909.10481, 2019.\\n[7] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entail-\\nment challenge. In Proceedings of the First International Conference on Machine Learning\\nChallenges: Evaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing\\nTextual Entailment, MLCW’05, pages 177–190, Berlin, Heidelberg, 2006. Springer-Verlag.\\n[8] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems 28, pages 3079–3087. Curran Associates, Inc., 2015.\\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.\\n[10] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005) ,\\n2005.\\n[11] Xinya Du and Claire Cardie. Harvesting paragraph-level question-answer pairs from Wikipedia.\\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,\\npages 1907–1917, Melbourne, Australia, July 2018. Association for Computational Linguistics.\\n[12] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for\\nreading comprehension. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long\\nPapers, pages 1342–1352, 2017.\\n[13] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations\\nfor language generation. CoRR, abs/1903.09722, 2019.\\n[14] Michel Galley, Chris Brockett, Xiang Gao, Jianfeng Gao, and Bill Dolan. Grounded response\\ngeneration task at dstc7. In AAAI Dialog System Technology Challenges Workshop, 2019.\\n[15] Jianfeng Gao, Michel Galley, Lihong Li, et al. Neural approaches to conversational ai. Founda-\\ntions and Trends in Information Retrieval, 13(2-3):127–298, 2019.\\n[16] Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. Bottom-up abstractive summarization.\\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\\npages 4098–4109, Brussels, Belgium, October-November 2018. Association for Computational\\nLinguistics.\\n10[17] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL\\nrecognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop\\non Textual Entailment and Paraphrasing , pages 1–9, Prague, June 2007. Association for\\nComputational Linguistics.\\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\\narXiv:1606.08415, 2016.\\n[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics.\\n[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify:\\nMachine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018'),\n",
       " Document(metadata={}, page_content=' Workshop\\non Textual Entailment and Paraphrasing , pages 1–9, Prague, June 2007. Association for\\nComputational Linguistics.\\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\\narXiv:1606.08415, 2016.\\n[19] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\\nLinguistics, pages 328–339, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics.\\n[20] Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang, Nan Yang, and Ming Zhou. Read + verify:\\nMachine reading comprehension with unanswerable questions. CoRR, abs/1808.05759, 2018.\\n[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd\\nInternational Conference on Learning Representations, San Diego, CA, 2015.\\n[23] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT:\\nOpen-source toolkit for neural machine translation. In Proceedings of ACL 2017, System\\nDemonstrations, pages 67–72, Vancouver, Canada, July 2017. Association for Computational\\nLinguistics.\\n[24] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\\nIn Thirteenth International Conference on the Principles of Knowledge Representation and\\nReasoning, 2012.\\n[25] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\\ntion Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July\\n2004. Association for Computational Linguistics.\\n[26] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\\nfor natural language understanding. CoRR, abs/1901.11504, 2019.\\n[27] Yang Liu. Fine-tune BERT for extractive summarization. CoRR, abs/1903.10318, 2019.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. CoRR, abs/1705.04304, 2018.\\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of\\nthe 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, pages 2227–2237, New Orleans, Louisiana, June\\n2018. Association for Computational Linguistics.\\n[30] Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao, Bill Dolan, Yejin\\nChoi, and Jianfeng Gao. Conversing by reading: Contentful neural conversation with on-\\ndemand machine reading. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 5427–5436, Florence, Italy, July 2019. Association for\\nComputational Linguistics.\\n[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. 2018.\\n[32] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. 2019.\\n[33] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ ques-\\ntions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016.\\nAssociation for Computational Linguistics.\\n11[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\\nquestions for SQuAD'),\n",
       " Document(metadata={}, page_content=' Percy Liang. SQuAD: 100,000+ ques-\\ntions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing, pages 2383–2392, Austin, Texas, November 2016.\\nAssociation for Computational Linguistics.\\n11[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\\nquestions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short\\nPapers, pages 784–789, 2018.\\n[35] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question\\nanswering challenge. Transactions of the Association for Computational Linguistics, 7:249–266,\\nMarch 2019.\\n[36] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\\nsentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in\\nNatural Language Processing, pages 379–389, Lisbon, Portugal, September 2015. Association\\nfor Computational Linguistics.\\n[37] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics, pages 1073–1083, Vancouver, Canada, July 2017. Association for\\nComputational Linguistics.\\n[38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language\\nprocessing, pages 1631–1642, 2013.\\n[39] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to\\nsequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\\n[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\\nthinking the inception architecture for computer vision. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 2818–2826, 2016.\\n[41] Y Tam, Jiachen Ding, Cheng Niu, and Jie Zhou. Cluster-based beam search for pointer-generator\\nchatbot grounded by knowledge. In AAAI Dialog System Technology Challenges Workshop,\\n2019.\\n[42] Wilson L Taylor. Cloze procedure: A new tool for measuring readability. Journalism Bulletin,\\n30(4):415–433, 1953.\\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\\ntion Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017.\\n[44] Alex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a markov\\nrandom ﬁeld language model. CoRR, abs/1902.04094, 2019.\\n[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\\nInternational Conference on Learning Representations, 2019.\\n[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability\\njudgments. arXiv preprint arXiv:1805.12471, 2018.\\n[47] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational\\nLinguistics.\\n[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan C'),\n",
       " Document(metadata={}, page_content=' through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, pages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational\\nLinguistics.\\n[48] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah,\\nMelvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,\\nHideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason\\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey\\nDean. Google’s neural machine translation system: Bridging the gap between human and\\nmachine translation. CoRR, abs/1609.08144, 2016.\\n12[49] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike\\nChrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and\\nPhil Blunsom. Learning and evaluating general linguistic intelligence. arXiv preprint\\narXiv:1901.11373, 2019.\\n[50] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. CoRR, abs/1909.06356, 2019.\\n[51] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question\\ngeneration with maxout pointer and gated self-attention networks. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 3901–3910, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics.\\n[52] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question\\ngeneration from text: A preliminary study. In Xuanjing Huang, Jing Jiang, Dongyan Zhao,\\nYansong Feng, and Yu Hong, editors,Natural Language Processing and Chinese Computing,\\npages 662–671. Springer International Publishing, 2018.\\n[53] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. In Proceedings of the IEEE International Conference on\\nComputer Vision, pages 19–27, 2015.\\nAppendix A Long Text Generation: A Case Study\\nOur model can generate text samples using the left-to-right setting. We picked three text samples\\nsampled from left to right using our model, as shown in Table 12. We use the top- 40 truncating\\nsampling strategy [ 32], and forbid duplicate 4-grams during generation. For each example, we\\nsampled 10 times from the same input and we hand-picked the best one; as such, these samples\\nshould be considered to be better than the average model output. From the examples, we ﬁnd that the\\nmodel can produce ﬂuent output with somewhat consistent contents which ﬁts the inputs’ genres and\\ntopics. In the ﬁrst example, given a modiﬁed excerpt from the novel “ 1984” as input, the model’s\\noutput is like a short paragraph in an fantasy novel; for the other two with input from Wikipedia, the\\nmodel makes up “facts” with plausible names, time stamps and events written in the Wikipedia style.\\nAppendix B GLUE Benchmark\\nAs shown in Table 13, we summarize the data size and the evaluation metrics used for the General\\nLanguage Understanding Evaluation (GLUE) benchmark.\\n13Input Output\\nWinston sat back. A\\nsense of complete\\nhelplessness had\\ndescended upon him.\\nWinston sat back. A sense of complete helplessness had descended\\nupon him. He was now helpless and unmoving. What would that\\nmean? To become a monk under her mother’s rule. What would it\\nmean for him to be put at the center of the most dangerous of all of\\ndemons, the demon known as Azrael? He would never know exactly\\nwhat that was, or even how it would end, unless he could sense the\\n'),\n",
       " Document(metadata={}, page_content='descended upon him.\\nWinston sat back. A sense of complete helplessness had descended\\nupon him. He was now helpless and unmoving. What would that\\nmean? To become a monk under her mother’s rule. What would it\\nmean for him to be put at the center of the most dangerous of all of\\ndemons, the demon known as Azrael? He would never know exactly\\nwhat that was, or even how it would end, unless he could sense the\\nstrength of Azrael’s mind and soul. It wasn’t only the existence of\\nhis mother, it was all he knew, and every moment. He was trapped\\nwithin her embrace, a trapped, torn, battered, crippling presence he\\nonly knew it would mean was eternity. And in the instant that he\\nwas lost his life changed forever, when Azraelis’ spirit touched his\\nmind, he was engulfed in this dark and mystical, so enchanting and\\nbeautiful force. (...)\\nBert is a yellow\\nMuppet character\\non the long running\\nchildren’s television\\nshow, Sesame Street.\\nBert is a yellow Muppet character on the long running children’s\\ntelevision show, Sesame Street. He would typically appear as a\\nsidekick or as a villain in the show; he is usually portrayed as\\na cowardly and untrustworthy ﬁgure that tends to intrude on the\\nnormal life of the street, such as “The Story of the Mole” , “The\\nStory from the Side”, and “Bump the Bush”. His voice was provided\\nby Michael Combs. Bert was a regular cast member of Sesame\\nStreet, and his appearance was always played throughout the series\\nby two of his regular cast members, Frank Laughner and Michael\\nCombs. (...)\\nMaurice Ravel (7\\nMarch 1875 - 28\\nDecember 1937) was\\na French composer,\\npianist and conductor.\\nMaurice Ravel (7 March 1875 - 28 December 1937) was a French\\ncomposer, pianist and conductor.He was born in Paris, and studied\\npiano with Eugene Massenet. In the early 1900s Ravel became\\ninterested in the composition of music, and he composed much of\\nhis early music for the piano. Ravel’s early work is marked by a\\nmarked sense of lyricism, but in the later years of his career he\\ndeveloped a strong preference for orchestral forms. His ﬁrst work,\\n“le Grand Cours d’ Auvergne”, was composed in 1909 by Maurice\\nRavel, a student of Jules Massenet, and was published by A.F.A.\\nin 1912. It was re-published in 1912 by the publisher J.S.D.M. de\\nl’Etablissement Musicale de la Musique Francaise. Ravel wrote\\nthe piano concerto “la Tragedie et la Chanson Dans le Theatre des\\nChamps Elysees” in 1916. (...)\\nTable 12: Text samples generated by our model using left-to-right generation.\\nCorpus #Train/#Dev/#Test Metrics\\nSingle-Sentence Classiﬁcation\\nCoLA (Acceptability) 8.5k/1k/1k Matthews corr\\nSST-2 (Sentiment) 67k/872/1.8k Accuracy\\nPairwise Text Classiﬁcation\\nMNLI (NLI) 393k/20k/20k Accuracy\\nRTE (NLI) 2.5k/276/3k Accuracy\\nQNLI (NLI) 108k/5.7k/5.7k Accuracy\\nWNLI (NLI) 634/71/146 Accuracy\\nQQP (Paraphrase) 364k/40k/391k F1 score\\nMRPC (Paraphrase) 3.7k/408/1.7k F1 score\\nText Similarity\\nSTS-B (Similarity) 7k/1.5k/1.4k Spearman corr\\nTable 13: Summary of the GLUE benchmark.\\n14')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_answer_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_answer_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We Successfully created the chunks,\n",
    "# Lets define the large language model\n",
    "from langchain.chat_models import ChatOpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ques_gen_pipeline = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    temperature = 0.3 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create the prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are an expert at creating questions based on coding materials and documentation --- > Here based on the document please change the content\n",
    "# here i upload the coding document, if it is stats based then you need to give stats materials there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use the prompt we use something called as prompt template from langchain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the prompt template\n",
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text']) #here the input is text - prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refine prompt template\n",
    "#Whatever questions that have generated by the model from the first prompt, we have to give that question to the second prompt, so that it will refine the question.\n",
    "refine_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the refine template\n",
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I have generated the questions and I need to integrate my large language model as well as the prompt template \n",
    "#for this iam going to use one function from langchain called “load_summarize_chain”\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline, \n",
    "                                          chain_type = \"refine\", \n",
    "                                          verbose = True, \n",
    "                                          question_prompt=PROMPT_QUESTIONS, \n",
    "                                          refine_prompt=REFINE_PROMPT_QUESTIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qacreator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
